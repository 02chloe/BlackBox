{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9899d2-780c-4e19-9e61-67a645ea18fb",
   "metadata": {},
   "source": [
    "# TMM代码：：\n",
    "完整修改代码  tmm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21fc353-e4a0-4b1e-b1c1-a1cb42a4cf3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载DETR-R50模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DETR-R50模型加载完成\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "迭代1/5 | 行人置信度损失: 0.3210 | 梯度历史数: 12\n",
      "迭代2/5 | 行人置信度损失: 0.3134 | 梯度历史数: 12\n",
      "迭代3/5 | 行人置信度损失: 0.3298 | 梯度历史数: 12\n",
      "迭代4/5 | 行人置信度损失: 0.3272 | 梯度历史数: 12\n",
      "迭代5/5 | 行人置信度损失: 0.3190 | 梯度历史数: 12\n",
      "✅ TMM已移除所有hook\n",
      "\n",
      "✅ 白盒实验核心流程验证完成（梯度传播正常）\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Dict, Literal\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "\n",
    "class NestedTensor:\n",
    "    \"\"\"匹配DETR的NestedTensor属性（复数tensors）\"\"\"\n",
    "    def __init__(self, tensors: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        self.tensors = tensors  # 复数属性，匹配DETR调用\n",
    "        self.mask = mask if mask is not None else torch.zeros(\n",
    "            (tensors.shape[0], tensors.shape[2], tensors.shape[3]), \n",
    "            dtype=torch.bool, \n",
    "            device=tensors.device\n",
    "        )\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.tensors.device\n",
    "\n",
    "\n",
    "class TransformerMaskingMatrix(nn.Module):\n",
    "    \"\"\"严格对齐《BlackBox》论文3.1节TMM模块（保留梯度传播）\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_enc_layers: int = 6,\n",
    "        num_dec_layers: int = 6,\n",
    "        p_base: float = 0.2,\n",
    "        sampling_strategy: Literal['categorical', 'bernoulli'] = 'categorical',\n",
    "        device: Optional[torch.device] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_enc_layers = num_enc_layers\n",
    "        self.num_dec_layers = num_dec_layers\n",
    "        self.p_base = p_base\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "\n",
    "        if self.sampling_strategy not in ['categorical', 'bernoulli']:\n",
    "            raise ValueError(f\"采样策略仅支持'categorical'和'bernoulli'，当前为{self.sampling_strategy}\")\n",
    "\n",
    "        self.grad_history: Dict[str, torch.Tensor] = {}\n",
    "        self.hooks: List[torch.utils.hooks.RemovableHandle] = []\n",
    "\n",
    "    def _categorical_mask_sampling(self, grad_abs: torch.Tensor) -> torch.Tensor:\n",
    "        grad_flat = grad_abs.flatten()\n",
    "        total_grad = grad_flat.sum()\n",
    "        num_elements = grad_flat.numel()\n",
    "\n",
    "        if total_grad < 1e-8:\n",
    "            prob_dist = torch.ones_like(grad_flat) / num_elements\n",
    "        else:\n",
    "            prob_dist = grad_flat / total_grad\n",
    "\n",
    "        num_to_mask = max(1, int(self.p_base * num_elements))\n",
    "        indices = torch.multinomial(prob_dist, num_to_mask, replacement=False)\n",
    "        mask_flat = torch.ones_like(grad_flat)\n",
    "        mask_flat = mask_flat.scatter_(0, indices, 0.0)\n",
    "\n",
    "        return mask_flat.view(grad_abs.shape).contiguous()\n",
    "\n",
    "    def _apply_mask_to_input(self, input_tensor: torch.Tensor, layer_key: str) -> torch.Tensor:\n",
    "        # 修正：仅用clone()确保内存独立，保留梯度（移除detach()）\n",
    "        input_tensor = input_tensor.clone().contiguous()\n",
    "        input_dim = input_tensor.dim()\n",
    "\n",
    "        if input_dim == 4:\n",
    "            B, C, H, W = input_tensor.shape\n",
    "            input_seq = input_tensor.flatten(2).permute(2, 0, 1).contiguous()\n",
    "            grad_abs = self.grad_history.get(layer_key)\n",
    "\n",
    "            mask = self._categorical_mask_sampling(grad_abs) if grad_abs is not None else \\\n",
    "                   (torch.rand_like(input_seq) > self.p_base).float()\n",
    "\n",
    "            # 修正：保留梯度（移除detach()）\n",
    "            masked_seq = (input_seq * mask).clone().contiguous()\n",
    "            return masked_seq.permute(1, 2, 0).view(B, C, H, W).contiguous()\n",
    "\n",
    "        elif input_dim == 3:\n",
    "            grad_abs = self.grad_history.get(layer_key)\n",
    "            mask = self._categorical_mask_sampling(grad_abs) if grad_abs is not None else \\\n",
    "                   (torch.rand_like(input_tensor) > self.p_base).float()\n",
    "\n",
    "            # 修正：保留梯度（移除detach()）\n",
    "            return (input_tensor * mask).clone().contiguous()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的输入维度：{input_dim}\")\n",
    "\n",
    "    def _register_layer_hooks(self, layers: nn.ModuleList, prefix: str):\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            layer_key = f\"{prefix}_{layer_idx}\"\n",
    "\n",
    "            def backward_hook(module, grad_in, grad_out, key=layer_key):\n",
    "                if grad_in[0] is not None:\n",
    "                    # 存储梯度时仍需detach（不影响传播链）\n",
    "                    self.grad_history[key] = grad_in[0].abs().detach().clone().contiguous()\n",
    "\n",
    "            def forward_hook(module, args, key=layer_key):\n",
    "                input_tensor = args[0]\n",
    "                return (self._apply_mask_to_input(input_tensor, key),) + args[1:]\n",
    "\n",
    "            self.hooks.append(layer.register_full_backward_hook(backward_hook, prepend=False))\n",
    "            self.hooks.append(layer.register_forward_pre_hook(forward_hook))\n",
    "\n",
    "    def register_hooks(self, model: nn.Module):\n",
    "        self.remove_hooks()\n",
    "        base_model = getattr(model, 'module', model)\n",
    "\n",
    "        assert hasattr(base_model, \"transformer\"), \"模型必须包含transformer属性\"\n",
    "        assert len(base_model.transformer.encoder.layers) >= self.num_enc_layers, \"encoder层数不足\"\n",
    "        assert len(base_model.transformer.decoder.layers) >= self.num_dec_layers, \"decoder层数不足\"\n",
    "\n",
    "        self._register_layer_hooks(base_model.transformer.encoder.layers, prefix=\"enc\")\n",
    "        self._register_layer_hooks(base_model.transformer.decoder.layers, prefix=\"dec\")\n",
    "\n",
    "        print(f\"✅ TMM已注册{len(self.hooks)}个hook（{self.num_enc_layers} encoder + {self.num_dec_layers} decoder）\")\n",
    "        print(f\"✅ 采样策略：{self.sampling_strategy}（符合论文设置）\")\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        print(\"✅ TMM已移除所有hook\")\n",
    "\n",
    "    def reset_grad_history(self):\n",
    "        self.grad_history.clear()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"TMM通过register_hooks()注入掩码，无需调用forward\")\n",
    "\n",
    "\n",
    "def load_detr_r50():\n",
    "    \"\"\"加载DETR-R50模型\"\"\"\n",
    "    model = torch.hub.load(\n",
    "        \"facebookresearch/detr:main\",\n",
    "        \"detr_resnet50\",\n",
    "        pretrained=False,\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "    weight_url = \"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\"\n",
    "    checkpoint = load_state_dict_from_url(weight_url, progress=True)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    model = model.cuda().train()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # 冻结模型参数，只优化补丁\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_blackbox_whitebox_demo():\n",
    "    # 1. 加载模型\n",
    "    print(\"正在加载DETR-R50模型...\")\n",
    "    model = load_detr_r50()\n",
    "    print(\"✅ DETR-R50模型加载完成\")\n",
    "\n",
    "    # 2. 初始化TMM\n",
    "    tmm = TransformerMaskingMatrix(\n",
    "        num_enc_layers=6,\n",
    "        num_dec_layers=6,\n",
    "        p_base=0.2,\n",
    "        sampling_strategy='categorical',\n",
    "        device='cuda'\n",
    "    )\n",
    "    tmm.register_hooks(model)\n",
    "\n",
    "    # 3. 初始化补丁（需要梯度）和优化器\n",
    "    patch = torch.randn(1, 3, 300, 300, device='cuda', requires_grad=True)  # 关键：requires_grad=True\n",
    "    optimizer = torch.optim.Adam([patch], lr=0.005)  # 优化器绑定patch\n",
    "\n",
    "    # 4. 模拟输入图像（无需梯度）\n",
    "    img = torch.randn(1, 3, 800, 800, device='cuda').clone().contiguous()\n",
    "    img.requires_grad = False\n",
    "\n",
    "    # 5. 优化循环\n",
    "    for iter in range(5):\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        tmm.reset_grad_history()\n",
    "\n",
    "        # 生成掩码（无需梯度）\n",
    "        mask = torch.zeros_like(img, device='cuda').clone().contiguous()\n",
    "        mask[:, :, 100:400, 100:400] = 1.0\n",
    "\n",
    "        # 补丁填充（保留梯度，移除detach()）\n",
    "        padded_patch = torch.nn.functional.pad(patch, (100, 400, 100, 400)).clone().contiguous()\n",
    "\n",
    "        # 生成patched_img（保留梯度传播链）\n",
    "        patched_img = torch.empty_like(img, device='cuda')\n",
    "        fusion_result = img * (1 - mask) + padded_patch * mask  # 融合逻辑（保留梯度）\n",
    "        patched_img.copy_(fusion_result.clone().contiguous())  # 仅clone，不detach\n",
    "        patched_img.requires_grad_(True)  # 确保启用梯度\n",
    "\n",
    "        # 构造NestedTensor输入模型\n",
    "        nested_patched_img = NestedTensor(tensors=patched_img)\n",
    "        outputs = model(nested_patched_img)\n",
    "\n",
    "        # 计算损失（行人类别置信度）\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        person_confidence = torch.sigmoid(pred_logits[..., 1]).mean()\n",
    "        loss = person_confidence  # 目标：降低行人置信度\n",
    "\n",
    "        # 反向传播（此时梯度链已连通）\n",
    "        loss.backward()  # 现在loss能找到需要梯度的patch\n",
    "        optimizer.step()\n",
    "\n",
    "        # 补丁裁剪\n",
    "        with torch.no_grad():\n",
    "            patch.data = torch.clamp(patch.data, -2.1179, 2.6400)\n",
    "\n",
    "        print(f\"迭代{iter+1}/5 | 行人置信度损失: {loss.item():.4f} | 梯度历史数: {len(tmm.grad_history)}\")\n",
    "\n",
    "    tmm.remove_hooks()\n",
    "    print(\"\\n✅ 白盒实验核心流程验证完成（梯度传播正常）\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    run_blackbox_whitebox_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d652a-3576-4aee-a384-f1081e5fb0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f455c2-8fff-4ab3-8445-5b4f5a0e5fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae58d679-4f65-4eb4-82aa-80dac4f32f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tmm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tmm.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Dict, Literal\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "\n",
    "class NestedTensor:\n",
    "    \"\"\"匹配DETR的NestedTensor属性（复数tensors）\"\"\"\n",
    "    def __init__(self, tensors: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        self.tensors = tensors  # 复数属性，匹配DETR调用\n",
    "        self.mask = mask if mask is not None else torch.zeros(\n",
    "            (tensors.shape[0], tensors.shape[2], tensors.shape[3]), \n",
    "            dtype=torch.bool, \n",
    "            device=tensors.device\n",
    "        )\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.tensors.device\n",
    "\n",
    "\n",
    "class TransformerMaskingMatrix(nn.Module):\n",
    "    \"\"\"严格对齐《BlackBox》论文3.1节TMM模块（保留梯度传播）\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_enc_layers: int = 6,\n",
    "        num_dec_layers: int = 6,\n",
    "        p_base: float = 0.2,\n",
    "        sampling_strategy: Literal['categorical', 'bernoulli'] = 'categorical',\n",
    "        device: Optional[torch.device] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_enc_layers = num_enc_layers\n",
    "        self.num_dec_layers = num_dec_layers\n",
    "        self.p_base = p_base\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "\n",
    "        if self.sampling_strategy not in ['categorical', 'bernoulli']:\n",
    "            raise ValueError(f\"采样策略仅支持'categorical'和'bernoulli'，当前为{self.sampling_strategy}\")\n",
    "\n",
    "        self.grad_history: Dict[str, torch.Tensor] = {}\n",
    "        self.hooks: List[torch.utils.hooks.RemovableHandle] = []\n",
    "\n",
    "    def _categorical_mask_sampling(self, grad_abs: torch.Tensor) -> torch.Tensor:\n",
    "        grad_flat = grad_abs.flatten()\n",
    "        total_grad = grad_flat.sum()\n",
    "        num_elements = grad_flat.numel()\n",
    "\n",
    "        if total_grad < 1e-8:\n",
    "            prob_dist = torch.ones_like(grad_flat) / num_elements\n",
    "        else:\n",
    "            prob_dist = grad_flat / total_grad\n",
    "\n",
    "        num_to_mask = max(1, int(self.p_base * num_elements))\n",
    "        indices = torch.multinomial(prob_dist, num_to_mask, replacement=False)\n",
    "        mask_flat = torch.ones_like(grad_flat)\n",
    "        mask_flat = mask_flat.scatter_(0, indices, 0.0)\n",
    "\n",
    "        return mask_flat.view(grad_abs.shape).contiguous()\n",
    "\n",
    "    def _apply_mask_to_input(self, input_tensor: torch.Tensor, layer_key: str) -> torch.Tensor:\n",
    "        # 修正：仅用clone()确保内存独立，保留梯度（移除detach()）\n",
    "        input_tensor = input_tensor.clone().contiguous()\n",
    "        input_dim = input_tensor.dim()\n",
    "\n",
    "        if input_dim == 4:\n",
    "            B, C, H, W = input_tensor.shape\n",
    "            input_seq = input_tensor.flatten(2).permute(2, 0, 1).contiguous()\n",
    "            grad_abs = self.grad_history.get(layer_key)\n",
    "\n",
    "            mask = self._categorical_mask_sampling(grad_abs) if grad_abs is not None else \\\n",
    "                   (torch.rand_like(input_seq) > self.p_base).float()\n",
    "\n",
    "            # 修正：保留梯度（移除detach()）\n",
    "            masked_seq = (input_seq * mask).clone().contiguous()\n",
    "            return masked_seq.permute(1, 2, 0).view(B, C, H, W).contiguous()\n",
    "\n",
    "        elif input_dim == 3:\n",
    "            grad_abs = self.grad_history.get(layer_key)\n",
    "            mask = self._categorical_mask_sampling(grad_abs) if grad_abs is not None else \\\n",
    "                   (torch.rand_like(input_tensor) > self.p_base).float()\n",
    "\n",
    "            # 修正：保留梯度（移除detach()）\n",
    "            return (input_tensor * mask).clone().contiguous()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的输入维度：{input_dim}\")\n",
    "\n",
    "    def _register_layer_hooks(self, layers: nn.ModuleList, prefix: str):\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            layer_key = f\"{prefix}_{layer_idx}\"\n",
    "\n",
    "            def backward_hook(module, grad_in, grad_out, key=layer_key):\n",
    "                if grad_in[0] is not None:\n",
    "                    # 存储梯度时仍需detach（不影响传播链）\n",
    "                    self.grad_history[key] = grad_in[0].abs().detach().clone().contiguous()\n",
    "\n",
    "            def forward_hook(module, args, key=layer_key):\n",
    "                input_tensor = args[0]\n",
    "                return (self._apply_mask_to_input(input_tensor, key),) + args[1:]\n",
    "\n",
    "            self.hooks.append(layer.register_full_backward_hook(backward_hook, prepend=False))\n",
    "            self.hooks.append(layer.register_forward_pre_hook(forward_hook))\n",
    "\n",
    "    def register_hooks(self, model: nn.Module):\n",
    "        self.remove_hooks()\n",
    "        base_model = getattr(model, 'module', model)\n",
    "\n",
    "        assert hasattr(base_model, \"transformer\"), \"模型必须包含transformer属性\"\n",
    "        assert len(base_model.transformer.encoder.layers) >= self.num_enc_layers, \"encoder层数不足\"\n",
    "        assert len(base_model.transformer.decoder.layers) >= self.num_dec_layers, \"decoder层数不足\"\n",
    "\n",
    "        self._register_layer_hooks(base_model.transformer.encoder.layers, prefix=\"enc\")\n",
    "        self._register_layer_hooks(base_model.transformer.decoder.layers, prefix=\"dec\")\n",
    "\n",
    "        print(f\"✅ TMM已注册{len(self.hooks)}个hook（{self.num_enc_layers} encoder + {self.num_dec_layers} decoder）\")\n",
    "        print(f\"✅ 采样策略：{self.sampling_strategy}（符合论文设置）\")\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        print(\"✅ TMM已移除所有hook\")\n",
    "\n",
    "    def reset_grad_history(self):\n",
    "        self.grad_history.clear()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"TMM通过register_hooks()注入掩码，无需调用forward\")\n",
    "\n",
    "\n",
    "def load_detr_r50():\n",
    "    \"\"\"加载DETR-R50模型\"\"\"\n",
    "    model = torch.hub.load(\n",
    "        \"facebookresearch/detr:main\",\n",
    "        \"detr_resnet50\",\n",
    "        pretrained=False,\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "    weight_url = \"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\"\n",
    "    checkpoint = load_state_dict_from_url(weight_url, progress=True)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    model = model.cuda().train()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # 冻结模型参数，只优化补丁\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_blackbox_whitebox_demo():\n",
    "    # 1. 加载模型\n",
    "    print(\"正在加载DETR-R50模型...\")\n",
    "    model = load_detr_r50()\n",
    "    print(\"✅ DETR-R50模型加载完成\")\n",
    "\n",
    "    # 2. 初始化TMM\n",
    "    tmm = TransformerMaskingMatrix(\n",
    "        num_enc_layers=6,\n",
    "        num_dec_layers=6,\n",
    "        p_base=0.2,\n",
    "        sampling_strategy='categorical',\n",
    "        device='cuda'\n",
    "    )\n",
    "    tmm.register_hooks(model)\n",
    "\n",
    "    # 3. 初始化补丁（需要梯度）和优化器\n",
    "    patch = torch.randn(1, 3, 300, 300, device='cuda', requires_grad=True)  # 关键：requires_grad=True\n",
    "    optimizer = torch.optim.Adam([patch], lr=0.005)  # 优化器绑定patch\n",
    "\n",
    "    # 4. 模拟输入图像（无需梯度）\n",
    "    img = torch.randn(1, 3, 800, 800, device='cuda').clone().contiguous()\n",
    "    img.requires_grad = False\n",
    "\n",
    "    # 5. 优化循环\n",
    "    for iter in range(5):\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        tmm.reset_grad_history()\n",
    "\n",
    "        # 生成掩码（无需梯度）\n",
    "        mask = torch.zeros_like(img, device='cuda').clone().contiguous()\n",
    "        mask[:, :, 100:400, 100:400] = 1.0\n",
    "\n",
    "        # 补丁填充（保留梯度，移除detach()）\n",
    "        padded_patch = torch.nn.functional.pad(patch, (100, 400, 100, 400)).clone().contiguous()\n",
    "\n",
    "        # 生成patched_img（保留梯度传播链）\n",
    "        patched_img = torch.empty_like(img, device='cuda')\n",
    "        fusion_result = img * (1 - mask) + padded_patch * mask  # 融合逻辑（保留梯度）\n",
    "        patched_img.copy_(fusion_result.clone().contiguous())  # 仅clone，不detach\n",
    "        patched_img.requires_grad_(True)  # 确保启用梯度\n",
    "\n",
    "        # 构造NestedTensor输入模型\n",
    "        nested_patched_img = NestedTensor(tensors=patched_img)\n",
    "        outputs = model(nested_patched_img)\n",
    "\n",
    "        # 计算损失（行人类别置信度）\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        person_confidence = torch.sigmoid(pred_logits[..., 1]).mean()\n",
    "        loss = person_confidence  # 目标：降低行人置信度\n",
    "\n",
    "        # 反向传播（此时梯度链已连通）\n",
    "        loss.backward()  # 现在loss能找到需要梯度的patch\n",
    "        optimizer.step()\n",
    "\n",
    "        # 补丁裁剪\n",
    "        with torch.no_grad():\n",
    "            patch.data = torch.clamp(patch.data, -2.1179, 2.6400)\n",
    "\n",
    "        print(f\"迭代{iter+1}/5 | 行人置信度损失: {loss.item():.4f} | 梯度历史数: {len(tmm.grad_history)}\")\n",
    "\n",
    "    tmm.remove_hooks()\n",
    "    print(\"\\n✅ 白盒实验核心流程验证完成（梯度传播正常）\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    run_blackbox_whitebox_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948930c-74bf-4eca-a393-4cc3b930f2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b6dfe-6e1e-4773-bec9-bd4c725befe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea230e65-c45f-4238-824b-00427dddb1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38644c1-fa3e-46df-80a6-e0acefb29757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12212fa-48c6-4fe5-ae63-f4abe2ae2b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a3bc2-a607-442a-b6a0-65e8c8b77b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd824b-8909-4600-82c8-15cf29028513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2921f-0e99-462a-9db1-9971e6cb34e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719562c-a09d-4268-8170-e5f6ba6ff0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc30dea-47dc-40e2-b21c-4dd4530fdbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5b4d6-6844-45d0-acbe-afc0fb364079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
