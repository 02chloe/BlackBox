{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc2527a-1062-464f-845b-34de6efb5e6a",
   "metadata": {},
   "source": [
    "# loss代码：\n",
    "/opt/data/private/BlackBox/loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6fe50bc-2ebb-4fde-90d6-59301fe9b78d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing loss.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile loss.py\n",
    "# 文件: loss.py\n",
    "# 目的：实现 BlackBox 论文风格的攻击损失（兼容 GSE + TMM）\n",
    "# 兼容路径: /opt/data/private/BlackBox/gse.py, /opt/data/private/BlackBox/tmm.py\n",
    "# 设计原则：\n",
    "#  - 用 GSE 提供的 decoder intermediate logits 作为检测打分来源\n",
    "#  - 提供多种 layer 聚合策略（论文语义通常按层 avg）\n",
    "#  - 提供 TV / L2 正则化（常用于补丁平滑与幅度控制）；保留 NPS 钩子以便外部实现\n",
    "#  - 训练时，应只优化补丁参数（模型参数冻结），调用 loss.backward() 会把梯度传回 patch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Callable, List, Dict\n",
    "from tmm import NestedTensor\n",
    "\n",
    "class BlackBoxLoss:\n",
    "    \"\"\"\n",
    "    BlackBox-style loss for adversarial patch optimization.\n",
    "\n",
    "    主要组件（可配置）：\n",
    "      - detection_weight: 检测目标损失权重（论文主目标）\n",
    "      - tv_weight: Total Variation 正则项权重（平滑）\n",
    "      - l2_weight: L2 约束权重（限制补丁幅度）\n",
    "      - target_class: 目标类别索引（例如论文目标: 行人类 index，DETR里可能是1）\n",
    "      - layer_aggregation: 聚合中间层输出策略，支持:\n",
    "          * 'mean_logits' - 对各层 logits 做平均后再算 softmax/sigmoid -> loss\n",
    "          * 'mean_prob'   - 先对每层 logits 做 softmax/sigmoid 得到 prob，再对 prob 求均值 -> loss\n",
    "          * 'per_layer_loss' - 对每层单独计算 loss，最后平均这些 loss（最贴近论文“对每层求和/均值”的做法）\n",
    "      - use_sigmoid_for_binary: 对单一目标（binary 目标如 person）是否用 sigmoid；DETR通常多分类，若class数>2建议用 softmax\n",
    "      - reduction: 'mean' or 'sum' for loss aggregation over batch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        gse,                         # GradientSelfEnsemble 实例或任意 callable that returns logits\n",
    "        target_class: int = 1,\n",
    "        detection_weight: float = 1.0,\n",
    "        tv_weight: float = 1e-3,\n",
    "        l2_weight: float = 1e-3,\n",
    "        layer_aggregation: str = 'per_layer_loss',  # 'mean_logits' | 'mean_prob' | 'per_layer_loss'\n",
    "        use_sigmoid_for_binary: bool = True,\n",
    "        reduction: str = 'mean',\n",
    "        nps_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,  # 非可打印性分数函数 (可选)\n",
    "        device: Optional[torch.device] = None\n",
    "    ):\n",
    "        self.gse = gse\n",
    "        self.target_class = target_class\n",
    "        self.detection_weight = float(detection_weight)\n",
    "        self.tv_weight = float(tv_weight)\n",
    "        self.l2_weight = float(l2_weight)\n",
    "        self.layer_aggregation = layer_aggregation\n",
    "        self.use_sigmoid_for_binary = use_sigmoid_for_binary\n",
    "        self.reduction = reduction\n",
    "        self.nps_fn = nps_fn\n",
    "        self.device = device if device is not None else next(gse.model.parameters()).device\n",
    "\n",
    "        assert layer_aggregation in ('mean_logits', 'mean_prob', 'per_layer_loss'), \\\n",
    "            \"layer_aggregation must be one of 'mean_logits','mean_prob','per_layer_loss'\"\n",
    "\n",
    "    # --------------------------\n",
    "    # detection loss helpers\n",
    "    # --------------------------\n",
    "    @staticmethod\n",
    "    def _softmax_probs_over_classes(logits: torch.Tensor):\n",
    "        # logits shape: [..., num_classes]\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid_probs_over_classes(logits: torch.Tensor):\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "    def _compute_detection_loss_from_logits(\n",
    "        self,\n",
    "        logits_all_layers: torch.Tensor,\n",
    "        target_class: Optional[int] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        logits_all_layers: [L, B, Q, C]   (L = decoder layers)\n",
    "        返回标量 loss（对 batch 聚合，按 reduction）\n",
    "        论文实现细节：对所有层进行聚合以得到最终判分 / loss。这里提供三种聚合策略（可配置）。\n",
    "        \"\"\"\n",
    "\n",
    "        if target_class is None:\n",
    "            target_class = self.target_class\n",
    "\n",
    "        L, B, Q, C = logits_all_layers.shape\n",
    "\n",
    "        # Strategy 1: mean_logits -> average logits across layers then compute prob on class\n",
    "        if self.layer_aggregation == 'mean_logits':\n",
    "            mean_logits = logits_all_layers.mean(dim=0)  # [B, Q, C]\n",
    "            if self.use_sigmoid_for_binary and C == 1:\n",
    "                probs = self._sigmoid_probs_over_classes(mean_logits).squeeze(-1)  # [B,Q]\n",
    "                # For binary sigmoid case, target class = single logit (we assume positive)\n",
    "                loss_per_query = probs[..., 0] if probs.dim() == 2 else probs  # safe\n",
    "            else:\n",
    "                probs = self._softmax_probs_over_classes(mean_logits)  # [B,Q,C]\n",
    "                loss_per_query = probs[..., target_class]  # [B,Q]\n",
    "\n",
    "            # want to minimize target_class probability (make model less confident)\n",
    "            # create loss = mean(probabilities) so gradient pushes down the prob\n",
    "            if self.reduction == 'mean':\n",
    "                return loss_per_query.mean() * self.detection_weight\n",
    "            else:\n",
    "                return loss_per_query.sum() * self.detection_weight\n",
    "\n",
    "        # Strategy 2: mean_prob -> per-layer probs then average\n",
    "        elif self.layer_aggregation == 'mean_prob':\n",
    "            # compute probs per layer then mean\n",
    "            if self.use_sigmoid_for_binary and C == 1:\n",
    "                probs = torch.sigmoid(logits_all_layers).squeeze(-1)  # [L,B,Q]\n",
    "                target_probs = probs[..., 0]\n",
    "            else:\n",
    "                probs = F.softmax(logits_all_layers, dim=-1)  # [L,B,Q,C]\n",
    "                target_probs = probs[..., target_class]  # [L,B,Q]\n",
    "            mean_target_prob = target_probs.mean(dim=0)  # [B,Q]\n",
    "            if self.reduction == 'mean':\n",
    "                return mean_target_prob.mean() * self.detection_weight\n",
    "            else:\n",
    "                return mean_target_prob.sum() * self.detection_weight\n",
    "\n",
    "        # Strategy 3: per_layer_loss -> compute loss per layer then average\n",
    "        elif self.layer_aggregation == 'per_layer_loss':\n",
    "            layer_losses = []\n",
    "            for li in range(L):\n",
    "                logits = logits_all_layers[li]  # [B,Q,C]\n",
    "                if self.use_sigmoid_for_binary and C == 1:\n",
    "                    probs = torch.sigmoid(logits).squeeze(-1)  # [B,Q]\n",
    "                    target_probs = probs  # assumes single logit is positive class\n",
    "                else:\n",
    "                    probs = F.softmax(logits, dim=-1)  # [B,Q,C]\n",
    "                    target_probs = probs[..., target_class]  # [B,Q]\n",
    "                if self.reduction == 'mean':\n",
    "                    layer_losses.append(target_probs.mean())\n",
    "                else:\n",
    "                    layer_losses.append(target_probs.sum())\n",
    "            # average over layers\n",
    "            loss = torch.stack(layer_losses).mean() * self.detection_weight\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(\"未知的 layer_aggregation 策略\")\n",
    "\n",
    "    # --------------------------\n",
    "    # regularizers\n",
    "    # --------------------------\n",
    "    @staticmethod\n",
    "    def total_variation(image: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Total variation for a batch of image patches or images.\n",
    "        Accepts tensor shape [B, C, H, W] or [1, C, H, W] (for a single patch padded into image).\n",
    "        Returns scalar (mean over batch).\n",
    "        \"\"\"\n",
    "        # use anisotropic TV (sum of abs of differences)\n",
    "        if image.dim() != 4:\n",
    "            raise ValueError(\"total_variation expects [B,C,H,W]\")\n",
    "        dh = torch.abs(image[:, :, 1:, :] - image[:, :, :-1, :])\n",
    "        dw = torch.abs(image[:, :, :, 1:] - image[:, :, :, :-1])\n",
    "        return (dh.mean() + dw.mean())\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_norm(image: torch.Tensor):\n",
    "        # L2 norm averaged over batch\n",
    "        return torch.mean(image.pow(2))\n",
    "\n",
    "    # --------------------------\n",
    "    # 主接口：给定原始图像、patched_image 以及补丁张量，返回总loss与各子项\n",
    "    # --------------------------\n",
    "    def __call__(\n",
    "        self,\n",
    "        imgs: torch.Tensor,              # 原始batch images [B,3,H,W] or list->stacked\n",
    "        patched_imgs: torch.Tensor,      # patched images [B,3,H,W] or already NestedTensor\n",
    "        patch_tensor: Optional[torch.Tensor] = None,  # 裁剪/原补丁 [1,C,ph,pw]（可选，用于 TV/L2 正则）\n",
    "        reduction: Optional[str] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        计算并返回损失字典：\n",
    "          {\n",
    "            'total_loss': Tensor(scalar),\n",
    "            'det_loss': Tensor,\n",
    "            'tv_loss': Tensor,\n",
    "            'l2_loss': Tensor,\n",
    "            'nps_loss': Tensor\n",
    "          }\n",
    "    \n",
    "        注意：\n",
    "        - 为避免 DETR 内部在 no_grad 路径中对 view 做 inplace copy 的问题，\n",
    "          我们在传入模型前将 patched_imgs 包装为 NestedTensor（若尚未包装）。\n",
    "        - patched_imgs 可以是 Tensor 或预先构造好的 NestedTensor；函数会兼容两者。\n",
    "        \"\"\"\n",
    "        if reduction is None:\n",
    "            reduction = self.reduction\n",
    "    \n",
    "        # --- Wrap patched images into NestedTensor to avoid DETR's internal inplace copy issue\n",
    "        # Ensure images are on the correct device\n",
    "        if isinstance(patched_imgs, torch.Tensor):\n",
    "            # move to device and ensure contiguous\n",
    "            patched_imgs = patched_imgs.to(self.device)\n",
    "            if not patched_imgs.is_contiguous():\n",
    "                patched_imgs = patched_imgs.contiguous()\n",
    "            samples = NestedTensor(tensors=patched_imgs)\n",
    "        else:\n",
    "            # assume caller has provided a NestedTensor or compatible object\n",
    "            samples = patched_imgs\n",
    "    \n",
    "        # 1) use GSE to compute logits across decoder layers\n",
    "        # GSE is expected to accept the same type as model.forward (NestedTensor for DETR)\n",
    "        logits_all = self.gse(samples, return_all_layers=True)  # expected shape [L, B, Q, C]\n",
    "        logits_all = logits_all.to(self.device)\n",
    "    \n",
    "        # 2) detection loss (target class prob) computed from GSE logits\n",
    "        det_loss = self._compute_detection_loss_from_logits(logits_all, target_class=self.target_class)\n",
    "    \n",
    "        # 3) regularization losses\n",
    "        tv_loss = torch.tensor(0.0, device=self.device)\n",
    "        l2_loss = torch.tensor(0.0, device=self.device)\n",
    "        nps_loss = torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "        # If patch_tensor provided, compute TV and L2 on the patch content\n",
    "        if patch_tensor is not None:\n",
    "            p = patch_tensor.to(self.device)\n",
    "            # Normalize shape to [B, C, H, W] if needed\n",
    "            if p.dim() == 3:\n",
    "                # [C, H, W] -> [1, C, H, W]\n",
    "                p_batch = p.unsqueeze(0)\n",
    "            elif p.dim() == 4 and p.shape[0] == 1:\n",
    "                p_batch = p\n",
    "            elif p.dim() == 4 and p.shape[0] > 1:\n",
    "                p_batch = p\n",
    "            else:\n",
    "                # fallback: try to unsqueeze\n",
    "                p_batch = p.unsqueeze(0) if p.dim() == 3 else p\n",
    "    \n",
    "            # compute TV on patch region; fallback to whole image TV if fails\n",
    "            try:\n",
    "                tv_loss = self.total_variation(p_batch)\n",
    "            except Exception:\n",
    "                # As a fallback compute TV on a dummy expanded patched image region if available\n",
    "                # Here we compute on p_batch anyway to avoid raising\n",
    "                tv_loss = self.total_variation(p_batch)\n",
    "    \n",
    "            l2_loss = self.l2_norm(p_batch)\n",
    "    \n",
    "        # optional NPS (non-printability score) if function provided\n",
    "        if self.nps_fn is not None and patch_tensor is not None:\n",
    "            try:\n",
    "                nps_loss = self.nps_fn(patch_tensor.to(self.device))\n",
    "            except Exception:\n",
    "                # If nps function fails, keep as zero but do not break optimization\n",
    "                nps_loss = torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "        # 4) combine losses\n",
    "        total_loss = det_loss + self.tv_weight * tv_loss + self.l2_weight * l2_loss\n",
    "        if self.nps_fn is not None:\n",
    "            total_loss = total_loss + nps_loss\n",
    "    \n",
    "        # Respect reduction if needed (det_loss and others already aggregated in helper)\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'det_loss': det_loss,\n",
    "            'tv_loss': tv_loss,\n",
    "            'l2_loss': l2_loss,\n",
    "            'nps_loss': nps_loss\n",
    "        }\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d66d1-8513-4182-a826-a6b4c9c838c4",
   "metadata": {},
   "source": [
    "# 替换GSE代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c956bc7-d845-40dc-a11a-1acbed0a8d1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gse.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile gse.py\n",
    "# /opt/data/private/BlackBox/gse.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional\n",
    "\n",
    "class GradientSelfEnsemble:\n",
    "    \"\"\"修正版：适配DETR的维度顺序与解码器层属性名差异，并兼容 NestedTensor 输入\"\"\"\n",
    "    def __init__(self, model: nn.Module, device: Optional[torch.device] = None):\n",
    "        self.model = model\n",
    "        self.device = device if device is not None else next(model.parameters()).device\n",
    "        self._last_captured_layers: List[torch.Tensor] = []\n",
    "        self._hooks = []\n",
    "\n",
    "    def _find_decoder_layers(self) -> List[nn.Module]:\n",
    "        \"\"\"适配：同时检查'layers'（复数）和'layer'（单数）属性\"\"\"\n",
    "        if hasattr(self.model, 'transformer'):\n",
    "            transformer = getattr(self.model, 'transformer')\n",
    "            if hasattr(transformer, 'decoder'):\n",
    "                dec = getattr(transformer, 'decoder')\n",
    "                # 优先检查'layers'，再检查'layer'（适配不同DETR版本）\n",
    "                for attr in ['layers', 'layer']:\n",
    "                    if hasattr(dec, attr):\n",
    "                        layers = getattr(dec, attr)\n",
    "                        # 转换为列表（处理ModuleList或list）\n",
    "                        return list(layers) if isinstance(layers, (nn.ModuleList, list, tuple)) else [layers]\n",
    "                # 兜底：遍历解码器子模块，找到所有层（命名含'layer'）\n",
    "                decoder_layers = []\n",
    "                for name, child in dec.named_children():\n",
    "                    if 'layer' in name.lower():\n",
    "                        decoder_layers.append(child)\n",
    "                if decoder_layers:\n",
    "                    return decoder_layers\n",
    "        # 若所有方法都找不到，报错\n",
    "        raise RuntimeError(\n",
    "            \"无法在model.transformer.decoder中找到解码器层（检查'layers'或'layer'属性）。\"\n",
    "            \"请使用官方DETR模型，或修改模型以暴露解码器层。\"\n",
    "        )\n",
    "\n",
    "    def _clear_hooks(self):\n",
    "        for h in self._hooks:\n",
    "            try:\n",
    "                h.remove()\n",
    "            except Exception:\n",
    "                pass\n",
    "        self._hooks = []\n",
    "        self._last_captured_layers = []\n",
    "\n",
    "    def _register_decoder_hooks(self, decoder_layers: List[nn.Module]):\n",
    "        self._clear_hooks()\n",
    "        captured = []\n",
    "        def make_hook(i):\n",
    "            def hook(module, inp, out):\n",
    "                # 提取输出张量（处理tuple/list）\n",
    "                o = out[0] if isinstance(out, (tuple, list)) else out\n",
    "                captured.append(o)\n",
    "            return hook\n",
    "        for idx, layer_module in enumerate(decoder_layers):\n",
    "            h = layer_module.register_forward_hook(make_hook(idx))\n",
    "            self._hooks.append(h)\n",
    "        self._last_captured_layers = captured\n",
    "\n",
    "    def call_model_and_get_all_layer_logits(self, imgs_list, return_mean: bool = False):\n",
    "        \"\"\"\n",
    "        imgs_list: can be\n",
    "           - list of torch.Tensor images (each [3,H,W]) -> stack into [B,3,H,W]\n",
    "           - torch.Tensor batch [B,3,H,W]\n",
    "           - NestedTensor instance (with .tensors and .mask) from tmm.py\n",
    "        return_mean: if True, return mean logits across layers (i.e. [B,Q,C]); otherwise return [L,B,Q,C]\n",
    "        \"\"\"\n",
    "        decoder_layers = self._find_decoder_layers()\n",
    "        self._register_decoder_hooks(decoder_layers)\n",
    "\n",
    "        # === Normalize input for model invocation ===\n",
    "        is_nested = False\n",
    "        batch_size = None\n",
    "\n",
    "        # If input is list of tensors\n",
    "        if isinstance(imgs_list, list) and all(isinstance(x, torch.Tensor) for x in imgs_list):\n",
    "            imgs_tensor = torch.stack(imgs_list, dim=0).to(self.device)  # [B,3,H,W]\n",
    "            batch_size = imgs_tensor.shape[0]\n",
    "            model_input = imgs_tensor\n",
    "        # If it's already a torch.Tensor (batch)\n",
    "        elif isinstance(imgs_list, torch.Tensor):\n",
    "            imgs_tensor = imgs_list.to(self.device)\n",
    "            batch_size = imgs_tensor.shape[0]\n",
    "            model_input = imgs_tensor\n",
    "        else:\n",
    "            # Could be NestedTensor or other object expected by DETR\n",
    "            # Detect by duck-typing: has attribute 'tensors'\n",
    "            if hasattr(imgs_list, 'tensors'):\n",
    "                # treat as NestedTensor-like (we pass it directly to model)\n",
    "                is_nested = True\n",
    "                model_input = imgs_list  # pass as-is (expected by DETR.forward)\n",
    "                # determine batch size from underlying tensors if possible\n",
    "                try:\n",
    "                    batch_size = imgs_list.tensors.shape[0]\n",
    "                except Exception:\n",
    "                    # fallback: None\n",
    "                    batch_size = None\n",
    "            else:\n",
    "                # last resort: try to convert to tensor\n",
    "                try:\n",
    "                    imgs_tensor = torch.stack(list(imgs_list), dim=0).to(self.device)\n",
    "                    batch_size = imgs_tensor.shape[0]\n",
    "                    model_input = imgs_tensor\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(\"Unsupported imgs_list type for GSE: must be list/tensor/NestedTensor\") from e\n",
    "\n",
    "        # call model (pass NestedTensor if that's what DETR expects)\n",
    "        out = self.model(model_input)\n",
    "\n",
    "        captured = self._last_captured_layers\n",
    "        self._clear_hooks()\n",
    "\n",
    "        if len(captured) == 0:\n",
    "            raise RuntimeError(\"未捕获到解码器层输出\")\n",
    "\n",
    "        # captured is a list of tensors (each layer output). Normalize them to [B,Q,D]\n",
    "        normalized = []\n",
    "        for t in captured:\n",
    "            # t expected shape either [Q, B, D] or [B, Q, D] or possibly [B, Q, D] with Q=100 as in DETR\n",
    "            if not isinstance(t, torch.Tensor):\n",
    "                raise RuntimeError(\"捕获到的解码器输出不是tensor类型\")\n",
    "            if t.dim() != 3:\n",
    "                raise RuntimeError(f\"解码器输出维度错误：预期3维，实际{t.dim()}维\")\n",
    "            # try to detect ordering: if first dim equals queries (commonly 100) and second equals batch\n",
    "            # but we can't assume batch dimension from model_input in all cases; use determined batch_size when available\n",
    "            # If batch_size is known and t.shape[1] == batch_size and t.shape[0] != batch_size -> assume [Q,B,D]\n",
    "            if batch_size is not None and t.shape[1] == batch_size and t.shape[0] != batch_size:\n",
    "                # common DETR case: [Q,B,D] -> transpose\n",
    "                t_proc = t.transpose(0, 1).contiguous()  # -> [B,Q,D]\n",
    "            else:\n",
    "                # otherwise assume it's already [B,Q,D]\n",
    "                t_proc = t.contiguous()\n",
    "            normalized.append(t_proc)\n",
    "\n",
    "        # apply class_embed to each layer output to get logits [B,Q,C]\n",
    "        logits_per_layer = []\n",
    "        for layer_out in normalized:\n",
    "            try:\n",
    "                logits = self.model.class_embed(layer_out)  # [B,Q,C]\n",
    "            except Exception:\n",
    "                # some models may expect different ordering; try transpose fallback\n",
    "                logits = self.model.class_embed(layer_out.transpose(0,1)).transpose(0,1)\n",
    "            logits_per_layer.append(logits)\n",
    "\n",
    "        all_logits = torch.stack(logits_per_layer, dim=0)  # [L, B, Q, C]\n",
    "\n",
    "        if return_mean:\n",
    "            return all_logits.mean(dim=0)  # [B,Q,C]\n",
    "        return all_logits  # [L,B,Q,C]\n",
    "\n",
    "    def __call__(self, imgs_list, return_all_layers: bool = False):\n",
    "        # return_all_layers True -> return [L,B,Q,C]; False -> mean across layers [B,Q,C]\n",
    "        return self.call_model_and_get_all_layer_logits(imgs_list, return_mean=(not return_all_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf09c4e-f701-4157-a8e5-3584b4c2d55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4739227-637b-4813-aa98-234f11f424b8",
   "metadata": {},
   "source": [
    "# train.py测试pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cd901c-14d2-4e60-9010-5d1acfd58bec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 INRIAPerson dataloader...\n",
      "加载 DETR-R50 模型（demo load）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "开始 demo 训练循环（小规模）...\n",
      "[iter 1] total_loss=0.058981 | det_loss=0.058755 | tv=0.225636 | l2=0.260113 | grad_history_layers=11\n",
      "[iter 2] total_loss=0.016582 | det_loss=0.016356 | tv=0.225856 | l2=0.260133 | grad_history_layers=11\n",
      "[iter 3] total_loss=0.023158 | det_loss=0.022932 | tv=0.226138 | l2=0.260166 | grad_history_layers=11\n",
      "[iter 4] total_loss=0.060522 | det_loss=0.060296 | tv=0.226394 | l2=0.260196 | grad_history_layers=11\n",
      "[iter 5] total_loss=0.009781 | det_loss=0.009554 | tv=0.226566 | l2=0.260223 | grad_history_layers=11\n",
      "✅ TMM已移除所有hook\n",
      "DEMO 完成。可视化图像已保存在： /opt/data/private/BlackBox/save/demo\n"
     ]
    }
   ],
   "source": [
    "# %%writefile train.py\n",
    "# 文件: /opt/data/private/BlackBox/train.py\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 修改为你的工程模块导入路径（假设 train.py 与这些模块处于同一 package 目录）\n",
    "from inria_dataloader import get_inria_dataloader\n",
    "from tmm import TransformerMaskingMatrix, load_detr_r50\n",
    "from gse import GradientSelfEnsemble\n",
    "from loss import BlackBoxLoss\n",
    "\n",
    "# -----------------------\n",
    "# 配置（少量、用于 demo）\n",
    "# -----------------------\n",
    "ROOT = \"/opt/data/private/BlackBox\"\n",
    "DATA_ROOT = os.path.join(ROOT, \"data\", \"INRIAPerson\")\n",
    "SAVE_DIR = os.path.join(ROOT, \"save\", \"demo\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 2          # demo 小 batch\n",
    "NUM_ITERS = 5           # 试验迭代次数（短）\n",
    "PATCH_SIZE = (300,300)  # 论文示例大小\n",
    "PATCH_INIT_STD = 0.1    # patch 初始噪声尺度\n",
    "PATCH_POS = (100, 100)  # 左上角坐标 (x_start, y_start) — demo 占位\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------\n",
    "# 辅助函数\n",
    "# -----------------------\n",
    "def tensor_to_pil(img_tensor: torch.Tensor):\n",
    "    \"\"\"把[3,H,W] 或 [B,3,H,W] 的 tensor (0..1 or normalized) 转 PIL\n",
    "       这里假设传入的是未经归一化到 ImageNet 的 ImageTensor (0..1).\n",
    "       如果你的 transform 包含 Normalize，请先反 normalize。\n",
    "    \"\"\"\n",
    "    if img_tensor.dim() == 4:\n",
    "        img_tensor = img_tensor[0]\n",
    "    img = img_tensor.detach().cpu().clamp(0,1)\n",
    "    arr = (img.numpy().transpose(1,2,0) * 255).astype(np.uint8)\n",
    "    return Image.fromarray(arr)\n",
    "\n",
    "def apply_patch_to_image_batch(images: torch.Tensor, patch: torch.Tensor, pos=(100,100)):\n",
    "    \"\"\"\n",
    "    images: [B,3,H,W], values expected in [0,1]\n",
    "    patch:  [1,3,ph,pw] or [3,ph,pw] values in same scale\n",
    "    pos: (x_start, y_start)\n",
    "    returns patched_images (new tensor, no in-place)\n",
    "    \"\"\"\n",
    "    B, C, H, W = images.shape\n",
    "    if patch.dim() == 4 and patch.shape[0] == 1:\n",
    "        p = patch[0]\n",
    "    elif patch.dim() == 3:\n",
    "        p = patch\n",
    "    else:\n",
    "        raise ValueError(\"patch shape expect [1,3,ph,pw] or [3,ph,pw]\")\n",
    "\n",
    "    ph, pw = p.shape[1], p.shape[2]\n",
    "    x0, y0 = pos\n",
    "    if x0 + pw > W or y0 + ph > H:\n",
    "        raise ValueError(\"patch does not fit in image at given pos\")\n",
    "\n",
    "    # create padded_patch [B,3,H,W] by broadcasting p into place\n",
    "    padded = torch.zeros_like(images)\n",
    "    # use out-of-place assignment via slice on copy\n",
    "    padded[:,:, y0:y0+ph, x0:x0+pw] = p.unsqueeze(0).expand(B, -1, -1, -1)\n",
    "    patched = images * (1.0 - (padded>0).float()) + padded * (padded>0).float()\n",
    "    return patched\n",
    "\n",
    "# -----------------------\n",
    "# 加载数据（demo：只取 Train 中少量）\n",
    "# -----------------------\n",
    "print(\"加载 INRIAPerson dataloader...\")\n",
    "dataloader = get_inria_dataloader(DATA_ROOT, split=\"Train\", batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "# -----------------------\n",
    "# 加载模型\n",
    "# -----------------------\n",
    "print(\"加载 DETR-R50 模型（demo load）...\")\n",
    "model = load_detr_r50()  # 你提供的 tmm.py 中有 load_detr_r50\n",
    "model = model.to(DEVICE).train()\n",
    "# 冻结模型参数（只优化 patch）\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# -----------------------\n",
    "# 初始化 TMM（注册 hooks）\n",
    "# -----------------------\n",
    "tmm = TransformerMaskingMatrix(num_enc_layers=6, num_dec_layers=6, p_base=0.2, sampling_strategy='categorical', device=DEVICE)\n",
    "tmm.register_hooks(model)\n",
    "# 安全：清空历史（初始）\n",
    "tmm.reset_grad_history()\n",
    "\n",
    "# -----------------------\n",
    "# 初始化 GSE\n",
    "# -----------------------\n",
    "gse = GradientSelfEnsemble(model=model, device=DEVICE)\n",
    "\n",
    "# -----------------------\n",
    "# 初始化 Loss (BlackBoxLoss)\n",
    "# -----------------------\n",
    "loss_fn = BlackBoxLoss(gse=gse, target_class=1,\n",
    "                       detection_weight=1.0,\n",
    "                       tv_weight=1e-3,\n",
    "                       l2_weight=0.0,            # 可选 0\n",
    "                       layer_aggregation='per_layer_loss',\n",
    "                       use_sigmoid_for_binary=False,  # DETR has multi-class logits\n",
    "                       device=DEVICE)\n",
    "\n",
    "# -----------------------\n",
    "# 初始化补丁与优化器\n",
    "# -----------------------\n",
    "# patch 范围使用 [0,1] 颜色空间（与 dataset transform 一致）\n",
    "ph, pw = PATCH_SIZE\n",
    "patch = torch.randn(1, 3, ph, pw, device=DEVICE) * PATCH_INIT_STD + 0.5\n",
    "patch = patch.clamp(0.0, 1.0)\n",
    "patch.requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([patch], lr=0.005)\n",
    "\n",
    "# -----------------------\n",
    "# Demo 训练 loop（短循环）\n",
    "# -----------------------\n",
    "print(\"开始 demo 训练循环（小规模）...\")\n",
    "it = 0\n",
    "for epoch in range(1):\n",
    "    for batch_idx, (imgs, boxes_list) in enumerate(dataloader):\n",
    "        if it >= NUM_ITERS:\n",
    "            break\n",
    "        imgs = imgs.to(DEVICE)              # [B,3,H,W]\n",
    "        # ensure in 0..1 range (dataset ToTensor should have done this)\n",
    "        imgs = imgs.clamp(0,1)\n",
    "\n",
    "        # 1) construct patched images by pasting patch at fixed position\n",
    "        padded_patch = torch.nn.functional.pad(patch, (PATCH_POS[0], imgs.shape[-1]-PATCH_POS[0]-pw, PATCH_POS[1], imgs.shape[-2]-PATCH_POS[1]-ph))\n",
    "        # padded_patch shape [1,3,H,W], expand to batch later\n",
    "        patched_imgs = imgs * 1.0  # copy\n",
    "        # do non-inplace combination\n",
    "        mask = (padded_patch > 0).float()\n",
    "        patched_imgs = imgs * (1.0 - mask) + padded_patch.expand(imgs.shape[0], -1, -1, -1) * mask\n",
    "\n",
    "        # 2) forward + compute loss via GSE and loss_fn\n",
    "        # NOTE: tmm hooks are active and will inject masks during model.forward\n",
    "        loss_dict = loss_fn(imgs, patched_imgs, patch_tensor=patch)\n",
    "        total_loss = loss_dict['total_loss']\n",
    "\n",
    "        # 3) backward -> optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # optional clamp to valid pixel range\n",
    "        with torch.no_grad():\n",
    "            patch.clamp_(0.0, 1.0)\n",
    "\n",
    "        # 4) save debug/visualization for the first sample in batch\n",
    "        # Save original, patched, and the patch itself for inspection\n",
    "        orig = imgs[0].detach().cpu()\n",
    "        patched0 = patched_imgs[0].detach().cpu()\n",
    "        single_patch = patch[0].detach().cpu()\n",
    "\n",
    "        save_image(orig, os.path.join(SAVE_DIR, f\"iter_{it+1}_orig.png\"))\n",
    "        save_image(patched0, os.path.join(SAVE_DIR, f\"iter_{it+1}_patched.png\"))\n",
    "        save_image(single_patch, os.path.join(SAVE_DIR, f\"iter_{it+1}_patch.png\"))\n",
    "\n",
    "        # print status\n",
    "        print(f\"[iter {it+1}] total_loss={total_loss.item():.6f} | det_loss={loss_dict['det_loss'].item():.6f} | tv={loss_dict['tv_loss'].item():.6f} | l2={loss_dict['l2_loss'].item():.6f} | grad_history_layers={len(tmm.grad_history)}\")\n",
    "\n",
    "        it += 1\n",
    "\n",
    "    if it >= NUM_ITERS:\n",
    "        break\n",
    "\n",
    "# -----------------------\n",
    "# 清理 hooks\n",
    "# -----------------------\n",
    "tmm.remove_hooks()\n",
    "print(\"DEMO 完成。可视化图像已保存在：\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2ca64-39c7-4126-8da0-f7d3860192c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42c32af7-5c4e-4d62-8d20-ab9a0baad29e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954bc03-fcb0-4aff-aa18-4a308d890f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629642c-20ff-4430-a07b-a9fb24d8664f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad070847-d365-4cf2-b61a-0e37c6d4792a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43629008-77dd-4a10-ade1-224e5504ef98",
   "metadata": {},
   "source": [
    "# 添加功能GPT train："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0de56-88ab-4887-83c1-8dbd970a3f37",
   "metadata": {},
   "source": [
    "# 替换dataloader："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb68abbd-20bd-465c-88c9-e4ab2a33d4ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inria_dataloader.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile inria_dataloader.py\n",
    "# 文件名：inria_dataloader.py\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "class INRIAPersonDataset(Dataset):\n",
    "    \"\"\"INRIA Person数据集加载器（严格对齐论文训练设置）\"\"\"\n",
    "    def __init__(self, data_root, split=\"Train\", augment=True, disable_random_aug=False):\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.disable_random_aug = disable_random_aug\n",
    "        # 加载解析后的标注\n",
    "        self.annotations = json.load(open(os.path.join(data_root, f\"inria_{split}_annotations.json\"), \"r\"))\n",
    "        # 论文数据增强流水线（图2隐含操作）\n",
    "        self.transform = self._build_transform()\n",
    "\n",
    "    def _build_transform(self):\n",
    "        base_transform = [T.Resize((640, 640)),  # 论文隐含输入尺寸（适配DETR）\n",
    "                         T.ToTensor()]\n",
    "        # 判断是否禁用增强\n",
    "        if not self.disable_random_aug and self.augment and self.split == \"Train\":\n",
    "            augment_transform = [\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.RandomRotation(degrees=10),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "            ]\n",
    "            return T.Compose(augment_transform + base_transform)\n",
    "        else:\n",
    "            return T.Compose(base_transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.annotations[idx]\n",
    "        # 加载图像\n",
    "        img = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        # 加载边界框（仅训练时用于辅助观察，论文攻击损失不依赖标注）\n",
    "        boxes = torch.tensor(sample[\"boxes\"], dtype=torch.float32)\n",
    "        return img_tensor, boxes\n",
    "\n",
    "def get_inria_dataloader(data_root, split=\"Train\", batch_size=8, num_workers=1, disable_random_aug=False):\n",
    "    \"\"\"获取数据加载器（论文4.1节：batch_size=8）\"\"\"\n",
    "    dataset = INRIAPersonDataset(data_root, split=split, augment=(split==\"Train\"),\n",
    "                                 disable_random_aug=disable_random_aug)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(split==\"Train\"),\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        collate_fn=lambda x: (torch.stack([i[0] for i in x]), [i[1] for i in x])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fe026-f9a5-4e3f-90d0-ef9bf30d4f31",
   "metadata": {},
   "source": [
    "# 替换train（旧逻辑错误）：gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94938421-f671-49e1-8eab-7eb9632b7c8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "Start training with NMS de-dup and controlled fallback...\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 1] total_loss=0.143936 | det_loss=0.143711 | tv=0.225462\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 2] total_loss=0.101182 | det_loss=0.100963 | tv=0.219129\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 3] total_loss=0.099098 | det_loss=0.098884 | tv=0.214126\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 4] total_loss=0.061028 | det_loss=0.060819 | tv=0.209241\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 5] total_loss=0.097850 | det_loss=0.097645 | tv=0.205035\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 6] total_loss=0.064338 | det_loss=0.064136 | tv=0.201263\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 7] total_loss=0.058307 | det_loss=0.058109 | tv=0.198082\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 8] total_loss=0.095833 | det_loss=0.095638 | tv=0.195047\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 9] total_loss=0.048717 | det_loss=0.048524 | tv=0.192089\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 10] total_loss=0.088315 | det_loss=0.088126 | tv=0.189201\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 11] total_loss=0.070665 | det_loss=0.070478 | tv=0.186592\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 12] total_loss=0.097571 | det_loss=0.097387 | tv=0.184050\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 13] total_loss=0.122978 | det_loss=0.122796 | tv=0.181854\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 14] total_loss=0.100623 | det_loss=0.100444 | tv=0.179725\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 15] total_loss=0.042688 | det_loss=0.042511 | tv=0.177726\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 16] total_loss=0.120157 | det_loss=0.119981 | tv=0.175964\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 17] total_loss=0.072434 | det_loss=0.072260 | tv=0.174367\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 18] total_loss=0.041628 | det_loss=0.041456 | tv=0.172825\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 19] total_loss=0.077109 | det_loss=0.076937 | tv=0.171328\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 20] total_loss=0.100855 | det_loss=0.100685 | tv=0.169869\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 21] total_loss=0.090314 | det_loss=0.090145 | tv=0.168459\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 22] total_loss=0.092243 | det_loss=0.092076 | tv=0.167091\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 23] total_loss=0.055205 | det_loss=0.055040 | tv=0.165762\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 24] total_loss=0.063163 | det_loss=0.062999 | tv=0.164640\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 25] total_loss=0.037494 | det_loss=0.037331 | tv=0.163565\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 26] total_loss=0.069112 | det_loss=0.068950 | tv=0.162525\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 27] total_loss=0.041945 | det_loss=0.041783 | tv=0.161514\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 28] total_loss=0.056386 | det_loss=0.056225 | tv=0.160540\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 29] total_loss=0.076876 | det_loss=0.076717 | tv=0.159744\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 30] total_loss=0.065295 | det_loss=0.065136 | tv=0.158997\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 31] total_loss=0.130737 | det_loss=0.130579 | tv=0.158287\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 32] total_loss=0.092969 | det_loss=0.092812 | tv=0.157605\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 33] total_loss=0.075899 | det_loss=0.075742 | tv=0.156952\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 34] total_loss=0.086419 | det_loss=0.086262 | tv=0.156387\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 35] total_loss=0.056228 | det_loss=0.056072 | tv=0.155911\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 36] total_loss=0.037385 | det_loss=0.037230 | tv=0.155475\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 37] total_loss=0.051863 | det_loss=0.051708 | tv=0.155065\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 38] total_loss=0.039218 | det_loss=0.039063 | tv=0.154679\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 39] total_loss=0.062828 | det_loss=0.062674 | tv=0.154308\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 40] total_loss=0.049616 | det_loss=0.049462 | tv=0.153943\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 41] total_loss=0.085718 | det_loss=0.085564 | tv=0.153709\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 42] total_loss=0.049937 | det_loss=0.049784 | tv=0.153516\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 43] total_loss=0.104098 | det_loss=0.103945 | tv=0.153349\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 44] total_loss=0.068336 | det_loss=0.068183 | tv=0.153199\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 45] total_loss=0.097554 | det_loss=0.097401 | tv=0.153056\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 46] total_loss=0.047932 | det_loss=0.047779 | tv=0.152915\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 47] total_loss=0.080146 | det_loss=0.079994 | tv=0.152771\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 48] total_loss=0.096122 | det_loss=0.095969 | tv=0.152631\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 49] total_loss=0.038393 | det_loss=0.038241 | tv=0.152503\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 50] total_loss=0.094334 | det_loss=0.094181 | tv=0.152382\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 51] total_loss=0.074622 | det_loss=0.074470 | tv=0.152265\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 52] total_loss=0.198056 | det_loss=0.197904 | tv=0.152151\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 53] total_loss=0.096416 | det_loss=0.096264 | tv=0.152050\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 54] total_loss=0.063197 | det_loss=0.063045 | tv=0.151961\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 55] total_loss=0.067022 | det_loss=0.066870 | tv=0.151891\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 56] total_loss=0.053158 | det_loss=0.053006 | tv=0.151834\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 57] total_loss=0.098792 | det_loss=0.098640 | tv=0.151784\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 58] total_loss=0.057830 | det_loss=0.057678 | tv=0.151741\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 59] total_loss=0.041175 | det_loss=0.041023 | tv=0.151701\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 60] total_loss=0.033106 | det_loss=0.032954 | tv=0.151659\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 61] total_loss=0.109514 | det_loss=0.109363 | tv=0.151615\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 62] total_loss=0.149549 | det_loss=0.149397 | tv=0.151569\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 63] total_loss=0.057830 | det_loss=0.057679 | tv=0.151525\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 64] total_loss=0.054464 | det_loss=0.054312 | tv=0.151483\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 65] total_loss=0.057509 | det_loss=0.057358 | tv=0.151439\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 66] total_loss=0.129578 | det_loss=0.129426 | tv=0.151391\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 67] total_loss=0.037097 | det_loss=0.036946 | tv=0.151344\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 68] total_loss=0.101161 | det_loss=0.101010 | tv=0.151297\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 69] total_loss=0.044150 | det_loss=0.043999 | tv=0.151248\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 70] total_loss=0.132592 | det_loss=0.132440 | tv=0.151201\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 71] total_loss=0.060800 | det_loss=0.060649 | tv=0.151154\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 72] total_loss=0.096802 | det_loss=0.096651 | tv=0.151107\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 73] total_loss=0.179255 | det_loss=0.179104 | tv=0.151077\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 74] total_loss=0.106623 | det_loss=0.106472 | tv=0.151125\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 75] total_loss=0.129179 | det_loss=0.129028 | tv=0.151223\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 76] total_loss=0.115315 | det_loss=0.115164 | tv=0.151343\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 77] total_loss=0.074415 | det_loss=0.074263 | tv=0.151473\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 78] total_loss=0.131906 | det_loss=0.131754 | tv=0.151599\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 79] total_loss=0.177317 | det_loss=0.177166 | tv=0.151717\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 80] total_loss=0.114375 | det_loss=0.114224 | tv=0.151826\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 81] total_loss=0.122358 | det_loss=0.122206 | tv=0.151927\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 82] total_loss=0.037951 | det_loss=0.037799 | tv=0.152019\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 83] total_loss=0.107682 | det_loss=0.107530 | tv=0.152102\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 84] total_loss=0.076489 | det_loss=0.076337 | tv=0.152174\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 85] total_loss=0.075933 | det_loss=0.075781 | tv=0.152234\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 86] total_loss=0.045427 | det_loss=0.045274 | tv=0.152284\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 87] total_loss=0.079421 | det_loss=0.079269 | tv=0.152327\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 88] total_loss=0.142236 | det_loss=0.142084 | tv=0.152360\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 89] total_loss=0.050223 | det_loss=0.050071 | tv=0.152387\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 90] total_loss=0.159358 | det_loss=0.159205 | tv=0.152537\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 91] total_loss=0.077234 | det_loss=0.077081 | tv=0.152758\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 92] total_loss=0.113757 | det_loss=0.113604 | tv=0.153002\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 93] total_loss=0.161008 | det_loss=0.160855 | tv=0.153249\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 94] total_loss=0.070542 | det_loss=0.070389 | tv=0.153489\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 95] total_loss=0.060517 | det_loss=0.060363 | tv=0.153718\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 96] total_loss=0.070212 | det_loss=0.070058 | tv=0.153932\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 97] total_loss=0.048884 | det_loss=0.048729 | tv=0.154135\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 98] total_loss=0.118358 | det_loss=0.118204 | tv=0.154329\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 99] total_loss=0.042155 | det_loss=0.042001 | tv=0.154510\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 100] total_loss=0.098081 | det_loss=0.097926 | tv=0.154685\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 101] total_loss=0.140232 | det_loss=0.140077 | tv=0.154858\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 102] total_loss=0.077352 | det_loss=0.077197 | tv=0.155016\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 103] total_loss=0.061277 | det_loss=0.061122 | tv=0.155164\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 104] total_loss=0.116065 | det_loss=0.115909 | tv=0.155299\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 105] total_loss=0.106996 | det_loss=0.106841 | tv=0.155424\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 106] total_loss=0.139400 | det_loss=0.139244 | tv=0.155546\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 107] total_loss=0.072159 | det_loss=0.072004 | tv=0.155664\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 108] total_loss=0.073422 | det_loss=0.073266 | tv=0.155774\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 109] total_loss=0.024709 | det_loss=0.024553 | tv=0.155901\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 110] total_loss=0.098447 | det_loss=0.098291 | tv=0.156044\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 111] total_loss=0.103918 | det_loss=0.103762 | tv=0.156192\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 112] total_loss=0.037289 | det_loss=0.037133 | tv=0.156335\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 113] total_loss=0.086081 | det_loss=0.085925 | tv=0.156477\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 114] total_loss=0.173881 | det_loss=0.173725 | tv=0.156611\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 115] total_loss=0.111185 | det_loss=0.111029 | tv=0.156738\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 116] total_loss=0.032219 | det_loss=0.032062 | tv=0.156856\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 117] total_loss=0.109722 | det_loss=0.109565 | tv=0.156967\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 118] total_loss=0.050950 | det_loss=0.050793 | tv=0.157077\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 119] total_loss=0.042448 | det_loss=0.042291 | tv=0.157193\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 120] total_loss=0.058866 | det_loss=0.058709 | tv=0.157309\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 121] total_loss=0.028925 | det_loss=0.028768 | tv=0.157419\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 122] total_loss=0.079783 | det_loss=0.079626 | tv=0.157517\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 123] total_loss=0.059209 | det_loss=0.059052 | tv=0.157608\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 124] total_loss=0.062555 | det_loss=0.062397 | tv=0.157692\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 125] total_loss=0.048387 | det_loss=0.048229 | tv=0.157771\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 126] total_loss=0.112063 | det_loss=0.111905 | tv=0.157843\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 127] total_loss=0.046868 | det_loss=0.046710 | tv=0.157909\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 128] total_loss=0.049307 | det_loss=0.049149 | tv=0.157967\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 129] total_loss=0.042898 | det_loss=0.042740 | tv=0.158073\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 130] total_loss=0.092157 | det_loss=0.091998 | tv=0.158211\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 131] total_loss=0.046809 | det_loss=0.046650 | tv=0.158362\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 132] total_loss=0.111705 | det_loss=0.111546 | tv=0.158517\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 133] total_loss=0.074192 | det_loss=0.074033 | tv=0.158678\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 134] total_loss=0.056005 | det_loss=0.055847 | tv=0.158833\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 135] total_loss=0.050766 | det_loss=0.050607 | tv=0.158988\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 136] total_loss=0.045882 | det_loss=0.045723 | tv=0.159143\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 137] total_loss=0.045189 | det_loss=0.045030 | tv=0.159316\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 138] total_loss=0.027559 | det_loss=0.027400 | tv=0.159520\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 139] total_loss=0.082953 | det_loss=0.082793 | tv=0.159730\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 140] total_loss=0.058772 | det_loss=0.058612 | tv=0.159935\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 141] total_loss=0.113543 | det_loss=0.113383 | tv=0.160140\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 142] total_loss=0.082509 | det_loss=0.082349 | tv=0.160351\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 143] total_loss=0.096533 | det_loss=0.096373 | tv=0.160558\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 144] total_loss=0.072882 | det_loss=0.072721 | tv=0.160757\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 145] total_loss=0.106591 | det_loss=0.106430 | tv=0.160942\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 146] total_loss=0.066021 | det_loss=0.065860 | tv=0.161117\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 147] total_loss=0.107836 | det_loss=0.107674 | tv=0.161288\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 148] total_loss=0.162870 | det_loss=0.162709 | tv=0.161446\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 149] total_loss=0.034327 | det_loss=0.034165 | tv=0.161601\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 150] total_loss=0.116445 | det_loss=0.116283 | tv=0.161758\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 151] total_loss=0.090313 | det_loss=0.090151 | tv=0.161914\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 152] total_loss=0.078391 | det_loss=0.078229 | tv=0.162070\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 153] total_loss=0.097873 | det_loss=0.097711 | tv=0.162219\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 154] total_loss=0.040791 | det_loss=0.040629 | tv=0.162360\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 155] total_loss=0.033641 | det_loss=0.033479 | tv=0.162492\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 156] total_loss=0.025339 | det_loss=0.025177 | tv=0.162619\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 157] total_loss=0.073498 | det_loss=0.073335 | tv=0.162737\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 158] total_loss=0.081648 | det_loss=0.081485 | tv=0.162845\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 159] total_loss=0.104589 | det_loss=0.104426 | tv=0.162950\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 160] total_loss=0.107828 | det_loss=0.107665 | tv=0.163050\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 161] total_loss=0.104101 | det_loss=0.103938 | tv=0.163178\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 162] total_loss=0.131234 | det_loss=0.131071 | tv=0.163336\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 163] total_loss=0.148096 | det_loss=0.147932 | tv=0.163513\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 164] total_loss=0.061196 | det_loss=0.061032 | tv=0.163697\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 165] total_loss=0.030679 | det_loss=0.030515 | tv=0.163883\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 166] total_loss=0.103980 | det_loss=0.103816 | tv=0.164062\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 167] total_loss=0.126865 | det_loss=0.126701 | tv=0.164228\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 168] total_loss=0.058362 | det_loss=0.058198 | tv=0.164396\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 169] total_loss=0.086023 | det_loss=0.085858 | tv=0.164558\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 170] total_loss=0.107275 | det_loss=0.107110 | tv=0.164711\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 171] total_loss=0.044820 | det_loss=0.044655 | tv=0.164854\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 172] total_loss=0.024852 | det_loss=0.024687 | tv=0.164988\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 173] total_loss=0.073488 | det_loss=0.073323 | tv=0.165114\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 174] total_loss=0.122630 | det_loss=0.122464 | tv=0.165240\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 175] total_loss=0.056366 | det_loss=0.056201 | tv=0.165359\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 176] total_loss=0.052115 | det_loss=0.051949 | tv=0.165469\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 177] total_loss=0.069322 | det_loss=0.069156 | tv=0.165574\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 178] total_loss=0.093494 | det_loss=0.093329 | tv=0.165677\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 179] total_loss=0.055254 | det_loss=0.055088 | tv=0.165767\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 180] total_loss=0.082913 | det_loss=0.082748 | tv=0.165855\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 181] total_loss=0.053604 | det_loss=0.053438 | tv=0.165937\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 182] total_loss=0.129548 | det_loss=0.129382 | tv=0.166012\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 183] total_loss=0.103071 | det_loss=0.102905 | tv=0.166080\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 184] total_loss=0.041148 | det_loss=0.040982 | tv=0.166144\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 185] total_loss=0.032009 | det_loss=0.031843 | tv=0.166201\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 186] total_loss=0.037906 | det_loss=0.037740 | tv=0.166249\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 187] total_loss=0.070645 | det_loss=0.070479 | tv=0.166298\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 188] total_loss=0.030726 | det_loss=0.030560 | tv=0.166345\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 189] total_loss=0.110212 | det_loss=0.110046 | tv=0.166386\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 190] total_loss=0.095427 | det_loss=0.095261 | tv=0.166423\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 191] total_loss=0.035763 | det_loss=0.035597 | tv=0.166481\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 192] total_loss=0.100613 | det_loss=0.100447 | tv=0.166560\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 193] total_loss=0.080349 | det_loss=0.080182 | tv=0.166644\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 194] total_loss=0.120758 | det_loss=0.120592 | tv=0.166779\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 195] total_loss=0.088476 | det_loss=0.088309 | tv=0.166952\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 196] total_loss=0.033685 | det_loss=0.033518 | tv=0.167138\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 197] total_loss=0.026243 | det_loss=0.026076 | tv=0.167327\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 198] total_loss=0.071078 | det_loss=0.070910 | tv=0.167512\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 199] total_loss=0.101091 | det_loss=0.100924 | tv=0.167695\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 200] total_loss=0.095666 | det_loss=0.095498 | tv=0.167869\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 201] total_loss=0.039801 | det_loss=0.039633 | tv=0.168042\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 202] total_loss=0.076733 | det_loss=0.076564 | tv=0.168204\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 203] total_loss=0.109155 | det_loss=0.108987 | tv=0.168351\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 204] total_loss=0.082538 | det_loss=0.082370 | tv=0.168492\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 205] total_loss=0.072832 | det_loss=0.072663 | tv=0.168627\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 206] total_loss=0.140049 | det_loss=0.139880 | tv=0.168753\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 207] total_loss=0.111226 | det_loss=0.111058 | tv=0.168866\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 208] total_loss=0.063810 | det_loss=0.063641 | tv=0.169007\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 209] total_loss=0.035531 | det_loss=0.035362 | tv=0.169165\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 210] total_loss=0.103968 | det_loss=0.103799 | tv=0.169324\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 211] total_loss=0.073345 | det_loss=0.073176 | tv=0.169480\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 212] total_loss=0.064446 | det_loss=0.064276 | tv=0.169636\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 213] total_loss=0.143314 | det_loss=0.143144 | tv=0.169793\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 214] total_loss=0.058001 | det_loss=0.057831 | tv=0.169955\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 215] total_loss=0.029114 | det_loss=0.028944 | tv=0.170132\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 216] total_loss=0.053748 | det_loss=0.053578 | tv=0.170317\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 217] total_loss=0.032436 | det_loss=0.032265 | tv=0.170500\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 218] total_loss=0.067724 | det_loss=0.067553 | tv=0.170683\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 219] total_loss=0.036548 | det_loss=0.036378 | tv=0.170862\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 220] total_loss=0.067529 | det_loss=0.067358 | tv=0.171037\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 221] total_loss=0.092504 | det_loss=0.092333 | tv=0.171200\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 222] total_loss=0.107197 | det_loss=0.107026 | tv=0.171350\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 223] total_loss=0.059635 | det_loss=0.059464 | tv=0.171480\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 224] total_loss=0.032625 | det_loss=0.032453 | tv=0.171613\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 225] total_loss=0.055048 | det_loss=0.054876 | tv=0.171741\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 226] total_loss=0.040132 | det_loss=0.039960 | tv=0.171858\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 227] total_loss=0.041596 | det_loss=0.041424 | tv=0.171970\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 228] total_loss=0.055340 | det_loss=0.055168 | tv=0.172073\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 229] total_loss=0.105237 | det_loss=0.105065 | tv=0.172165\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 230] total_loss=0.119857 | det_loss=0.119685 | tv=0.172250\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 231] total_loss=0.083618 | det_loss=0.083446 | tv=0.172330\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 232] total_loss=0.087901 | det_loss=0.087729 | tv=0.172404\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 233] total_loss=0.037628 | det_loss=0.037456 | tv=0.172482\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 234] total_loss=0.051002 | det_loss=0.050830 | tv=0.172563\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 235] total_loss=0.077615 | det_loss=0.077442 | tv=0.172646\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 236] total_loss=0.072690 | det_loss=0.072517 | tv=0.172725\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 237] total_loss=0.071026 | det_loss=0.070854 | tv=0.172797\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 238] total_loss=0.043237 | det_loss=0.043064 | tv=0.172862\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 239] total_loss=0.050258 | det_loss=0.050085 | tv=0.172921\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 240] total_loss=0.093652 | det_loss=0.093479 | tv=0.172975\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 241] total_loss=0.093059 | det_loss=0.092886 | tv=0.173019\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 242] total_loss=0.113767 | det_loss=0.113594 | tv=0.173135\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 243] total_loss=0.036764 | det_loss=0.036590 | tv=0.173299\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 244] total_loss=0.018549 | det_loss=0.018376 | tv=0.173478\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 245] total_loss=0.063453 | det_loss=0.063279 | tv=0.173678\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 246] total_loss=0.061395 | det_loss=0.061221 | tv=0.173888\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 247] total_loss=0.092848 | det_loss=0.092674 | tv=0.174097\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 248] total_loss=0.066888 | det_loss=0.066714 | tv=0.174304\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 249] total_loss=0.110846 | det_loss=0.110672 | tv=0.174504\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 250] total_loss=0.090230 | det_loss=0.090056 | tv=0.174693\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 251] total_loss=0.077479 | det_loss=0.077304 | tv=0.174878\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 252] total_loss=0.041642 | det_loss=0.041467 | tv=0.175050\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 253] total_loss=0.043984 | det_loss=0.043809 | tv=0.175208\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 254] total_loss=0.082703 | det_loss=0.082528 | tv=0.175353\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 255] total_loss=0.035068 | det_loss=0.034892 | tv=0.175486\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 256] total_loss=0.143405 | det_loss=0.143230 | tv=0.175603\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 257] total_loss=0.083651 | det_loss=0.083476 | tv=0.175711\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 258] total_loss=0.106982 | det_loss=0.106807 | tv=0.175810\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 259] total_loss=0.024602 | det_loss=0.024426 | tv=0.175901\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 260] total_loss=0.099425 | det_loss=0.099249 | tv=0.175983\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 261] total_loss=0.072861 | det_loss=0.072685 | tv=0.176066\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 262] total_loss=0.034318 | det_loss=0.034142 | tv=0.176166\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 263] total_loss=0.060626 | det_loss=0.060449 | tv=0.176280\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 264] total_loss=0.049970 | det_loss=0.049793 | tv=0.176401\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 265] total_loss=0.051235 | det_loss=0.051059 | tv=0.176507\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 266] total_loss=0.038740 | det_loss=0.038564 | tv=0.176623\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 267] total_loss=0.107548 | det_loss=0.107371 | tv=0.176743\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 268] total_loss=0.063704 | det_loss=0.063527 | tv=0.176869\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 269] total_loss=0.071259 | det_loss=0.071082 | tv=0.176996\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 270] total_loss=0.043040 | det_loss=0.042863 | tv=0.177119\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 271] total_loss=0.084943 | det_loss=0.084766 | tv=0.177240\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 272] total_loss=0.084285 | det_loss=0.084108 | tv=0.177351\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 273] total_loss=0.038021 | det_loss=0.037844 | tv=0.177460\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 274] total_loss=0.092240 | det_loss=0.092062 | tv=0.177575\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 275] total_loss=0.090746 | det_loss=0.090569 | tv=0.177689\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 276] total_loss=0.133361 | det_loss=0.133183 | tv=0.177800\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 277] total_loss=0.040557 | det_loss=0.040379 | tv=0.177926\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 278] total_loss=0.058252 | det_loss=0.058074 | tv=0.178075\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 279] total_loss=0.079978 | det_loss=0.079800 | tv=0.178229\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 280] total_loss=0.065470 | det_loss=0.065291 | tv=0.178394\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 281] total_loss=0.115122 | det_loss=0.114944 | tv=0.178561\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 282] total_loss=0.099544 | det_loss=0.099365 | tv=0.178725\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 283] total_loss=0.031029 | det_loss=0.030850 | tv=0.178882\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 284] total_loss=0.050180 | det_loss=0.050001 | tv=0.179029\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 285] total_loss=0.070596 | det_loss=0.070416 | tv=0.179163\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 286] total_loss=0.176102 | det_loss=0.175923 | tv=0.179287\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 287] total_loss=0.098121 | det_loss=0.097942 | tv=0.179406\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 288] total_loss=0.144428 | det_loss=0.144249 | tv=0.179515\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 289] total_loss=0.026707 | det_loss=0.026527 | tv=0.179609\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 290] total_loss=0.060333 | det_loss=0.060153 | tv=0.179706\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 291] total_loss=0.066038 | det_loss=0.065858 | tv=0.179801\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 292] total_loss=0.056010 | det_loss=0.055830 | tv=0.179892\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 293] total_loss=0.066567 | det_loss=0.066387 | tv=0.179980\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 294] total_loss=0.126945 | det_loss=0.126765 | tv=0.180066\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 295] total_loss=0.033471 | det_loss=0.033291 | tv=0.180145\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 296] total_loss=0.044942 | det_loss=0.044762 | tv=0.180217\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 297] total_loss=0.058615 | det_loss=0.058434 | tv=0.180281\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 298] total_loss=0.024170 | det_loss=0.023990 | tv=0.180338\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 299] total_loss=0.078057 | det_loss=0.077877 | tv=0.180388\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "✅ TMM已移除所有hook\n",
      "[iter 300] total_loss=0.072569 | det_loss=0.072388 | tv=0.180432\n",
      "✅ TMM已移除所有hook\n",
      "Done. Visuals saved to /opt/data/private/BlackBox/save/demo\n"
     ]
    }
   ],
   "source": [
    "# /opt/data/private/BlackBox/train.py\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, draw_bounding_boxes\n",
    "from torchvision.ops import box_convert, nms\n",
    "from torch.nn.functional import interpolate\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from inria_dataloader import get_inria_dataloader\n",
    "from tmm import TransformerMaskingMatrix, load_detr_r50, NestedTensor\n",
    "from gse import GradientSelfEnsemble\n",
    "from loss import BlackBoxLoss\n",
    "\n",
    "# -----------------------\n",
    "# Config (可调整)\n",
    "# -----------------------\n",
    "ROOT = \"/opt/data/private/BlackBox\"\n",
    "DATA_ROOT = os.path.join(ROOT, \"data\", \"INRIAPerson\")\n",
    "SAVE_DIR = os.path.join(ROOT, \"save\", \"demo\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "NUM_ITERS = 300\n",
    "\n",
    "PATCH_BASE = 300               # initial global patch size (square)\n",
    "PATCH_INIT_STD = 0.1\n",
    "PATCH_RATIO = 0.15             # T-SEA recommended ratio\n",
    "MIN_PATCH_PX = 16               # minimum patch side in px (for production use 1; debug: set to 8/16)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_INPUT_H, MODEL_INPUT_W = 640, 640  # dataloader resize (H, W)\n",
    "TARGET_CLASS_IDX = 1\n",
    "SCORE_THRESH = 0.5             # keep candidates above this\n",
    "FALLBACK_TO_TOP = True\n",
    "FALLBACK_SCORE_THRESH = 0.2    # only fallback top-1 if its score >= this\n",
    "IOU_NMS_THRESH = 0.5           # NMS IoU threshold\n",
    "MIN_BOX_SIDE = 5               # ignore boxes with width or height < this (px)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def detach_cpu(img: torch.Tensor):\n",
    "    \"\"\"return CPU float tensor in 0..1\"\"\"\n",
    "    return img.detach().cpu().clamp(0,1)\n",
    "\n",
    "def draw_boxes_on_tensor(img_tensor: torch.Tensor, boxes_xyxy_cpu: torch.Tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: [3,H,W] float on CPU\n",
    "    boxes_xyxy_cpu: [N,4] CPU float xmin,ymin,xmax,ymax\n",
    "    \"\"\"\n",
    "    if boxes_xyxy_cpu is None or boxes_xyxy_cpu.numel() == 0:\n",
    "        return img_tensor\n",
    "    img_uint8 = (img_tensor * 255).byte()\n",
    "    boxes = boxes_xyxy_cpu.clone()\n",
    "    H, W = img_tensor.shape[1], img_tensor.shape[2]\n",
    "    boxes[:, [0,2]] = boxes[:, [0,2]].clamp(0, W-1)\n",
    "    boxes[:, [1,3]] = boxes[:, [1,3]].clamp(0, H-1)\n",
    "    valid = (boxes[:,2] > boxes[:,0]) & (boxes[:,3] > boxes[:,1])\n",
    "    boxes = boxes[valid]\n",
    "    if boxes.shape[0] == 0:\n",
    "        return img_tensor\n",
    "    boxes_int = boxes.to(torch.int64)\n",
    "    img_boxes = draw_bounding_boxes(img_uint8, boxes=boxes_int, colors=\"red\", width=2)\n",
    "    return img_boxes.float() / 255.0\n",
    "\n",
    "def detr_boxes_to_xyxy_pixel(pred_boxes):\n",
    "    \"\"\"\n",
    "    pred_boxes: [Q,4] cx,cy,w,h (normalized 0..1 or absolute)\n",
    "    returns [Q,4] xyxy in pixel coords (CPU tensor)\n",
    "    \"\"\"\n",
    "    pb = pred_boxes.clone()\n",
    "    if pb.max() <= 1.01:\n",
    "        pb[:,0] = pb[:,0] * MODEL_INPUT_W\n",
    "        pb[:,1] = pb[:,1] * MODEL_INPUT_H\n",
    "        pb[:,2] = pb[:,2] * MODEL_INPUT_W\n",
    "        pb[:,3] = pb[:,3] * MODEL_INPUT_H\n",
    "    xyxy = box_convert(pb, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "    return xyxy.cpu()\n",
    "\n",
    "def paste_patch_centered(base_img: torch.Tensor, patch_tensor: torch.Tensor, center_xy: tuple):\n",
    "    \"\"\"\n",
    "    base_img: [3,H,W] float on device\n",
    "    patch_tensor: [1,3,ph,pw] or [3,ph,pw] on same device\n",
    "    center_xy: (cx, cy) pixel coords (float)\n",
    "    returns new image [3,H,W] (device) with patch pasted (non-inplace)\n",
    "    \"\"\"\n",
    "    if patch_tensor.dim() == 4 and patch_tensor.shape[0] == 1:\n",
    "        p = patch_tensor[0]\n",
    "    elif patch_tensor.dim() == 3:\n",
    "        p = patch_tensor\n",
    "    else:\n",
    "        raise ValueError(\"invalid patch shape\")\n",
    "\n",
    "    ph, pw = p.shape[1], p.shape[2]\n",
    "    cx, cy = int(round(center_xy[0])), int(round(center_xy[1]))\n",
    "    x0 = cx - pw // 2\n",
    "    y0 = cy - ph // 2\n",
    "\n",
    "    H, W = base_img.shape[1], base_img.shape[2]\n",
    "    src_x0, src_y0 = 0, 0\n",
    "    dst_x0, dst_y0 = x0, y0\n",
    "    dst_x1, dst_y1 = x0 + pw, y0 + ph\n",
    "\n",
    "    if dst_x0 < 0:\n",
    "        src_x0 = -dst_x0; dst_x0 = 0\n",
    "    if dst_y0 < 0:\n",
    "        src_y0 = -dst_y0; dst_y0 = 0\n",
    "    if dst_x1 > W:\n",
    "        dst_x1 = W\n",
    "    if dst_y1 > H:\n",
    "        dst_y1 = H\n",
    "\n",
    "    out_w = dst_x1 - dst_x0\n",
    "    out_h = dst_y1 - dst_y0\n",
    "    if out_w <= 0 or out_h <= 0:\n",
    "        return base_img.clone()\n",
    "\n",
    "    src_x1 = src_x0 + out_w\n",
    "    src_y1 = src_y0 + out_h\n",
    "    p_cropped = p[:, src_y0:src_y1, src_x0:src_x1]\n",
    "\n",
    "    new_img = base_img.clone()\n",
    "    mask = (p_cropped > 0).float()\n",
    "    new_img[:, dst_y0:dst_y1, dst_x0:dst_x1] = new_img[:, dst_y0:dst_y1, dst_x0:dst_x1] * (1.0 - (mask>0).float()) + p_cropped * (mask>0).float()\n",
    "    return new_img\n",
    "\n",
    "# -----------------------\n",
    "# Data and model init\n",
    "# -----------------------\n",
    "dataloader = get_inria_dataloader(DATA_ROOT, split=\"Train\", batch_size=BATCH_SIZE, num_workers=0, disable_random_aug=True)\n",
    "\n",
    "model = load_detr_r50().to(DEVICE)\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "tmm = TransformerMaskingMatrix(num_enc_layers=6, num_dec_layers=6, p_base=0.2, sampling_strategy='categorical', device=DEVICE)\n",
    "tmm.register_hooks(model)\n",
    "tmm.reset_grad_history()\n",
    "\n",
    "gse = GradientSelfEnsemble(model=model, device=DEVICE)\n",
    "loss_fn = BlackBoxLoss(gse=gse, target_class=TARGET_CLASS_IDX,\n",
    "                       detection_weight=1.0, tv_weight=1e-3, l2_weight=0.0,\n",
    "                       layer_aggregation='per_layer_loss', use_sigmoid_for_binary=False,\n",
    "                       device=DEVICE)\n",
    "\n",
    "# global square patch\n",
    "patch = torch.randn(1, 3, PATCH_BASE, PATCH_BASE, device=DEVICE) * PATCH_INIT_STD + 0.7\n",
    "patch = patch.clamp(0.0, 1.0)\n",
    "patch.requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([patch], lr=0.005)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop with NMS & fallback control\n",
    "# -----------------------\n",
    "print(\"Start training with NMS de-dup and controlled fallback...\")\n",
    "it = 0\n",
    "for epoch in range(1):\n",
    "    for batch_idx, (imgs, _) in enumerate(dataloader):\n",
    "        if it >= NUM_ITERS:\n",
    "            break\n",
    "        imgs = imgs.to(DEVICE).clamp(0,1)\n",
    "        B = imgs.shape[0]\n",
    "\n",
    "        # --- STEP A: clean DETR detection (remove TMM hooks)\n",
    "        tmm.remove_hooks()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                det_out = model(imgs)\n",
    "            except Exception:\n",
    "                det_out = model(NestedTensor(imgs))\n",
    "\n",
    "        batch_boxes_all = []  # list of CPU tensors per image\n",
    "        for bi in range(B):\n",
    "            logits = det_out['pred_logits'][bi]  # [Q,C]\n",
    "            boxes = det_out['pred_boxes'][bi]    # [Q,4]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            cls_scores = probs[..., TARGET_CLASS_IDX]  # [Q]\n",
    "\n",
    "            # candidate indices above SCORE_THRESH\n",
    "            keep_idx = (cls_scores > SCORE_THRESH).nonzero(as_tuple=False).squeeze(1) if (cls_scores > SCORE_THRESH).any() else torch.tensor([], dtype=torch.long, device=cls_scores.device)\n",
    "\n",
    "            # fallback logic: only fallback if highest-scoring query >= FALLBACK_SCORE_THRESH\n",
    "            if keep_idx.numel() == 0 and FALLBACK_TO_TOP:\n",
    "                top_score, top_idx = torch.max(cls_scores, dim=0)\n",
    "                if top_score.item() >= FALLBACK_SCORE_THRESH:\n",
    "                    keep_idx = top_idx.unsqueeze(0)\n",
    "                else:\n",
    "                    keep_idx = torch.tensor([], dtype=torch.long, device=cls_scores.device)\n",
    "\n",
    "            if keep_idx.numel() == 0:\n",
    "                batch_boxes_all.append(torch.empty((0,4), dtype=torch.float32))\n",
    "                continue\n",
    "\n",
    "            sel_boxes = boxes[keep_idx]  # [K,4] (cxcywh, may be normalized)\n",
    "            sel_scores = cls_scores[keep_idx].detach()  # keep on same device as boxes\n",
    "\n",
    "            # convert to xyxy pixel coords on CPU for NMS (we can nms on device too)\n",
    "            sel_xyxy = detr_boxes_to_xyxy_pixel(sel_boxes.detach().cpu())  # CPU\n",
    "            # filter out tiny boxes before NMS\n",
    "            widths = (sel_xyxy[:,2] - sel_xyxy[:,0])\n",
    "            heights = (sel_xyxy[:,3] - sel_xyxy[:,1])\n",
    "            large_mask = (widths >= MIN_BOX_SIDE) & (heights >= MIN_BOX_SIDE)\n",
    "            if large_mask.sum() == 0:\n",
    "                batch_boxes_all.append(torch.empty((0,4), dtype=torch.float32))\n",
    "                continue\n",
    "            sel_xyxy = sel_xyxy[large_mask]\n",
    "            sel_scores_cpu = sel_scores.detach().cpu()[large_mask]\n",
    "\n",
    "            # NMS: needs tensors on same device; we will run on CPU\n",
    "            try:\n",
    "                keep_nms = nms(sel_xyxy, sel_scores_cpu, IOU_NMS_THRESH)\n",
    "            except Exception:\n",
    "                # if nms complains about dtype/device, move to CPU\n",
    "                keep_nms = nms(sel_xyxy.cpu(), sel_scores_cpu.cpu(), IOU_NMS_THRESH)\n",
    "\n",
    "            sel_xyxy_nms = sel_xyxy[keep_nms]\n",
    "            batch_boxes_all.append(sel_xyxy_nms)  # CPU tensor\n",
    "\n",
    "        # Save orig and orig boxes (visualize first sample)\n",
    "        save_image(detach_cpu(imgs[0]), os.path.join(SAVE_DIR, f\"iter_{it+1}_orig.png\"))\n",
    "        boxes0_cpu = batch_boxes_all[0] if len(batch_boxes_all)>0 else torch.empty((0,4))\n",
    "        img_orig_v = draw_boxes_on_tensor(detach_cpu(imgs[0]), boxes0_cpu)\n",
    "        save_image(img_orig_v, os.path.join(SAVE_DIR, f\"iter_{it+1}_orig_boxes.png\"))\n",
    "\n",
    "        # --- STEP B: enable TMM and build patched images by pasting patch on all selected boxes\n",
    "        tmm.register_hooks(model)\n",
    "        patched = imgs.clone()\n",
    "        for bi in range(B):\n",
    "            sel_boxes_cpu = batch_boxes_all[bi]  # CPU [K,4]\n",
    "            if sel_boxes_cpu.numel() == 0:\n",
    "                continue\n",
    "            sel_boxes_dev = sel_boxes_cpu.to(DEVICE)\n",
    "            for box in sel_boxes_dev:\n",
    "                xmin, ymin, xmax, ymax = box.tolist()\n",
    "                box_w = max(int(xmax - xmin), 1)\n",
    "                box_h = max(int(ymax - ymin), 1)\n",
    "                short = min(box_w, box_h)\n",
    "                side = max(MIN_PATCH_PX, int(round(short * PATCH_RATIO)))\n",
    "                side = max(1, side)\n",
    "                patch_resized = interpolate(patch, size=(side, side), mode='bilinear', align_corners=False)\n",
    "                cx = (xmin + xmax) / 2.0\n",
    "                cy = (ymin + ymax) / 2.0\n",
    "                patched_img = paste_patch_centered(patched[bi], patch_resized, center_xy=(cx, cy))\n",
    "                patched[bi] = patched_img\n",
    "\n",
    "        # --- STEP C: compute loss and update patch\n",
    "        loss_dict = loss_fn(imgs, patched, patch_tensor=patch)\n",
    "        total_loss = loss_dict['total_loss']\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            patch.clamp_(0.0, 1.0)\n",
    "\n",
    "        # Save patched and patch (first sample)\n",
    "        save_image(detach_cpu(patched[0]), os.path.join(SAVE_DIR, f\"iter_{it+1}_patched.png\"))\n",
    "        save_image(patch[0].detach().cpu(), os.path.join(SAVE_DIR, f\"iter_{it+1}_patch.png\"))\n",
    "\n",
    "        # --- STEP D: optional clean DETR on patched image and save boxes for comparison\n",
    "        tmm.remove_hooks()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                det_out_p = model(patched)\n",
    "            except Exception:\n",
    "                det_out_p = model(NestedTensor(patched))\n",
    "\n",
    "        # use first image predictions to visualize\n",
    "        logits_p = det_out_p['pred_logits'][0]\n",
    "        boxes_p = det_out_p['pred_boxes'][0]\n",
    "        probs_p = torch.softmax(logits_p, dim=-1)\n",
    "        cls_scores_p = probs_p[..., TARGET_CLASS_IDX]\n",
    "        keep_idx_p = (cls_scores_p > SCORE_THRESH).nonzero(as_tuple=False).squeeze(1) if (cls_scores_p > SCORE_THRESH).any() else torch.tensor([], dtype=torch.long, device=cls_scores_p.device)\n",
    "        if keep_idx_p.numel() == 0 and FALLBACK_TO_TOP:\n",
    "            top_score_p, top_idx_p = torch.max(cls_scores_p, dim=0)\n",
    "            if top_score_p.item() >= FALLBACK_SCORE_THRESH:\n",
    "                keep_idx_p = top_idx_p.unsqueeze(0)\n",
    "            else:\n",
    "                keep_idx_p = torch.tensor([], dtype=torch.long, device=cls_scores_p.device)\n",
    "\n",
    "        if keep_idx_p.numel() == 0:\n",
    "            boxes_p_xyxy = torch.empty((0,4))\n",
    "        else:\n",
    "            sel_boxes_p = boxes_p[keep_idx_p]\n",
    "            sel_xyxy_p = detr_boxes_to_xyxy_pixel(sel_boxes_p.detach().cpu())\n",
    "            # filter tiny + NMS for patched boxes (cpu)\n",
    "            widths_p = (sel_xyxy_p[:,2] - sel_xyxy_p[:,0])\n",
    "            heights_p = (sel_xyxy_p[:,3] - sel_xyxy_p[:,1])\n",
    "            large_mask_p = (widths_p >= MIN_BOX_SIDE) & (heights_p >= MIN_BOX_SIDE)\n",
    "            if large_mask_p.sum() == 0:\n",
    "                boxes_p_xyxy = torch.empty((0,4))\n",
    "            else:\n",
    "                sel_xyxy_p = sel_xyxy_p[large_mask_p]\n",
    "                sel_scores_p = cls_scores_p[keep_idx_p].detach().cpu()[large_mask_p]\n",
    "                keep_nms_p = nms(sel_xyxy_p, sel_scores_p, IOU_NMS_THRESH)\n",
    "                boxes_p_xyxy = sel_xyxy_p[keep_nms_p]\n",
    "\n",
    "        img_patched_v = draw_boxes_on_tensor(detach_cpu(patched[0]), boxes_p_xyxy)\n",
    "        save_image(img_patched_v, os.path.join(SAVE_DIR, f\"iter_{it+1}_patched_boxes.png\"))\n",
    "\n",
    "        print(f\"[iter {it+1}] total_loss={total_loss.item():.6f} | det_loss={loss_dict['det_loss'].item():.6f} | tv={loss_dict['tv_loss'].item():.6f}\")\n",
    "        it += 1\n",
    "\n",
    "    if it >= NUM_ITERS:\n",
    "        break\n",
    "\n",
    "# cleanup\n",
    "tmm.remove_hooks()\n",
    "print(\"Done. Visuals saved to\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bd39d-5dc3-4388-8495-ec4980c83f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d16a785-c339-4b8b-99da-e4d20041cc7c",
   "metadata": {},
   "source": [
    "# 正式train非demo：GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc6d19-bc69-41d8-9311-f851ac02d37a",
   "metadata": {},
   "source": [
    "## 需要先修改tmm："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5aab573-92bf-4490-befe-5106adf3d757",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tmm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tmm.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Dict, Literal\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "\n",
    "class NestedTensor:\n",
    "    \"\"\"匹配DETR的NestedTensor属性（复数tensors）\"\"\"\n",
    "    def __init__(self, tensors: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        self.tensors = tensors  # 复数属性，匹配DETR调用\n",
    "        self.mask = mask if mask is not None else torch.zeros(\n",
    "            (tensors.shape[0], tensors.shape[2], tensors.shape[3]), \n",
    "            dtype=torch.bool, \n",
    "            device=tensors.device\n",
    "        )\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.tensors.device\n",
    "\n",
    "\n",
    "class TransformerMaskingMatrix(nn.Module):\n",
    "    \"\"\"严格对齐《BlackBox》论文3.1节TMM模块（保留梯度传播）\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_enc_layers: int = 6,\n",
    "        num_dec_layers: int = 6,\n",
    "        p_base: float = 0.2,\n",
    "        sampling_strategy: Literal['categorical', 'bernoulli'] = 'categorical',\n",
    "        device: Optional[torch.device] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_enc_layers = num_enc_layers\n",
    "        self.num_dec_layers = num_dec_layers\n",
    "        self.p_base = p_base\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "\n",
    "        if self.sampling_strategy not in ['categorical', 'bernoulli']:\n",
    "            raise ValueError(f\"采样策略仅支持'categorical'和'bernoulli'，当前为{self.sampling_strategy}\")\n",
    "\n",
    "        self.grad_history: Dict[str, torch.Tensor] = {}\n",
    "        self.hooks: List[torch.utils.hooks.RemovableHandle] = []\n",
    "\n",
    "    def _categorical_mask_sampling(self, grad_abs: torch.Tensor) -> torch.Tensor:\n",
    "        grad_flat = grad_abs.flatten()\n",
    "        total_grad = grad_flat.sum()\n",
    "        num_elements = grad_flat.numel()\n",
    "\n",
    "        if total_grad < 1e-8:\n",
    "            prob_dist = torch.ones_like(grad_flat) / num_elements\n",
    "        else:\n",
    "            prob_dist = grad_flat / total_grad\n",
    "\n",
    "        num_to_mask = max(1, int(self.p_base * num_elements))\n",
    "        indices = torch.multinomial(prob_dist, num_to_mask, replacement=False)\n",
    "        mask_flat = torch.ones_like(grad_flat)\n",
    "        mask_flat = mask_flat.scatter_(0, indices, 0.0)\n",
    "\n",
    "        return mask_flat.view(grad_abs.shape).contiguous()\n",
    "\n",
    "    def _apply_mask_to_input(self, input_tensor: torch.Tensor, layer_key: str) -> torch.Tensor:\n",
    "        input_tensor = input_tensor.clone().contiguous()\n",
    "        input_dim = input_tensor.dim()\n",
    "    \n",
    "        if input_dim == 4:  # (B, C, H, W)\n",
    "            B, C, H, W = input_tensor.shape\n",
    "            input_seq = input_tensor.flatten(2).permute(2, 0, 1).contiguous()  # (seq_len, B, C)\n",
    "            masked_seq_list = []\n",
    "    \n",
    "            for b in range(B):\n",
    "                S_len = input_seq.shape[0]  # 当前样本 seq_len\n",
    "                mask = torch.rand(S_len, 1, C, device=input_tensor.device) > self.p_base\n",
    "                mask = mask.float()\n",
    "                masked_seq = input_seq[:, b:b+1, :] * mask\n",
    "                masked_seq_list.append(masked_seq)\n",
    "    \n",
    "            masked_seq = torch.cat(masked_seq_list, dim=1)  # (seq_len, B, C)\n",
    "            return masked_seq.permute(1, 2, 0).view(B, C, H, W).contiguous()\n",
    "    \n",
    "        elif input_dim == 3:  # (B, S, C)\n",
    "            B, S, C = input_tensor.shape\n",
    "            masked_list = []\n",
    "            for b in range(B):\n",
    "                mask = torch.rand(S, C, device=input_tensor.device) > self.p_base\n",
    "                mask = mask.float()\n",
    "                masked_list.append(input_tensor[b] * mask)\n",
    "            return torch.stack(masked_list, dim=0)\n",
    "    \n",
    "        else:\n",
    "            raise ValueError(f\"不支持的输入维度：{input_dim}\")\n",
    "\n",
    "    def _register_layer_hooks(self, layers: nn.ModuleList, prefix: str):\n",
    "        for layer_idx, layer in enumerate(layers):\n",
    "            layer_key = f\"{prefix}_{layer_idx}\"\n",
    "\n",
    "            def backward_hook(module, grad_in, grad_out, key=layer_key):\n",
    "                if grad_in[0] is not None:\n",
    "                    # 存储梯度时仍需detach（不影响传播链）\n",
    "                    self.grad_history[key] = grad_in[0].abs().detach().clone().contiguous()\n",
    "\n",
    "            def forward_hook(module, args, key=layer_key):\n",
    "                input_tensor = args[0]\n",
    "                return (self._apply_mask_to_input(input_tensor, key),) + args[1:]\n",
    "\n",
    "            self.hooks.append(layer.register_full_backward_hook(backward_hook, prepend=False))\n",
    "            self.hooks.append(layer.register_forward_pre_hook(forward_hook))\n",
    "\n",
    "    def register_hooks(self, model: nn.Module):\n",
    "        self.remove_hooks()\n",
    "        base_model = getattr(model, 'module', model)\n",
    "\n",
    "        assert hasattr(base_model, \"transformer\"), \"模型必须包含transformer属性\"\n",
    "        assert len(base_model.transformer.encoder.layers) >= self.num_enc_layers, \"encoder层数不足\"\n",
    "        assert len(base_model.transformer.decoder.layers) >= self.num_dec_layers, \"decoder层数不足\"\n",
    "\n",
    "        self._register_layer_hooks(base_model.transformer.encoder.layers, prefix=\"enc\")\n",
    "        self._register_layer_hooks(base_model.transformer.decoder.layers, prefix=\"dec\")\n",
    "\n",
    "        print(f\"✅ TMM已注册{len(self.hooks)}个hook（{self.num_enc_layers} encoder + {self.num_dec_layers} decoder）\")\n",
    "        print(f\"✅ 采样策略：{self.sampling_strategy}（符合论文设置）\")\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        print(\"✅ TMM已移除所有hook\")\n",
    "\n",
    "    def reset_grad_history(self):\n",
    "        self.grad_history.clear()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"TMM通过register_hooks()注入掩码，无需调用forward\")\n",
    "\n",
    "\n",
    "def load_detr_r50():\n",
    "    \"\"\"加载DETR-R50模型\"\"\"\n",
    "    model = torch.hub.load(\n",
    "        \"facebookresearch/detr:main\",\n",
    "        \"detr_resnet50\",\n",
    "        pretrained=False,\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "    weight_url = \"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\"\n",
    "    checkpoint = load_state_dict_from_url(weight_url, progress=True)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    model = model.cuda().train()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # 冻结模型参数，只优化补丁\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_blackbox_whitebox_demo():\n",
    "    # 1. 加载模型\n",
    "    print(\"正在加载DETR-R50模型...\")\n",
    "    model = load_detr_r50()\n",
    "    print(\"✅ DETR-R50模型加载完成\")\n",
    "\n",
    "    # 2. 初始化TMM\n",
    "    tmm = TransformerMaskingMatrix(\n",
    "        num_enc_layers=6,\n",
    "        num_dec_layers=6,\n",
    "        p_base=0.2,\n",
    "        sampling_strategy='categorical',\n",
    "        device='cuda'\n",
    "    )\n",
    "    tmm.register_hooks(model)\n",
    "\n",
    "    # 3. 初始化补丁（需要梯度）和优化器\n",
    "    patch = torch.randn(1, 3, 300, 300, device='cuda', requires_grad=True)  # 关键：requires_grad=True\n",
    "    optimizer = torch.optim.Adam([patch], lr=0.005)  # 优化器绑定patch\n",
    "\n",
    "    # 4. 模拟输入图像（无需梯度）\n",
    "    img = torch.randn(1, 3, 800, 800, device='cuda').clone().contiguous()\n",
    "    img.requires_grad = False\n",
    "\n",
    "    # 5. 优化循环\n",
    "    for iter in range(5):\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        tmm.reset_grad_history()\n",
    "\n",
    "        # 生成掩码（无需梯度）\n",
    "        mask = torch.zeros_like(img, device='cuda').clone().contiguous()\n",
    "        mask[:, :, 100:400, 100:400] = 1.0\n",
    "\n",
    "        # 补丁填充（保留梯度，移除detach()）\n",
    "        padded_patch = torch.nn.functional.pad(patch, (100, 400, 100, 400)).clone().contiguous()\n",
    "\n",
    "        # 生成patched_img（保留梯度传播链）\n",
    "        patched_img = torch.empty_like(img, device='cuda')\n",
    "        fusion_result = img * (1 - mask) + padded_patch * mask  # 融合逻辑（保留梯度）\n",
    "        patched_img.copy_(fusion_result.clone().contiguous())  # 仅clone，不detach\n",
    "        patched_img.requires_grad_(True)  # 确保启用梯度\n",
    "\n",
    "        # 构造NestedTensor输入模型\n",
    "        nested_patched_img = NestedTensor(tensors=patched_img)\n",
    "        outputs = model(nested_patched_img)\n",
    "\n",
    "        # 计算损失（行人类别置信度）\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        person_confidence = torch.sigmoid(pred_logits[..., 1]).mean()\n",
    "        loss = person_confidence  # 目标：降低行人置信度\n",
    "\n",
    "        # 反向传播（此时梯度链已连通）\n",
    "        loss.backward()  # 现在loss能找到需要梯度的patch\n",
    "        optimizer.step()\n",
    "\n",
    "        # 补丁裁剪\n",
    "        with torch.no_grad():\n",
    "            patch.data = torch.clamp(patch.data, -2.1179, 2.6400)\n",
    "\n",
    "        print(f\"迭代{iter+1}/5 | 行人置信度损失: {loss.item():.4f} | 梯度历史数: {len(tmm.grad_history)}\")\n",
    "\n",
    "    tmm.remove_hooks()\n",
    "    print(\"\\n✅ 白盒实验核心流程验证完成（梯度传播正常）\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    run_blackbox_whitebox_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e03969-1387-4d2f-a530-b173463200ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TMM已移除所有hook\n",
      "✅ TMM已注册24个hook（6 encoder + 6 decoder）\n",
      "✅ 采样策略：categorical（符合论文设置）\n",
      "Start training (TMM enabled during all forwards). Saving to: /opt/data/private/BlackBox/save/demo\n",
      "[epoch 1 batch 0] total_loss=0.039853 | det_loss=0.039079 | tv=0.774092 | nps=0.000000 | grad_norm=0.005961076822131872 | selected_counts=[3, 1, 8, 3, 9, 1, 5, 12]\n",
      "[epoch 1 batch 1] total_loss=0.066802 | det_loss=0.066030 | tv=0.771549 | nps=0.000000 | grad_norm=0.00907645933330059 | selected_counts=[6, 1, 1, 1, 5, 17, 1, 2]\n",
      "[epoch 1 batch 2] total_loss=0.036438 | det_loss=0.035668 | tv=0.770258 | nps=0.000000 | grad_norm=0.00730087049305439 | selected_counts=[1, 3, 7, 3, 3, 10, 1, 5]\n",
      "[epoch 1 batch 3] total_loss=0.037939 | det_loss=0.037169 | tv=0.769276 | nps=0.000000 | grad_norm=0.007412371225655079 | selected_counts=[1, 1, 1, 10, 4, 4, 8, 1]\n",
      "[epoch 1 batch 4] total_loss=0.060026 | det_loss=0.059258 | tv=0.768370 | nps=0.000000 | grad_norm=0.007400457747280598 | selected_counts=[5, 8, 3, 3, 2, 3, 7, 0]\n",
      "[epoch 1 batch 5] total_loss=0.038110 | det_loss=0.037343 | tv=0.767576 | nps=0.000000 | grad_norm=0.006573542486876249 | selected_counts=[11, 7, 8, 3, 3, 2, 5, 8]\n",
      "[epoch 1 batch 6] total_loss=0.052558 | det_loss=0.051791 | tv=0.766864 | nps=0.000000 | grad_norm=0.00611809641122818 | selected_counts=[1, 1, 4, 8, 2, 5, 14, 7]\n",
      "[epoch 1 batch 7] total_loss=0.040339 | det_loss=0.039573 | tv=0.766214 | nps=0.000000 | grad_norm=0.003860488999634981 | selected_counts=[1, 7, 1, 3, 7, 2, 1, 1]\n",
      "[epoch 1 batch 8] total_loss=0.060376 | det_loss=0.059610 | tv=0.765610 | nps=0.000000 | grad_norm=0.0074568199925124645 | selected_counts=[2, 6, 14, 12, 9, 2, 17, 6]\n",
      "[epoch 1 batch 9] total_loss=0.047471 | det_loss=0.046706 | tv=0.765043 | nps=0.000000 | grad_norm=0.005975717678666115 | selected_counts=[1, 11, 1, 11, 3, 1, 5, 5]\n",
      "[epoch 1 batch 10] total_loss=0.044318 | det_loss=0.043553 | tv=0.764514 | nps=0.000000 | grad_norm=0.006531438324600458 | selected_counts=[5, 1, 10, 3, 11, 1, 2, 6]\n",
      "[epoch 1 batch 11] total_loss=0.048051 | det_loss=0.047287 | tv=0.764011 | nps=0.000000 | grad_norm=0.0069648330099880695 | selected_counts=[5, 8, 1, 3, 4, 4, 11, 12]\n",
      "[epoch 1 batch 12] total_loss=0.047447 | det_loss=0.046684 | tv=0.763521 | nps=0.000000 | grad_norm=0.004712129011750221 | selected_counts=[2, 1, 12, 6, 1, 1, 1, 2]\n",
      "[epoch 1 batch 13] total_loss=0.058376 | det_loss=0.057613 | tv=0.763053 | nps=0.000000 | grad_norm=0.006308217532932758 | selected_counts=[2, 8, 6, 4, 1, 2, 6, 2]\n",
      "[epoch 1 batch 14] total_loss=0.037700 | det_loss=0.036937 | tv=0.762595 | nps=0.000000 | grad_norm=0.004599687643349171 | selected_counts=[2, 4, 4, 2, 11, 8, 12, 0]\n",
      "[epoch 1 batch 15] total_loss=0.028462 | det_loss=0.027700 | tv=0.762156 | nps=0.000000 | grad_norm=0.005427265074104071 | selected_counts=[5, 11, 3, 1, 6, 9, 2, 1]\n",
      "[epoch 1 batch 16] total_loss=0.034793 | det_loss=0.034032 | tv=0.761733 | nps=0.000000 | grad_norm=0.005914354231208563 | selected_counts=[2, 11, 1, 1, 5, 4, 12, 5]\n",
      "[epoch 1 batch 17] total_loss=0.033107 | det_loss=0.032346 | tv=0.761314 | nps=0.000000 | grad_norm=0.0036575004924088717 | selected_counts=[10, 5, 8, 1, 4, 1, 12, 2]\n",
      "[epoch 1 batch 18] total_loss=0.039662 | det_loss=0.038901 | tv=0.760912 | nps=0.000000 | grad_norm=0.008885679766535759 | selected_counts=[5, 10, 2, 1, 1, 5, 1, 4]\n",
      "[epoch 1 batch 19] total_loss=0.043482 | det_loss=0.042721 | tv=0.760515 | nps=0.000000 | grad_norm=0.01026245579123497 | selected_counts=[6, 1, 3, 13, 4, 2, 10, 1]\n",
      "[epoch 1 batch 20] total_loss=0.043196 | det_loss=0.042436 | tv=0.760119 | nps=0.000000 | grad_norm=0.004524547141045332 | selected_counts=[2, 6, 4, 1, 1, 3, 2, 2]\n",
      "[epoch 1 batch 21] total_loss=0.040005 | det_loss=0.039245 | tv=0.759728 | nps=0.000000 | grad_norm=0.0037031001411378384 | selected_counts=[6, 2, 2, 1, 1, 1, 1, 1]\n",
      "[epoch 1 batch 22] total_loss=0.021173 | det_loss=0.020414 | tv=0.759356 | nps=0.000000 | grad_norm=0.0028731897473335266 | selected_counts=[2, 16, 5, 4, 7, 1, 1, 3]\n",
      "[epoch 1 batch 23] total_loss=0.042139 | det_loss=0.041380 | tv=0.758997 | nps=0.000000 | grad_norm=0.004671491216868162 | selected_counts=[6, 7, 3, 7, 5, 2, 2, 1]\n",
      "[epoch 1 batch 24] total_loss=0.030420 | det_loss=0.029662 | tv=0.758647 | nps=0.000000 | grad_norm=0.006753310561180115 | selected_counts=[1, 16, 1, 10, 8, 6, 10, 2]\n",
      "[epoch 1 batch 25] total_loss=0.065290 | det_loss=0.064531 | tv=0.758300 | nps=0.000000 | grad_norm=0.0087201576679945 | selected_counts=[3, 6, 6, 1, 3, 6, 6, 1]\n",
      "[epoch 1 batch 26] total_loss=0.029064 | det_loss=0.028306 | tv=0.757942 | nps=0.000000 | grad_norm=0.004946614149957895 | selected_counts=[3, 2, 6, 5, 1, 6, 4, 1]\n",
      "[epoch 1 batch 27] total_loss=0.033675 | det_loss=0.032917 | tv=0.757584 | nps=0.000000 | grad_norm=0.006122302729636431 | selected_counts=[4, 7, 4, 4, 5, 2, 4, 10]\n",
      "[epoch 1 batch 28] total_loss=0.034200 | det_loss=0.033443 | tv=0.757226 | nps=0.000000 | grad_norm=0.004378109704703093 | selected_counts=[1, 13, 2, 3, 2, 2, 8, 6]\n",
      "[epoch 1 batch 29] total_loss=0.034866 | det_loss=0.034109 | tv=0.756879 | nps=0.000000 | grad_norm=0.0035703424364328384 | selected_counts=[4, 5, 2, 2, 3, 1, 6, 1]\n",
      "[epoch 1 batch 30] total_loss=0.036202 | det_loss=0.035446 | tv=0.756541 | nps=0.000000 | grad_norm=0.006777063012123108 | selected_counts=[2, 8, 5, 4, 2, 8, 10, 2]\n",
      "[epoch 1 batch 31] total_loss=0.052153 | det_loss=0.051396 | tv=0.756211 | nps=0.000000 | grad_norm=0.0069181169383227825 | selected_counts=[2, 1, 2, 4, 3, 1, 11, 3]\n",
      "[epoch 1 batch 32] total_loss=0.040270 | det_loss=0.039514 | tv=0.755878 | nps=0.000000 | grad_norm=0.006267608143389225 | selected_counts=[1, 16, 1, 1, 1, 6, 7, 2]\n",
      "[epoch 1 batch 33] total_loss=0.033046 | det_loss=0.032290 | tv=0.755545 | nps=0.000000 | grad_norm=0.004051785916090012 | selected_counts=[6, 2, 13, 1, 9, 3, 3, 1]\n",
      "[epoch 1 batch 34] total_loss=0.049697 | det_loss=0.048942 | tv=0.755215 | nps=0.000000 | grad_norm=0.00773537065833807 | selected_counts=[3, 1, 9, 6, 8, 5, 5, 3]\n",
      "[epoch 1 batch 35] total_loss=0.038456 | det_loss=0.037701 | tv=0.754873 | nps=0.000000 | grad_norm=0.006247044540941715 | selected_counts=[12, 7, 1, 5, 13, 6, 3, 2]\n",
      "[epoch 1 batch 36] total_loss=0.036063 | det_loss=0.035308 | tv=0.754521 | nps=0.000000 | grad_norm=0.0042661093175411224 | selected_counts=[1, 9, 6, 6, 1, 3, 3, 2]\n",
      "[epoch 1 batch 37] total_loss=0.039588 | det_loss=0.038834 | tv=0.754175 | nps=0.000000 | grad_norm=0.0038710767403244972 | selected_counts=[2, 3, 1, 5, 1, 2, 5, 1]\n",
      "[epoch 1 batch 38] total_loss=0.032031 | det_loss=0.031277 | tv=0.753838 | nps=0.000000 | grad_norm=0.0031228912994265556 | selected_counts=[3, 2, 2, 1, 2, 3, 1, 1]\n",
      "[epoch 1 batch 39] total_loss=0.035953 | det_loss=0.035199 | tv=0.753510 | nps=0.000000 | grad_norm=0.007644813507795334 | selected_counts=[9, 1, 1, 2, 8, 1, 6, 1]\n",
      "[epoch 1 batch 40] total_loss=0.026758 | det_loss=0.026005 | tv=0.753178 | nps=0.000000 | grad_norm=0.005353492684662342 | selected_counts=[1, 12, 12, 1, 9, 8, 7, 1]\n",
      "[epoch 1 batch 41] total_loss=0.034300 | det_loss=0.033547 | tv=0.752844 | nps=0.000000 | grad_norm=0.005441697780042887 | selected_counts=[7, 5, 6, 2, 15, 3, 0, 9]\n",
      "[epoch 1 batch 42] total_loss=0.034407 | det_loss=0.033654 | tv=0.752509 | nps=0.000000 | grad_norm=0.005211277864873409 | selected_counts=[1, 3, 3, 1, 8, 2, 3, 1]\n",
      "[epoch 1 batch 43] total_loss=0.033170 | det_loss=0.032418 | tv=0.752176 | nps=0.000000 | grad_norm=0.004204387776553631 | selected_counts=[2, 1, 6, 2, 2, 1, 2, 1]\n",
      "[epoch 1 batch 44] total_loss=0.027817 | det_loss=0.027066 | tv=0.751847 | nps=0.000000 | grad_norm=0.005656317342072725 | selected_counts=[5, 1, 2, 3, 8, 4, 8, 4]\n",
      "[epoch 1 batch 45] total_loss=0.036119 | det_loss=0.035368 | tv=0.751513 | nps=0.000000 | grad_norm=0.00437683193013072 | selected_counts=[0, 1, 3, 1, 4, 4, 4, 3]\n",
      "[epoch 1 batch 46] total_loss=0.035143 | det_loss=0.034392 | tv=0.751186 | nps=0.000000 | grad_norm=0.004208364058285952 | selected_counts=[2, 1, 1, 1, 1, 3, 2, 3]\n",
      "[epoch 1 batch 47] total_loss=0.053118 | det_loss=0.052368 | tv=0.750865 | nps=0.000000 | grad_norm=0.010617922991514206 | selected_counts=[5, 7, 3, 1, 4, 2, 8, 2]\n",
      "[epoch 1 batch 48] total_loss=0.029138 | det_loss=0.028388 | tv=0.750526 | nps=0.000000 | grad_norm=0.003965003415942192 | selected_counts=[1, 1, 1, 1, 6, 3, 5, 9]\n",
      "[epoch 1 batch 49] total_loss=0.038994 | det_loss=0.038244 | tv=0.750186 | nps=0.000000 | grad_norm=0.008452525362372398 | selected_counts=[6, 7, 8, 5, 3, 7, 9, 9]\n",
      "[epoch 1 batch 50] total_loss=0.044367 | det_loss=0.043617 | tv=0.749833 | nps=0.000000 | grad_norm=0.00600195350125432 | selected_counts=[1, 8, 8, 7, 8, 2, 4, 1]\n",
      "[epoch 1 batch 51] total_loss=0.034086 | det_loss=0.033337 | tv=0.749468 | nps=0.000000 | grad_norm=0.004856661893427372 | selected_counts=[0, 4, 1, 2, 1, 9, 5, 1]\n",
      "[epoch 1 batch 52] total_loss=0.038288 | det_loss=0.037538 | tv=0.749109 | nps=0.000000 | grad_norm=0.00493892515078187 | selected_counts=[8, 0, 1, 5, 5, 8, 3, 3]\n",
      "[epoch 1 batch 53] total_loss=0.041654 | det_loss=0.040906 | tv=0.748749 | nps=0.000000 | grad_norm=0.005606689024716616 | selected_counts=[6, 3, 3, 3, 1, 2, 4, 1]\n",
      "[epoch 1 batch 54] total_loss=0.040422 | det_loss=0.039674 | tv=0.748395 | nps=0.000000 | grad_norm=0.004390415735542774 | selected_counts=[1, 7, 5, 7, 2, 3, 4, 6]\n",
      "[epoch 1 batch 55] total_loss=0.033610 | det_loss=0.032862 | tv=0.748048 | nps=0.000000 | grad_norm=0.007307157386094332 | selected_counts=[7, 10, 7, 1, 2, 1, 4, 11]\n",
      "[epoch 1 batch 56] total_loss=0.036095 | det_loss=0.035347 | tv=0.747701 | nps=0.000000 | grad_norm=0.005564429797232151 | selected_counts=[4, 1, 2, 6, 4, 1, 1, 0]\n",
      "[epoch 1 batch 57] total_loss=0.035730 | det_loss=0.034983 | tv=0.747361 | nps=0.000000 | grad_norm=0.006947639398276806 | selected_counts=[7, 4, 4, 15, 5, 8, 4, 3]\n",
      "[epoch 1 batch 58] total_loss=0.049632 | det_loss=0.048885 | tv=0.747010 | nps=0.000000 | grad_norm=0.01130591332912445 | selected_counts=[4, 8, 3, 1, 2, 1, 2, 2]\n",
      "[epoch 1 batch 59] total_loss=0.036466 | det_loss=0.035720 | tv=0.746658 | nps=0.000000 | grad_norm=0.0082479789853096 | selected_counts=[1, 5, 1, 11, 1, 5, 3, 5]\n",
      "[epoch 1 batch 60] total_loss=0.049689 | det_loss=0.048942 | tv=0.746298 | nps=0.000000 | grad_norm=0.005534703377634287 | selected_counts=[4, 4, 2, 9, 7, 2, 2, 2]\n",
      "[epoch 1 batch 61] total_loss=0.036148 | det_loss=0.035402 | tv=0.745938 | nps=0.000000 | grad_norm=0.011334411799907684 | selected_counts=[3, 1, 10, 1, 4, 4, 3, 7]\n",
      "[epoch 1 batch 62] total_loss=0.034287 | det_loss=0.033542 | tv=0.745575 | nps=0.000000 | grad_norm=0.00607953080907464 | selected_counts=[1, 4, 9, 2, 7, 8, 3, 9]\n",
      "[epoch 1 batch 63] total_loss=0.036797 | det_loss=0.036052 | tv=0.745214 | nps=0.000000 | grad_norm=0.0058058565482497215 | selected_counts=[13, 2, 3, 4, 1, 4, 2, 12]\n",
      "[epoch 1 batch 64] total_loss=0.023839 | det_loss=0.023094 | tv=0.744858 | nps=0.000000 | grad_norm=0.003102889284491539 | selected_counts=[2, 12, 1, 1, 4, 8, 7, 1]\n",
      "[epoch 1 batch 65] total_loss=0.026354 | det_loss=0.025610 | tv=0.744516 | nps=0.000000 | grad_norm=0.007721350993961096 | selected_counts=[4, 6, 4, 8, 1, 3, 12, 2]\n",
      "[epoch 1 batch 66] total_loss=0.030883 | det_loss=0.030139 | tv=0.744175 | nps=0.000000 | grad_norm=0.006699492689222097 | selected_counts=[1, 3, 6, 4, 2, 6, 9, 8]\n",
      "[epoch 1 batch 67] total_loss=0.029074 | det_loss=0.028330 | tv=0.743828 | nps=0.000000 | grad_norm=0.004594975151121616 | selected_counts=[1, 6, 2, 11, 1, 2, 8, 3]\n",
      "[epoch 1 batch 68] total_loss=0.056788 | det_loss=0.056044 | tv=0.743488 | nps=0.000000 | grad_norm=0.00941777229309082 | selected_counts=[6, 4, 2, 6, 8, 3, 7, 10]\n",
      "[epoch 1 batch 69] total_loss=0.029707 | det_loss=0.028964 | tv=0.743144 | nps=0.000000 | grad_norm=0.0057840775698423386 | selected_counts=[1, 5, 1, 2, 1, 1, 3, 2]\n",
      "[epoch 1 batch 70] total_loss=0.019444 | det_loss=0.018701 | tv=0.742805 | nps=0.000000 | grad_norm=0.004077814985066652 | selected_counts=[1, 5, 4, 2, 8, 11, 2, 1]\n",
      "[epoch 1 batch 71] total_loss=0.026958 | det_loss=0.026215 | tv=0.742473 | nps=0.000000 | grad_norm=0.004655642434954643 | selected_counts=[2, 10, 4, 3, 12, 2, 1, 5]\n",
      "[epoch 1 batch 72] total_loss=0.037148 | det_loss=0.036406 | tv=0.742147 | nps=0.000000 | grad_norm=0.005763120949268341 | selected_counts=[3, 3, 1, 3, 1, 7, 1, 5]\n",
      "[epoch 1 batch 73] total_loss=0.048040 | det_loss=0.047298 | tv=0.741823 | nps=0.000000 | grad_norm=0.004904306959360838 | selected_counts=[3, 3, 1, 3, 1, 1, 1, 2]\n",
      "[epoch 1 batch 74] total_loss=0.037924 | det_loss=0.037182 | tv=0.741505 | nps=0.000000 | grad_norm=0.006184292491525412 | selected_counts=[6, 1, 3, 13, 2, 4, 1, 9]\n",
      "[epoch 1 batch 75] total_loss=0.041856 | det_loss=0.041114 | tv=0.741194 | nps=0.000000 | grad_norm=0.00509547907859087 | selected_counts=[4, 3, 4, 4, 6, 1, 3, 1]\n",
      "[epoch 1 batch 76] total_loss=0.034316 | det_loss=0.033575 | tv=0.740883 | nps=0.000000 | grad_norm=0.005064457654953003 | selected_counts=[3, 2, 3, 4, 4, 7]\n",
      "Epoch 1 saved patch snapshot.\n",
      "[epoch 2 batch 0] total_loss=0.017929 | det_loss=0.017188 | tv=0.740579 | nps=0.000000 | grad_norm=0.0025233980268239975 | selected_counts=[11, 1, 8, 1, 14, 8, 2, 1]\n",
      "[epoch 2 batch 1] total_loss=0.035797 | det_loss=0.035057 | tv=0.740287 | nps=0.000000 | grad_norm=0.004809483885765076 | selected_counts=[1, 1, 3, 2, 1, 1, 4, 4]\n",
      "[epoch 2 batch 2] total_loss=0.020930 | det_loss=0.020190 | tv=0.739999 | nps=0.000000 | grad_norm=0.0020802277140319347 | selected_counts=[2, 8, 1, 1, 9, 1, 1, 1]\n",
      "[epoch 2 batch 3] total_loss=0.033121 | det_loss=0.032382 | tv=0.739727 | nps=0.000000 | grad_norm=0.004185593221336603 | selected_counts=[0, 4, 2, 4, 2, 3, 1, 6]\n",
      "[epoch 2 batch 4] total_loss=0.032838 | det_loss=0.032099 | tv=0.739458 | nps=0.000000 | grad_norm=0.004142554942518473 | selected_counts=[2, 1, 4, 2, 4, 3, 1, 6]\n",
      "[epoch 2 batch 5] total_loss=0.034871 | det_loss=0.034132 | tv=0.739195 | nps=0.000000 | grad_norm=0.005616683978587389 | selected_counts=[7, 7, 7, 2, 3, 2, 6, 17]\n",
      "[epoch 2 batch 6] total_loss=0.037253 | det_loss=0.036514 | tv=0.738936 | nps=0.000000 | grad_norm=0.007741882931441069 | selected_counts=[3, 7, 2, 1, 6, 11, 5, 9]\n",
      "[epoch 2 batch 7] total_loss=0.022570 | det_loss=0.021832 | tv=0.738659 | nps=0.000000 | grad_norm=0.004123836290091276 | selected_counts=[1, 9, 1, 9, 11, 1, 4, 3]\n",
      "[epoch 2 batch 8] total_loss=0.041042 | det_loss=0.040304 | tv=0.738387 | nps=0.000000 | grad_norm=0.005310166627168655 | selected_counts=[4, 2, 3, 2, 1, 1, 1, 6]\n",
      "[epoch 2 batch 9] total_loss=0.032853 | det_loss=0.032115 | tv=0.738115 | nps=0.000000 | grad_norm=0.004433098714798689 | selected_counts=[3, 2, 2, 3, 1, 1, 2, 2]\n",
      "[epoch 2 batch 10] total_loss=0.028870 | det_loss=0.028133 | tv=0.737841 | nps=0.000000 | grad_norm=0.004365691915154457 | selected_counts=[3, 3, 0, 1, 4, 3, 1, 1]\n",
      "[epoch 2 batch 11] total_loss=0.017141 | det_loss=0.016403 | tv=0.737570 | nps=0.000000 | grad_norm=0.005102467257529497 | selected_counts=[1, 5, 6, 4, 6, 2, 3, 11]\n",
      "[epoch 2 batch 12] total_loss=0.027987 | det_loss=0.027250 | tv=0.737303 | nps=0.000000 | grad_norm=0.004395374096930027 | selected_counts=[6, 4, 5, 5, 1, 1, 2, 6]\n",
      "[epoch 2 batch 13] total_loss=0.014501 | det_loss=0.013764 | tv=0.737038 | nps=0.000000 | grad_norm=0.003468669718131423 | selected_counts=[2, 1, 13, 3, 4, 5, 8, 4]\n",
      "[epoch 2 batch 14] total_loss=0.021188 | det_loss=0.020451 | tv=0.736778 | nps=0.000000 | grad_norm=0.0032412465661764145 | selected_counts=[2, 17, 0, 6, 5, 5, 4, 14]\n",
      "[epoch 2 batch 15] total_loss=0.039367 | det_loss=0.038631 | tv=0.736531 | nps=0.000000 | grad_norm=0.005690773017704487 | selected_counts=[2, 10, 4, 7, 4, 2, 1, 4]\n",
      "[epoch 2 batch 16] total_loss=0.022154 | det_loss=0.021418 | tv=0.736280 | nps=0.000000 | grad_norm=0.004667118191719055 | selected_counts=[2, 4, 4, 4, 2, 2, 7, 4]\n",
      "[epoch 2 batch 17] total_loss=0.043081 | det_loss=0.042345 | tv=0.736030 | nps=0.000000 | grad_norm=0.004598333965986967 | selected_counts=[1, 8, 2, 9, 8, 3, 4, 2]\n",
      "[epoch 2 batch 18] total_loss=0.012863 | det_loss=0.012127 | tv=0.735778 | nps=0.000000 | grad_norm=0.002402720507234335 | selected_counts=[1, 7, 5, 1, 8, 1, 2, 7]\n",
      "[epoch 2 batch 19] total_loss=0.033388 | det_loss=0.032652 | tv=0.735536 | nps=0.000000 | grad_norm=0.004025084897875786 | selected_counts=[6, 2, 14, 1, 2, 4, 6, 0]\n",
      "[epoch 2 batch 20] total_loss=0.022003 | det_loss=0.021267 | tv=0.735298 | nps=0.000000 | grad_norm=0.005023142322897911 | selected_counts=[6, 2, 1, 7, 1, 9, 3, 6]\n",
      "[epoch 2 batch 21] total_loss=0.046250 | det_loss=0.045515 | tv=0.735053 | nps=0.000000 | grad_norm=0.004911850206553936 | selected_counts=[2, 2, 3, 1, 2, 1, 6, 10]\n",
      "[epoch 2 batch 22] total_loss=0.030444 | det_loss=0.029709 | tv=0.734801 | nps=0.000000 | grad_norm=0.007671770174056292 | selected_counts=[10, 12, 8, 5, 3, 11, 8, 2]\n",
      "[epoch 2 batch 23] total_loss=0.029574 | det_loss=0.028839 | tv=0.734536 | nps=0.000000 | grad_norm=0.004236448556184769 | selected_counts=[2, 5, 1, 1, 3, 1, 4, 2]\n",
      "[epoch 2 batch 24] total_loss=0.037187 | det_loss=0.036453 | tv=0.734269 | nps=0.000000 | grad_norm=0.0053965849801898 | selected_counts=[5, 9, 3, 9, 1, 8, 1, 1]\n",
      "[epoch 2 batch 25] total_loss=0.017256 | det_loss=0.016522 | tv=0.733997 | nps=0.000000 | grad_norm=0.003791768802329898 | selected_counts=[11, 5, 15, 7, 1, 5, 10, 7]\n",
      "[epoch 2 batch 26] total_loss=0.017638 | det_loss=0.016904 | tv=0.733726 | nps=0.000000 | grad_norm=0.0038258028216660023 | selected_counts=[1, 9, 4, 1, 1, 3, 1, 2]\n",
      "[epoch 2 batch 27] total_loss=0.014702 | det_loss=0.013969 | tv=0.733459 | nps=0.000000 | grad_norm=0.004185330588370562 | selected_counts=[4, 5, 1, 1, 6, 2, 9, 3]\n",
      "[epoch 2 batch 28] total_loss=0.019986 | det_loss=0.019253 | tv=0.733197 | nps=0.000000 | grad_norm=0.0038345116190612316 | selected_counts=[5, 3, 9, 1, 1, 7, 1, 1]\n",
      "[epoch 2 batch 29] total_loss=0.022437 | det_loss=0.021704 | tv=0.732942 | nps=0.000000 | grad_norm=0.005101571325212717 | selected_counts=[13, 2, 12, 2, 4, 3, 3, 7]\n",
      "[epoch 2 batch 30] total_loss=0.015440 | det_loss=0.014707 | tv=0.732686 | nps=0.000000 | grad_norm=0.0024819045793265104 | selected_counts=[3, 8, 1, 1, 3, 7, 1, 3]\n",
      "[epoch 2 batch 31] total_loss=0.035824 | det_loss=0.035092 | tv=0.732439 | nps=0.000000 | grad_norm=0.00569584034383297 | selected_counts=[2, 4, 4, 2, 2, 1, 1, 7]\n",
      "[epoch 2 batch 32] total_loss=0.031511 | det_loss=0.030779 | tv=0.732186 | nps=0.000000 | grad_norm=0.00326602254062891 | selected_counts=[2, 7, 4, 11, 0, 3, 1, 1]\n",
      "[epoch 2 batch 33] total_loss=0.041073 | det_loss=0.040341 | tv=0.731939 | nps=0.000000 | grad_norm=0.005548482295125723 | selected_counts=[1, 1, 9, 3, 2, 2, 2, 1]\n",
      "[epoch 2 batch 34] total_loss=0.030362 | det_loss=0.029630 | tv=0.731694 | nps=0.000000 | grad_norm=0.00534339202567935 | selected_counts=[2, 16, 7, 4, 10, 5, 7, 6]\n",
      "[epoch 2 batch 35] total_loss=0.032019 | det_loss=0.031288 | tv=0.731433 | nps=0.000000 | grad_norm=0.00452471012249589 | selected_counts=[0, 4, 6, 1, 4, 7, 2, 1]\n",
      "[epoch 2 batch 36] total_loss=0.027927 | det_loss=0.027196 | tv=0.731168 | nps=0.000000 | grad_norm=0.005364745389670134 | selected_counts=[2, 2, 3, 2, 1, 4, 1, 2]\n",
      "[epoch 2 batch 37] total_loss=0.024909 | det_loss=0.024178 | tv=0.730895 | nps=0.000000 | grad_norm=0.00436831172555685 | selected_counts=[14, 8, 8, 3, 2, 1, 6, 7]\n",
      "[epoch 2 batch 38] total_loss=0.021738 | det_loss=0.021008 | tv=0.730613 | nps=0.000000 | grad_norm=0.004767471458762884 | selected_counts=[1, 3, 3, 5, 6, 3, 1, 7]\n",
      "[epoch 2 batch 39] total_loss=0.024902 | det_loss=0.024171 | tv=0.730331 | nps=0.000000 | grad_norm=0.004647248424589634 | selected_counts=[2, 15, 6, 3, 3, 6, 7, 5]\n",
      "[epoch 2 batch 40] total_loss=0.028332 | det_loss=0.027602 | tv=0.730044 | nps=0.000000 | grad_norm=0.005211123265326023 | selected_counts=[4, 3, 3, 3, 2, 1, 1, 1]\n",
      "[epoch 2 batch 41] total_loss=0.019029 | det_loss=0.018300 | tv=0.729754 | nps=0.000000 | grad_norm=0.0029519330710172653 | selected_counts=[1, 7, 1, 3, 5, 5, 4, 1]\n",
      "[epoch 2 batch 42] total_loss=0.016329 | det_loss=0.015600 | tv=0.729470 | nps=0.000000 | grad_norm=0.002782014664262533 | selected_counts=[10, 13, 1, 9, 6, 7, 2, 2]\n",
      "[epoch 2 batch 43] total_loss=0.018036 | det_loss=0.017307 | tv=0.729194 | nps=0.000000 | grad_norm=0.00266944314353168 | selected_counts=[2, 0, 1, 5, 4, 1, 1, 1]\n",
      "[epoch 2 batch 44] total_loss=0.021021 | det_loss=0.020292 | tv=0.728929 | nps=0.000000 | grad_norm=0.003139728680253029 | selected_counts=[1, 2, 5, 5, 5, 1, 12, 2]\n",
      "[epoch 2 batch 45] total_loss=0.039570 | det_loss=0.038841 | tv=0.728674 | nps=0.000000 | grad_norm=0.005788950249552727 | selected_counts=[1, 4, 14, 7, 2, 2, 2, 4]\n",
      "[epoch 2 batch 46] total_loss=0.020630 | det_loss=0.019901 | tv=0.728418 | nps=0.000000 | grad_norm=0.002690312685444951 | selected_counts=[1, 2, 2, 2, 8, 12, 1, 2]\n",
      "[epoch 2 batch 47] total_loss=0.020242 | det_loss=0.019514 | tv=0.728167 | nps=0.000000 | grad_norm=0.0044973683543503284 | selected_counts=[6, 2, 1, 7, 8, 10, 2, 1]\n",
      "[epoch 2 batch 48] total_loss=0.030891 | det_loss=0.030163 | tv=0.727913 | nps=0.000000 | grad_norm=0.004090123809874058 | selected_counts=[1, 2, 1, 5, 7, 1, 2, 4]\n",
      "[epoch 2 batch 49] total_loss=0.017908 | det_loss=0.017181 | tv=0.727662 | nps=0.000000 | grad_norm=0.003240463323891163 | selected_counts=[1, 1, 1, 10, 4, 3, 1, 1]\n",
      "[epoch 2 batch 50] total_loss=0.018619 | det_loss=0.017892 | tv=0.727417 | nps=0.000000 | grad_norm=0.005366024561226368 | selected_counts=[4, 2, 2, 2, 18, 2, 1, 1]\n",
      "[epoch 2 batch 51] total_loss=0.023046 | det_loss=0.022319 | tv=0.727168 | nps=0.000000 | grad_norm=0.005168002564460039 | selected_counts=[2, 13, 3, 1, 5, 6, 2, 2]\n",
      "[epoch 2 batch 52] total_loss=0.020262 | det_loss=0.019535 | tv=0.726912 | nps=0.000000 | grad_norm=0.0036831714678555727 | selected_counts=[3, 10, 1, 9, 1, 12, 13, 9]\n",
      "[epoch 2 batch 53] total_loss=0.019153 | det_loss=0.018426 | tv=0.726658 | nps=0.000000 | grad_norm=0.0029982286505401134 | selected_counts=[2, 7, 1, 1, 7, 8, 1, 2]\n",
      "[epoch 2 batch 54] total_loss=0.030648 | det_loss=0.029921 | tv=0.726408 | nps=0.000000 | grad_norm=0.0030488462653011084 | selected_counts=[1, 1, 1, 3, 1, 3, 12, 1]\n",
      "[epoch 2 batch 55] total_loss=0.034873 | det_loss=0.034147 | tv=0.726164 | nps=0.000000 | grad_norm=0.006584180518984795 | selected_counts=[9, 2, 1, 3, 10, 10, 0, 7]\n",
      "[epoch 2 batch 56] total_loss=0.012205 | det_loss=0.011479 | tv=0.725893 | nps=0.000000 | grad_norm=0.0017903175903484225 | selected_counts=[2, 0, 1, 8, 1, 5, 6, 7]\n",
      "[epoch 2 batch 57] total_loss=0.027128 | det_loss=0.026402 | tv=0.725634 | nps=0.000000 | grad_norm=0.00471480144187808 | selected_counts=[1, 1, 2, 1, 5, 1, 9, 1]\n",
      "[epoch 2 batch 58] total_loss=0.026144 | det_loss=0.025419 | tv=0.725381 | nps=0.000000 | grad_norm=0.0031196018680930138 | selected_counts=[1, 1, 14, 1, 3, 4, 3, 1]\n",
      "[epoch 2 batch 59] total_loss=0.020006 | det_loss=0.019281 | tv=0.725135 | nps=0.000000 | grad_norm=0.0028599391225725412 | selected_counts=[1, 2, 2, 1, 2, 7, 6, 7]\n",
      "[epoch 2 batch 60] total_loss=0.016834 | det_loss=0.016109 | tv=0.724895 | nps=0.000000 | grad_norm=0.0038989782333374023 | selected_counts=[1, 4, 8, 8, 2, 4, 2, 3]\n",
      "[epoch 2 batch 61] total_loss=0.014015 | det_loss=0.013291 | tv=0.724659 | nps=0.000000 | grad_norm=0.003563797799870372 | selected_counts=[1, 1, 1, 4, 5, 10, 2, 12]\n",
      "[epoch 2 batch 62] total_loss=0.020024 | det_loss=0.019300 | tv=0.724431 | nps=0.000000 | grad_norm=0.004021014086902142 | selected_counts=[13, 4, 0, 15, 4, 8, 9, 3]\n",
      "[epoch 2 batch 63] total_loss=0.030530 | det_loss=0.029806 | tv=0.724204 | nps=0.000000 | grad_norm=0.0037169065326452255 | selected_counts=[3, 1, 2, 1, 5, 3, 8, 6]\n",
      "[epoch 2 batch 64] total_loss=0.020572 | det_loss=0.019848 | tv=0.723977 | nps=0.000000 | grad_norm=0.0033427926246076822 | selected_counts=[2, 5, 2, 1, 13, 1, 1, 5]\n",
      "[epoch 2 batch 65] total_loss=0.027646 | det_loss=0.026922 | tv=0.723751 | nps=0.000000 | grad_norm=0.0034851417876780033 | selected_counts=[1, 7, 1, 2, 11, 3, 3, 1]\n",
      "[epoch 2 batch 66] total_loss=0.018486 | det_loss=0.017762 | tv=0.723524 | nps=0.000000 | grad_norm=0.0032540704123675823 | selected_counts=[4, 4, 7, 4, 10, 7, 1, 3]\n",
      "[epoch 2 batch 67] total_loss=0.009639 | det_loss=0.008916 | tv=0.723295 | nps=0.000000 | grad_norm=0.01646222174167633 | selected_counts=[1, 1, 3, 12, 3, 3, 1, 1]\n",
      "[epoch 2 batch 68] total_loss=0.014602 | det_loss=0.013879 | tv=0.723045 | nps=0.000000 | grad_norm=0.0028888939414173365 | selected_counts=[7, 10, 6, 7, 4, 5, 2, 5]\n",
      "[epoch 2 batch 69] total_loss=0.011478 | det_loss=0.010755 | tv=0.722788 | nps=0.000000 | grad_norm=0.005239680875092745 | selected_counts=[9, 7, 1, 13, 12, 1, 2, 3]\n",
      "[epoch 2 batch 70] total_loss=0.015441 | det_loss=0.014718 | tv=0.722529 | nps=0.000000 | grad_norm=0.003348546102643013 | selected_counts=[1, 1, 5, 10, 7, 5, 5, 9]\n",
      "[epoch 2 batch 71] total_loss=0.018647 | det_loss=0.017924 | tv=0.722276 | nps=0.000000 | grad_norm=0.0028580310754477978 | selected_counts=[8, 2, 4, 10, 4, 4, 3, 1]\n",
      "[epoch 2 batch 72] total_loss=0.010749 | det_loss=0.010027 | tv=0.722032 | nps=0.000000 | grad_norm=0.004188010934740305 | selected_counts=[9, 0, 9, 12, 15, 1, 1, 1]\n",
      "[epoch 2 batch 73] total_loss=0.015647 | det_loss=0.014925 | tv=0.721793 | nps=0.000000 | grad_norm=0.004270025994628668 | selected_counts=[5, 4, 6, 9, 7, 7, 10, 2]\n",
      "[epoch 2 batch 74] total_loss=0.010080 | det_loss=0.009358 | tv=0.721554 | nps=0.000000 | grad_norm=0.004068310838192701 | selected_counts=[1, 4, 7, 1, 7, 11, 6, 11]\n",
      "[epoch 2 batch 75] total_loss=0.021669 | det_loss=0.020947 | tv=0.721320 | nps=0.000000 | grad_norm=0.0038755310233682394 | selected_counts=[2, 1, 8, 3, 2, 1, 3, 3]\n",
      "[epoch 2 batch 76] total_loss=0.011767 | det_loss=0.011046 | tv=0.721085 | nps=0.000000 | grad_norm=0.002562505891546607 | selected_counts=[4, 1, 2, 3, 12, 6]\n",
      "Epoch 2 saved patch snapshot.\n",
      "[epoch 3 batch 0] total_loss=0.015738 | det_loss=0.015017 | tv=0.720856 | nps=0.000000 | grad_norm=0.0039383238181471825 | selected_counts=[9, 1, 1, 11, 6, 7, 1, 2]\n",
      "[epoch 3 batch 1] total_loss=0.009843 | det_loss=0.009123 | tv=0.720631 | nps=0.000000 | grad_norm=0.002301413333043456 | selected_counts=[5, 7, 7, 10, 6, 4, 7, 9]\n",
      "[epoch 3 batch 2] total_loss=0.018410 | det_loss=0.017689 | tv=0.720410 | nps=0.000000 | grad_norm=0.0030771682504564524 | selected_counts=[4, 2, 6, 1, 9, 4, 10, 6]\n",
      "[epoch 3 batch 3] total_loss=0.019385 | det_loss=0.018665 | tv=0.720191 | nps=0.000000 | grad_norm=0.004606936126947403 | selected_counts=[14, 1, 5, 4, 8, 5, 1, 3]\n",
      "[epoch 3 batch 4] total_loss=0.029174 | det_loss=0.028454 | tv=0.719968 | nps=0.000000 | grad_norm=0.004871710669249296 | selected_counts=[1, 2, 1, 2, 6, 9, 2, 2]\n",
      "[epoch 3 batch 5] total_loss=0.028576 | det_loss=0.027856 | tv=0.719740 | nps=0.000000 | grad_norm=0.004382703918963671 | selected_counts=[2, 2, 5, 1, 2, 2, 1, 7]\n",
      "[epoch 3 batch 6] total_loss=0.020438 | det_loss=0.019719 | tv=0.719514 | nps=0.000000 | grad_norm=0.004218253307044506 | selected_counts=[4, 4, 3, 3, 2, 15, 10, 1]\n",
      "[epoch 3 batch 7] total_loss=0.021215 | det_loss=0.020495 | tv=0.719289 | nps=0.000000 | grad_norm=0.0033023389987647533 | selected_counts=[2, 7, 1, 5, 0, 3, 2, 3]\n",
      "[epoch 3 batch 8] total_loss=0.011830 | det_loss=0.011111 | tv=0.719069 | nps=0.000000 | grad_norm=0.003409208497032523 | selected_counts=[10, 4, 11, 6, 18, 5, 10, 9]\n",
      "[epoch 3 batch 9] total_loss=0.008999 | det_loss=0.008280 | tv=0.718857 | nps=0.000000 | grad_norm=0.001833205227740109 | selected_counts=[11, 2, 1, 4, 3, 8, 2, 1]\n",
      "[epoch 3 batch 10] total_loss=0.017513 | det_loss=0.016794 | tv=0.718654 | nps=0.000000 | grad_norm=0.0033421257976442575 | selected_counts=[10, 3, 2, 2, 1, 1, 4, 8]\n",
      "[epoch 3 batch 11] total_loss=0.019039 | det_loss=0.018320 | tv=0.718452 | nps=0.000000 | grad_norm=0.0026333595160394907 | selected_counts=[4, 2, 4, 6, 10, 5, 7, 2]\n",
      "[epoch 3 batch 12] total_loss=0.020249 | det_loss=0.019531 | tv=0.718251 | nps=0.000000 | grad_norm=0.00502747343853116 | selected_counts=[1, 4, 3, 1, 6, 9, 5, 3]\n",
      "[epoch 3 batch 13] total_loss=0.028678 | det_loss=0.027960 | tv=0.718049 | nps=0.000000 | grad_norm=0.004595938138663769 | selected_counts=[9, 4, 0, 2, 1, 0, 5, 5]\n",
      "[epoch 3 batch 14] total_loss=0.013722 | det_loss=0.013004 | tv=0.717843 | nps=0.000000 | grad_norm=0.00419993931427598 | selected_counts=[6, 3, 1, 1, 2, 7, 7, 1]\n",
      "[epoch 3 batch 15] total_loss=0.012176 | det_loss=0.011458 | tv=0.717633 | nps=0.000000 | grad_norm=0.0031275528017431498 | selected_counts=[4, 6, 8, 1, 4, 4, 4, 4]\n",
      "[epoch 3 batch 16] total_loss=0.013677 | det_loss=0.012960 | tv=0.717429 | nps=0.000000 | grad_norm=0.001999584725126624 | selected_counts=[2, 2, 5, 1, 1, 1, 0, 17]\n",
      "[epoch 3 batch 17] total_loss=0.013071 | det_loss=0.012353 | tv=0.717235 | nps=0.000000 | grad_norm=0.00418291799724102 | selected_counts=[8, 10, 3, 1, 6, 5, 5, 1]\n",
      "[epoch 3 batch 18] total_loss=0.009630 | det_loss=0.008912 | tv=0.717040 | nps=0.000000 | grad_norm=0.002005440415814519 | selected_counts=[1, 3, 3, 1, 10, 1, 6, 10]\n",
      "[epoch 3 batch 19] total_loss=0.016311 | det_loss=0.015594 | tv=0.716852 | nps=0.000000 | grad_norm=0.002080152975395322 | selected_counts=[1, 1, 2, 8, 9, 0, 3, 1]\n",
      "[epoch 3 batch 20] total_loss=0.026028 | det_loss=0.025311 | tv=0.716669 | nps=0.000000 | grad_norm=0.0035478367935866117 | selected_counts=[1, 2, 1, 1, 1, 7, 3, 6]\n",
      "[epoch 3 batch 21] total_loss=0.018152 | det_loss=0.017435 | tv=0.716488 | nps=0.000000 | grad_norm=0.0035598778631538153 | selected_counts=[1, 4, 12, 3, 5, 1, 3, 13]\n",
      "[epoch 3 batch 22] total_loss=0.023678 | det_loss=0.022962 | tv=0.716306 | nps=0.000000 | grad_norm=0.002771727740764618 | selected_counts=[4, 5, 2, 2, 1, 1, 6, 4]\n",
      "[epoch 3 batch 23] total_loss=0.020457 | det_loss=0.019741 | tv=0.716126 | nps=0.000000 | grad_norm=0.0055408840999007225 | selected_counts=[3, 1, 3, 5, 3, 6, 2, 6]\n",
      "[epoch 3 batch 24] total_loss=0.011092 | det_loss=0.010377 | tv=0.715938 | nps=0.000000 | grad_norm=0.0024218857288360596 | selected_counts=[3, 9, 2, 1, 2, 1, 1, 7]\n",
      "[epoch 3 batch 25] total_loss=0.015813 | det_loss=0.015097 | tv=0.715755 | nps=0.000000 | grad_norm=0.008681698702275753 | selected_counts=[3, 1, 3, 3, 1, 1, 1, 8]\n",
      "[epoch 3 batch 26] total_loss=0.015117 | det_loss=0.014401 | tv=0.715564 | nps=0.000000 | grad_norm=0.0031628881115466356 | selected_counts=[9, 3, 7, 4, 1, 7, 9, 1]\n",
      "[epoch 3 batch 27] total_loss=0.014726 | det_loss=0.014011 | tv=0.715368 | nps=0.000000 | grad_norm=0.0026886991690844297 | selected_counts=[7, 1, 6, 1, 6, 3, 1, 3]\n",
      "[epoch 3 batch 28] total_loss=0.022905 | det_loss=0.022189 | tv=0.715173 | nps=0.000000 | grad_norm=0.002170878928154707 | selected_counts=[9, 3, 3, 1, 16, 1, 1, 1]\n",
      "[epoch 3 batch 29] total_loss=0.020650 | det_loss=0.019935 | tv=0.714984 | nps=0.000000 | grad_norm=0.0038722227327525616 | selected_counts=[2, 3, 2, 3, 2, 1, 3, 4]\n",
      "[epoch 3 batch 30] total_loss=0.025441 | det_loss=0.024726 | tv=0.714792 | nps=0.000000 | grad_norm=0.0029425600077956915 | selected_counts=[0, 1, 6, 4, 1, 10, 9, 1]\n",
      "[epoch 3 batch 31] total_loss=0.026353 | det_loss=0.025638 | tv=0.714603 | nps=0.000000 | grad_norm=0.003483431413769722 | selected_counts=[5, 8, 1, 2, 7, 1, 4, 2]\n",
      "[epoch 3 batch 32] total_loss=0.020876 | det_loss=0.020162 | tv=0.714411 | nps=0.000000 | grad_norm=0.0027060862630605698 | selected_counts=[10, 3, 1, 3, 2, 8, 3, 3]\n",
      "[epoch 3 batch 33] total_loss=0.015286 | det_loss=0.014572 | tv=0.714221 | nps=0.000000 | grad_norm=0.0021174722351133823 | selected_counts=[1, 11, 2, 4, 1, 3, 0, 6]\n",
      "[epoch 3 batch 34] total_loss=0.013029 | det_loss=0.012315 | tv=0.714034 | nps=0.000000 | grad_norm=0.0039014534559100866 | selected_counts=[1, 9, 5, 3, 2, 2, 1, 9]\n",
      "[epoch 3 batch 35] total_loss=0.018723 | det_loss=0.018009 | tv=0.713845 | nps=0.000000 | grad_norm=0.005148806609213352 | selected_counts=[1, 6, 2, 1, 1, 3, 3, 5]\n",
      "[epoch 3 batch 36] total_loss=0.035299 | det_loss=0.034586 | tv=0.713651 | nps=0.000000 | grad_norm=0.004219833295792341 | selected_counts=[2, 0, 2, 2, 0, 1, 5, 4]\n",
      "[epoch 3 batch 37] total_loss=0.021414 | det_loss=0.020701 | tv=0.713450 | nps=0.000000 | grad_norm=0.003545374609529972 | selected_counts=[9, 2, 11, 2, 4, 1, 3, 10]\n",
      "[epoch 3 batch 38] total_loss=0.014905 | det_loss=0.014191 | tv=0.713247 | nps=0.000000 | grad_norm=0.004171808250248432 | selected_counts=[5, 4, 5, 3, 7, 7, 13, 8]\n",
      "[epoch 3 batch 39] total_loss=0.011047 | det_loss=0.010334 | tv=0.713042 | nps=0.000000 | grad_norm=0.001962908310815692 | selected_counts=[1, 3, 1, 10, 1, 5, 5, 2]\n",
      "[epoch 3 batch 40] total_loss=0.008380 | det_loss=0.007667 | tv=0.712839 | nps=0.000000 | grad_norm=0.0025184950791299343 | selected_counts=[4, 7, 3, 4, 8, 3, 4, 3]\n",
      "[epoch 3 batch 41] total_loss=0.007156 | det_loss=0.006443 | tv=0.712643 | nps=0.000000 | grad_norm=0.002440338721498847 | selected_counts=[11, 2, 2, 4, 18, 8, 7, 8]\n",
      "[epoch 3 batch 42] total_loss=0.016443 | det_loss=0.015731 | tv=0.712452 | nps=0.000000 | grad_norm=0.0030527054332196712 | selected_counts=[13, 4, 5, 6, 1, 1, 2, 2]\n",
      "[epoch 3 batch 43] total_loss=0.013733 | det_loss=0.013021 | tv=0.712269 | nps=0.000000 | grad_norm=0.00283565535210073 | selected_counts=[5, 7, 1, 2, 10, 7, 5, 7]\n",
      "[epoch 3 batch 44] total_loss=0.018308 | det_loss=0.017596 | tv=0.712089 | nps=0.000000 | grad_norm=0.0045615569688379765 | selected_counts=[1, 6, 4, 1, 1, 3, 4, 10]\n",
      "[epoch 3 batch 45] total_loss=0.025084 | det_loss=0.024372 | tv=0.711909 | nps=0.000000 | grad_norm=0.0032777569722384214 | selected_counts=[1, 5, 4, 13, 5, 3, 1, 3]\n",
      "[epoch 3 batch 46] total_loss=0.015438 | det_loss=0.014726 | tv=0.711720 | nps=0.000000 | grad_norm=0.002785881282761693 | selected_counts=[6, 0, 2, 1, 1, 2, 1, 6]\n",
      "[epoch 3 batch 47] total_loss=0.022377 | det_loss=0.021665 | tv=0.711532 | nps=0.000000 | grad_norm=0.003383354051038623 | selected_counts=[10, 1, 13, 5, 2, 4, 12, 3]\n",
      "[epoch 3 batch 48] total_loss=0.013909 | det_loss=0.013198 | tv=0.711343 | nps=0.000000 | grad_norm=0.005858235526829958 | selected_counts=[3, 13, 1, 2, 3, 7, 10, 6]\n",
      "[epoch 3 batch 49] total_loss=0.022421 | det_loss=0.021710 | tv=0.711135 | nps=0.000000 | grad_norm=0.00662169000133872 | selected_counts=[1, 5, 1, 2, 2, 7, 5, 1]\n",
      "[epoch 3 batch 50] total_loss=0.018078 | det_loss=0.017367 | tv=0.710916 | nps=0.000000 | grad_norm=0.002855089493095875 | selected_counts=[2, 1, 1, 12, 3, 7, 2, 1]\n",
      "[epoch 3 batch 51] total_loss=0.016829 | det_loss=0.016118 | tv=0.710705 | nps=0.000000 | grad_norm=0.0031288955360651016 | selected_counts=[1, 1, 0, 1, 3, 13, 2, 3]\n",
      "[epoch 3 batch 52] total_loss=0.032679 | det_loss=0.031969 | tv=0.710495 | nps=0.000000 | grad_norm=0.004992127884179354 | selected_counts=[3, 2, 1, 8, 1, 1, 6, 1]\n",
      "[epoch 3 batch 53] total_loss=0.015096 | det_loss=0.014386 | tv=0.710277 | nps=0.000000 | grad_norm=0.003663855604827404 | selected_counts=[7, 8, 12, 7, 7, 1, 1, 2]\n",
      "[epoch 3 batch 54] total_loss=0.023570 | det_loss=0.022860 | tv=0.710058 | nps=0.000000 | grad_norm=0.004516232758760452 | selected_counts=[1, 1, 3, 9, 2, 3, 5, 3]\n",
      "[epoch 3 batch 55] total_loss=0.008176 | det_loss=0.007466 | tv=0.709836 | nps=0.000000 | grad_norm=0.0021628818940371275 | selected_counts=[1, 4, 5, 6, 1, 3, 12, 2]\n",
      "[epoch 3 batch 56] total_loss=0.029759 | det_loss=0.029050 | tv=0.709620 | nps=0.000000 | grad_norm=0.005751459393650293 | selected_counts=[0, 4, 2, 6, 1, 3, 3, 2]\n",
      "[epoch 3 batch 57] total_loss=0.020022 | det_loss=0.019312 | tv=0.709394 | nps=0.000000 | grad_norm=0.0036819467786699533 | selected_counts=[2, 3, 4, 12, 2, 2, 3, 1]\n",
      "[epoch 3 batch 58] total_loss=0.017943 | det_loss=0.017234 | tv=0.709167 | nps=0.000000 | grad_norm=0.002554173581302166 | selected_counts=[1, 1, 1, 2, 1, 3, 3, 6]\n",
      "[epoch 3 batch 59] total_loss=0.009431 | det_loss=0.008722 | tv=0.708944 | nps=0.000000 | grad_norm=0.001802189159207046 | selected_counts=[3, 1, 3, 2, 3, 9, 5, 1]\n",
      "[epoch 3 batch 60] total_loss=0.015113 | det_loss=0.014404 | tv=0.708730 | nps=0.000000 | grad_norm=0.0028992341831326485 | selected_counts=[3, 3, 1, 9, 2, 4, 1, 11]\n",
      "[epoch 3 batch 61] total_loss=0.015413 | det_loss=0.014705 | tv=0.708523 | nps=0.000000 | grad_norm=0.0025001089088618755 | selected_counts=[1, 13, 6, 1, 6, 2, 2, 1]\n",
      "[epoch 3 batch 62] total_loss=0.016481 | det_loss=0.015773 | tv=0.708326 | nps=0.000000 | grad_norm=0.003915376495569944 | selected_counts=[6, 6, 1, 11, 3, 12, 10, 3]\n",
      "[epoch 3 batch 63] total_loss=0.008206 | det_loss=0.007498 | tv=0.708127 | nps=0.000000 | grad_norm=0.0019410897511988878 | selected_counts=[10, 0, 2, 6, 11, 6, 5, 11]\n",
      "[epoch 3 batch 64] total_loss=0.014333 | det_loss=0.013625 | tv=0.707935 | nps=0.000000 | grad_norm=0.0029508627485483885 | selected_counts=[5, 2, 7, 1, 12, 1, 3, 6]\n",
      "[epoch 3 batch 65] total_loss=0.012823 | det_loss=0.012116 | tv=0.707742 | nps=0.000000 | grad_norm=0.003357055364176631 | selected_counts=[2, 2, 4, 2, 0, 9, 1, 1]\n",
      "[epoch 3 batch 66] total_loss=0.017603 | det_loss=0.016896 | tv=0.707554 | nps=0.000000 | grad_norm=0.002872765064239502 | selected_counts=[9, 3, 5, 1, 1, 7, 6, 7]\n",
      "[epoch 3 batch 67] total_loss=0.019381 | det_loss=0.018673 | tv=0.707369 | nps=0.000000 | grad_norm=0.0033600046299397945 | selected_counts=[2, 13, 6, 3, 2, 8, 3, 2]\n",
      "[epoch 3 batch 68] total_loss=0.015370 | det_loss=0.014663 | tv=0.707180 | nps=0.000000 | grad_norm=0.003792869858443737 | selected_counts=[7, 1, 2, 8, 2, 8, 2, 10]\n",
      "[epoch 3 batch 69] total_loss=0.018481 | det_loss=0.017774 | tv=0.706990 | nps=0.000000 | grad_norm=0.004187059588730335 | selected_counts=[0, 3, 4, 11, 5, 3, 1, 0]\n",
      "[epoch 3 batch 70] total_loss=0.021657 | det_loss=0.020950 | tv=0.706797 | nps=0.000000 | grad_norm=0.00410925829783082 | selected_counts=[2, 2, 1, 7, 7, 2, 1, 5]\n",
      "[epoch 3 batch 71] total_loss=0.020145 | det_loss=0.019438 | tv=0.706598 | nps=0.000000 | grad_norm=0.0033877091482281685 | selected_counts=[1, 10, 1, 4, 6, 4, 7, 3]\n",
      "[epoch 3 batch 72] total_loss=0.016039 | det_loss=0.015333 | tv=0.706399 | nps=0.000000 | grad_norm=0.002333665732294321 | selected_counts=[3, 4, 1, 7, 1, 6, 2, 1]\n",
      "[epoch 3 batch 73] total_loss=0.020012 | det_loss=0.019306 | tv=0.706205 | nps=0.000000 | grad_norm=0.005848081316798925 | selected_counts=[5, 2, 10, 1, 1, 2, 8, 4]\n",
      "[epoch 3 batch 74] total_loss=0.007004 | det_loss=0.006298 | tv=0.705998 | nps=0.000000 | grad_norm=0.0030145072378218174 | selected_counts=[7, 3, 6, 8, 7, 8, 3, 9]\n",
      "[epoch 3 batch 75] total_loss=0.006281 | det_loss=0.005575 | tv=0.705795 | nps=0.000000 | grad_norm=0.00122449838090688 | selected_counts=[3, 4, 5, 2, 3, 1, 4, 3]\n",
      "[epoch 3 batch 76] total_loss=0.012170 | det_loss=0.011465 | tv=0.705602 | nps=0.000000 | grad_norm=0.0031526153907179832 | selected_counts=[1, 5, 11, 3, 3, 7]\n",
      "Epoch 3 saved patch snapshot.\n",
      "[epoch 4 batch 0] total_loss=0.013169 | det_loss=0.012464 | tv=0.705412 | nps=0.000000 | grad_norm=0.002959370845928788 | selected_counts=[1, 1, 1, 8, 3, 1, 10, 12]\n",
      "[epoch 4 batch 1] total_loss=0.016736 | det_loss=0.016031 | tv=0.705229 | nps=0.000000 | grad_norm=0.0016648333985358477 | selected_counts=[9, 0, 1, 5, 11, 1, 1, 8]\n",
      "[epoch 4 batch 2] total_loss=0.015152 | det_loss=0.014447 | tv=0.705053 | nps=0.000000 | grad_norm=0.002465215278789401 | selected_counts=[4, 9, 3, 1, 3, 3, 1, 9]\n",
      "[epoch 4 batch 3] total_loss=0.022686 | det_loss=0.021981 | tv=0.704883 | nps=0.000000 | grad_norm=0.005002365913242102 | selected_counts=[3, 1, 6, 4, 5, 1, 2, 1]\n",
      "[epoch 4 batch 4] total_loss=0.006730 | det_loss=0.006026 | tv=0.704708 | nps=0.000000 | grad_norm=0.0018238312331959605 | selected_counts=[5, 2, 4, 9, 6, 2, 4, 1]\n",
      "[epoch 4 batch 5] total_loss=0.006988 | det_loss=0.006283 | tv=0.704539 | nps=0.000000 | grad_norm=0.0018338850932195783 | selected_counts=[4, 5, 3, 1, 3, 1, 1, 7]\n",
      "[epoch 4 batch 6] total_loss=0.008816 | det_loss=0.008111 | tv=0.704376 | nps=0.000000 | grad_norm=0.002168932929635048 | selected_counts=[4, 18, 3, 4, 3, 6, 14, 1]\n",
      "[epoch 4 batch 7] total_loss=0.017060 | det_loss=0.016356 | tv=0.704217 | nps=0.000000 | grad_norm=0.0034535278100520372 | selected_counts=[1, 2, 1, 4, 2, 4, 1, 3]\n",
      "[epoch 4 batch 8] total_loss=0.021123 | det_loss=0.020419 | tv=0.704059 | nps=0.000000 | grad_norm=0.00379691319540143 | selected_counts=[7, 4, 5, 7, 4, 4, 1, 1]\n",
      "[epoch 4 batch 9] total_loss=0.012549 | det_loss=0.011845 | tv=0.703897 | nps=0.000000 | grad_norm=0.0022130089346319437 | selected_counts=[5, 1, 4, 8, 4, 2, 1, 3]\n",
      "[epoch 4 batch 10] total_loss=0.013936 | det_loss=0.013232 | tv=0.703736 | nps=0.000000 | grad_norm=0.002346895169466734 | selected_counts=[5, 2, 3, 1, 12, 0, 8, 6]\n",
      "[epoch 4 batch 11] total_loss=0.011658 | det_loss=0.010955 | tv=0.703580 | nps=0.000000 | grad_norm=0.0025074989534914494 | selected_counts=[1, 1, 1, 5, 2, 8, 3, 2]\n",
      "[epoch 4 batch 12] total_loss=0.019780 | det_loss=0.019076 | tv=0.703426 | nps=0.000000 | grad_norm=0.003970201127231121 | selected_counts=[1, 2, 1, 1, 4, 19, 1, 2]\n",
      "[epoch 4 batch 13] total_loss=0.018124 | det_loss=0.017421 | tv=0.703271 | nps=0.000000 | grad_norm=0.002883864799514413 | selected_counts=[1, 12, 4, 18, 7, 1, 8, 2]\n",
      "[epoch 4 batch 14] total_loss=0.012412 | det_loss=0.011708 | tv=0.703118 | nps=0.000000 | grad_norm=0.0017523459391668439 | selected_counts=[5, 1, 2, 9, 4, 2, 1, 1]\n",
      "[epoch 4 batch 15] total_loss=0.010597 | det_loss=0.009894 | tv=0.702969 | nps=0.000000 | grad_norm=0.0017899014055728912 | selected_counts=[3, 1, 9, 7, 10, 2, 3, 6]\n",
      "[epoch 4 batch 16] total_loss=0.009166 | det_loss=0.008463 | tv=0.702823 | nps=0.000000 | grad_norm=0.0015908766072243452 | selected_counts=[6, 8, 1, 13, 5, 8, 1, 7]\n",
      "[epoch 4 batch 17] total_loss=0.015606 | det_loss=0.014904 | tv=0.702682 | nps=0.000000 | grad_norm=0.003100893460214138 | selected_counts=[2, 2, 3, 8, 5, 2, 3, 1]\n",
      "[epoch 4 batch 18] total_loss=0.012525 | det_loss=0.011823 | tv=0.702540 | nps=0.000000 | grad_norm=0.004112038295716047 | selected_counts=[5, 3, 2, 5, 6, 4, 1, 1]\n",
      "[epoch 4 batch 19] total_loss=0.021196 | det_loss=0.020494 | tv=0.702390 | nps=0.000000 | grad_norm=0.002766328863799572 | selected_counts=[2, 7, 2, 1, 1, 1, 4, 3]\n",
      "[epoch 4 batch 20] total_loss=0.003576 | det_loss=0.002874 | tv=0.702237 | nps=0.000000 | grad_norm=0.0006965741631574929 | selected_counts=[8, 13, 9, 2, 7, 15, 7, 1]\n",
      "[epoch 4 batch 21] total_loss=0.015958 | det_loss=0.015256 | tv=0.702093 | nps=0.000000 | grad_norm=0.0033347627613693476 | selected_counts=[4, 4, 5, 2, 2, 3, 1, 5]\n",
      "[epoch 4 batch 22] total_loss=0.006848 | det_loss=0.006146 | tv=0.701943 | nps=0.000000 | grad_norm=0.0021879100240767 | selected_counts=[7, 9, 8, 4, 11, 4, 1, 4]\n",
      "[epoch 4 batch 23] total_loss=0.024555 | det_loss=0.023853 | tv=0.701795 | nps=0.000000 | grad_norm=0.005592005793005228 | selected_counts=[4, 2, 5, 2, 1, 1, 9, 2]\n",
      "[epoch 4 batch 24] total_loss=0.018655 | det_loss=0.017953 | tv=0.701637 | nps=0.000000 | grad_norm=0.0034890046808868647 | selected_counts=[2, 1, 9, 1, 1, 6, 3, 6]\n",
      "[epoch 4 batch 25] total_loss=0.014346 | det_loss=0.013645 | tv=0.701478 | nps=0.000000 | grad_norm=0.002088675508275628 | selected_counts=[1, 3, 10, 2, 8, 1, 9, 10]\n",
      "[epoch 4 batch 26] total_loss=0.015117 | det_loss=0.014416 | tv=0.701322 | nps=0.000000 | grad_norm=0.0021196904126554728 | selected_counts=[1, 10, 3, 1, 3, 2, 2, 7]\n",
      "[epoch 4 batch 27] total_loss=0.008973 | det_loss=0.008271 | tv=0.701170 | nps=0.000000 | grad_norm=0.002881258027628064 | selected_counts=[8, 6, 2, 10, 2, 5, 1, 4]\n",
      "[epoch 4 batch 28] total_loss=0.015277 | det_loss=0.014576 | tv=0.701020 | nps=0.000000 | grad_norm=0.0030430355109274387 | selected_counts=[10, 1, 3, 5, 5, 1, 8, 1]\n",
      "[epoch 4 batch 29] total_loss=0.017090 | det_loss=0.016389 | tv=0.700869 | nps=0.000000 | grad_norm=0.003970657009631395 | selected_counts=[1, 3, 1, 1, 1, 13, 2, 5]\n",
      "[epoch 4 batch 30] total_loss=0.017656 | det_loss=0.016956 | tv=0.700710 | nps=0.000000 | grad_norm=0.003924694377928972 | selected_counts=[1, 12, 6, 10, 2, 1, 10, 4]\n",
      "[epoch 4 batch 31] total_loss=0.013191 | det_loss=0.012491 | tv=0.700541 | nps=0.000000 | grad_norm=0.003522932529449463 | selected_counts=[5, 1, 4, 2, 2, 7, 3, 4]\n",
      "[epoch 4 batch 32] total_loss=0.015581 | det_loss=0.014881 | tv=0.700370 | nps=0.000000 | grad_norm=0.0032498803921043873 | selected_counts=[2, 4, 12, 2, 6, 3, 1, 3]\n",
      "[epoch 4 batch 33] total_loss=0.011112 | det_loss=0.010412 | tv=0.700193 | nps=0.000000 | grad_norm=0.002745275618508458 | selected_counts=[2, 8, 8, 1, 9, 3, 16, 6]\n",
      "[epoch 4 batch 34] total_loss=0.025056 | det_loss=0.024356 | tv=0.700020 | nps=0.000000 | grad_norm=0.0036398256197571754 | selected_counts=[2, 4, 8, 1, 1, 5, 1, 1]\n",
      "[epoch 4 batch 35] total_loss=0.011316 | det_loss=0.010616 | tv=0.699848 | nps=0.000000 | grad_norm=0.0016469372203573585 | selected_counts=[0, 6, 0, 8, 3, 3, 2, 2]\n",
      "[epoch 4 batch 36] total_loss=0.019650 | det_loss=0.018951 | tv=0.699682 | nps=0.000000 | grad_norm=0.0026135738007724285 | selected_counts=[1, 2, 3, 10, 1, 2, 5, 10]\n",
      "[epoch 4 batch 37] total_loss=0.013925 | det_loss=0.013226 | tv=0.699520 | nps=0.000000 | grad_norm=0.002199582988396287 | selected_counts=[5, 3, 3, 1, 1, 10, 9, 1]\n",
      "[epoch 4 batch 38] total_loss=0.015944 | det_loss=0.015245 | tv=0.699363 | nps=0.000000 | grad_norm=0.006320068147033453 | selected_counts=[1, 6, 3, 10, 10, 1, 2, 3]\n",
      "[epoch 4 batch 39] total_loss=0.012433 | det_loss=0.011733 | tv=0.699193 | nps=0.000000 | grad_norm=0.00224004196934402 | selected_counts=[1, 1, 2, 2, 2, 3, 5, 4]\n",
      "[epoch 4 batch 40] total_loss=0.017671 | det_loss=0.016972 | tv=0.699026 | nps=0.000000 | grad_norm=0.006882403511554003 | selected_counts=[6, 8, 1, 5, 3, 2, 7, 1]\n",
      "[epoch 4 batch 41] total_loss=0.004957 | det_loss=0.004258 | tv=0.698846 | nps=0.000000 | grad_norm=0.0011941487900912762 | selected_counts=[1, 8, 1, 3, 7, 11, 3, 7]\n",
      "[epoch 4 batch 42] total_loss=0.014182 | det_loss=0.013484 | tv=0.698671 | nps=0.000000 | grad_norm=0.0032029051799327135 | selected_counts=[10, 3, 9, 2, 2, 2, 0, 6]\n",
      "[epoch 4 batch 43] total_loss=0.022097 | det_loss=0.021399 | tv=0.698500 | nps=0.000000 | grad_norm=0.005114288069307804 | selected_counts=[9, 5, 1, 3, 4, 2, 11, 1]\n",
      "[epoch 4 batch 44] total_loss=0.017068 | det_loss=0.016369 | tv=0.698313 | nps=0.000000 | grad_norm=0.003871068125590682 | selected_counts=[4, 5, 4, 2, 6, 4, 1, 1]\n",
      "[epoch 4 batch 45] total_loss=0.021959 | det_loss=0.021260 | tv=0.698128 | nps=0.000000 | grad_norm=0.0038911665324121714 | selected_counts=[5, 4, 6, 9, 8, 4, 1, 1]\n",
      "[epoch 4 batch 46] total_loss=0.025532 | det_loss=0.024835 | tv=0.697942 | nps=0.000000 | grad_norm=0.004334393888711929 | selected_counts=[4, 6, 8, 2, 4, 1, 6, 11]\n",
      "[epoch 4 batch 47] total_loss=0.007987 | det_loss=0.007289 | tv=0.697751 | nps=0.000000 | grad_norm=0.0019515912281349301 | selected_counts=[2, 1, 3, 2, 2, 5, 1, 4]\n",
      "[epoch 4 batch 48] total_loss=0.019616 | det_loss=0.018919 | tv=0.697566 | nps=0.000000 | grad_norm=0.004039688967168331 | selected_counts=[1, 9, 1, 1, 1, 2, 6, 6]\n",
      "[epoch 4 batch 49] total_loss=0.019619 | det_loss=0.018922 | tv=0.697374 | nps=0.000000 | grad_norm=0.004187289159744978 | selected_counts=[1, 2, 3, 5, 4, 4, 3, 2]\n",
      "[epoch 4 batch 50] total_loss=0.019312 | det_loss=0.018615 | tv=0.697184 | nps=0.000000 | grad_norm=0.003119351575151086 | selected_counts=[2, 2, 1, 1, 2, 1, 5, 1]\n",
      "[epoch 4 batch 51] total_loss=0.016192 | det_loss=0.015495 | tv=0.696998 | nps=0.000000 | grad_norm=0.003908797167241573 | selected_counts=[1, 4, 11, 10, 8, 2, 10, 3]\n",
      "[epoch 4 batch 52] total_loss=0.015872 | det_loss=0.015175 | tv=0.696810 | nps=0.000000 | grad_norm=0.003114387160167098 | selected_counts=[2, 2, 3, 3, 5, 4, 2, 1]\n",
      "[epoch 4 batch 53] total_loss=0.008596 | det_loss=0.007899 | tv=0.696626 | nps=0.000000 | grad_norm=0.001542647834867239 | selected_counts=[1, 2, 0, 3, 1, 4, 5, 2]\n",
      "[epoch 4 batch 54] total_loss=0.011617 | det_loss=0.010920 | tv=0.696452 | nps=0.000000 | grad_norm=0.001432140008546412 | selected_counts=[1, 1, 4, 8, 1, 3, 4, 1]\n",
      "[epoch 4 batch 55] total_loss=0.017347 | det_loss=0.016651 | tv=0.696287 | nps=0.000000 | grad_norm=0.003771201241761446 | selected_counts=[3, 2, 5, 15, 3, 2, 2, 5]\n",
      "[epoch 4 batch 56] total_loss=0.009229 | det_loss=0.008533 | tv=0.696121 | nps=0.000000 | grad_norm=0.00237878505140543 | selected_counts=[3, 1, 8, 5, 1, 1, 4, 9]\n",
      "[epoch 4 batch 57] total_loss=0.011644 | det_loss=0.010948 | tv=0.695958 | nps=0.000000 | grad_norm=0.0038142434787005186 | selected_counts=[5, 2, 3, 6, 2, 1, 1, 7]\n",
      "[epoch 4 batch 58] total_loss=0.011595 | det_loss=0.010899 | tv=0.695796 | nps=0.000000 | grad_norm=0.0024688944686204195 | selected_counts=[8, 1, 7, 2, 5, 11, 4, 5]\n",
      "[epoch 4 batch 59] total_loss=0.006971 | det_loss=0.006275 | tv=0.695639 | nps=0.000000 | grad_norm=0.0017706647049635649 | selected_counts=[4, 1, 1, 4, 5, 2, 8, 1]\n",
      "[epoch 4 batch 60] total_loss=0.027208 | det_loss=0.026512 | tv=0.695488 | nps=0.000000 | grad_norm=0.005294361617416143 | selected_counts=[3, 12, 3, 1, 1, 1, 2, 5]\n",
      "[epoch 4 batch 61] total_loss=0.017050 | det_loss=0.016355 | tv=0.695330 | nps=0.000000 | grad_norm=0.004169592168182135 | selected_counts=[4, 3, 10, 6, 8, 7, 5, 3]\n",
      "[epoch 4 batch 62] total_loss=0.017334 | det_loss=0.016639 | tv=0.695164 | nps=0.000000 | grad_norm=0.0025544899981468916 | selected_counts=[1, 5, 1, 6, 1, 1, 2, 10]\n",
      "[epoch 4 batch 63] total_loss=0.015189 | det_loss=0.014494 | tv=0.694999 | nps=0.000000 | grad_norm=0.007929455488920212 | selected_counts=[2, 2, 4, 4, 1, 2, 3, 8]\n",
      "[epoch 4 batch 64] total_loss=0.023363 | det_loss=0.022668 | tv=0.694816 | nps=0.000000 | grad_norm=0.003991497680544853 | selected_counts=[11, 4, 1, 6, 2, 9, 10, 1]\n",
      "[epoch 4 batch 65] total_loss=0.011192 | det_loss=0.010498 | tv=0.694629 | nps=0.000000 | grad_norm=0.0015546124195680022 | selected_counts=[6, 0, 11, 5, 6, 1, 6, 15]\n",
      "[epoch 4 batch 66] total_loss=0.016478 | det_loss=0.015784 | tv=0.694447 | nps=0.000000 | grad_norm=0.00407239468768239 | selected_counts=[1, 3, 6, 8, 5, 7, 2, 1]\n",
      "[epoch 4 batch 67] total_loss=0.011769 | det_loss=0.011075 | tv=0.694263 | nps=0.000000 | grad_norm=0.0021122123580425978 | selected_counts=[2, 8, 4, 9, 1, 1, 2, 5]\n",
      "[epoch 4 batch 68] total_loss=0.016748 | det_loss=0.016054 | tv=0.694085 | nps=0.000000 | grad_norm=0.0032804496586322784 | selected_counts=[4, 8, 5, 2, 6, 6, 3, 3]\n",
      "[epoch 4 batch 69] total_loss=0.018536 | det_loss=0.017842 | tv=0.693910 | nps=0.000000 | grad_norm=0.0027329043950885534 | selected_counts=[1, 5, 1, 3, 1, 5, 1, 2]\n",
      "[epoch 4 batch 70] total_loss=0.011916 | det_loss=0.011222 | tv=0.693740 | nps=0.000000 | grad_norm=0.0019725740421563387 | selected_counts=[1, 1, 12, 1, 2, 3, 9, 4]\n",
      "[epoch 4 batch 71] total_loss=0.011538 | det_loss=0.010844 | tv=0.693576 | nps=0.000000 | grad_norm=0.003240094054490328 | selected_counts=[3, 3, 3, 1, 8, 5, 7, 5]\n",
      "[epoch 4 batch 72] total_loss=0.011687 | det_loss=0.010993 | tv=0.693412 | nps=0.000000 | grad_norm=0.0026953136548399925 | selected_counts=[3, 1, 1, 17, 5, 9, 6, 6]\n",
      "[epoch 4 batch 73] total_loss=0.014376 | det_loss=0.013683 | tv=0.693248 | nps=0.000000 | grad_norm=0.0036833505146205425 | selected_counts=[1, 4, 4, 9, 10, 2, 10, 11]\n",
      "[epoch 4 batch 74] total_loss=0.015206 | det_loss=0.014513 | tv=0.693085 | nps=0.000000 | grad_norm=0.0027319123037159443 | selected_counts=[6, 3, 3, 7, 3, 1, 1, 10]\n",
      "[epoch 4 batch 75] total_loss=0.007814 | det_loss=0.007121 | tv=0.692923 | nps=0.000000 | grad_norm=0.00160815694835037 | selected_counts=[9, 3, 12, 1, 8, 18, 2, 10]\n",
      "[epoch 4 batch 76] total_loss=0.021359 | det_loss=0.020666 | tv=0.692768 | nps=0.000000 | grad_norm=0.002925505628809333 | selected_counts=[1, 6, 1, 4, 1, 1]\n",
      "Epoch 4 saved patch snapshot.\n",
      "[epoch 5 batch 0] total_loss=0.011064 | det_loss=0.010371 | tv=0.692619 | nps=0.000000 | grad_norm=0.003285388695076108 | selected_counts=[6, 9, 6, 9, 7, 9, 3, 1]\n",
      "[epoch 5 batch 1] total_loss=0.006906 | det_loss=0.006213 | tv=0.692468 | nps=0.000000 | grad_norm=0.0016202793922275305 | selected_counts=[1, 12, 1, 9, 3, 1, 3, 5]\n",
      "[epoch 5 batch 2] total_loss=0.010253 | det_loss=0.009561 | tv=0.692321 | nps=0.000000 | grad_norm=0.0018142907647415996 | selected_counts=[7, 9, 5, 1, 7, 5, 3, 1]\n",
      "[epoch 5 batch 3] total_loss=0.036865 | det_loss=0.036172 | tv=0.692181 | nps=0.000000 | grad_norm=0.004646993707865477 | selected_counts=[4, 5, 2, 1, 2, 1, 14, 4]\n",
      "[epoch 5 batch 4] total_loss=0.016393 | det_loss=0.015701 | tv=0.692027 | nps=0.000000 | grad_norm=0.004700582940131426 | selected_counts=[10, 4, 9, 16, 2, 2, 1, 2]\n",
      "[epoch 5 batch 5] total_loss=0.013511 | det_loss=0.012819 | tv=0.691868 | nps=0.000000 | grad_norm=0.002724215853959322 | selected_counts=[10, 6, 2, 5, 1, 8, 3, 1]\n",
      "[epoch 5 batch 6] total_loss=0.008861 | det_loss=0.008169 | tv=0.691708 | nps=0.000000 | grad_norm=0.002419420052319765 | selected_counts=[6, 5, 4, 6, 5, 1, 7, 2]\n",
      "[epoch 5 batch 7] total_loss=0.008247 | det_loss=0.007555 | tv=0.691550 | nps=0.000000 | grad_norm=0.001679120701737702 | selected_counts=[7, 7, 3, 1, 5, 1, 8, 7]\n",
      "[epoch 5 batch 8] total_loss=0.010991 | det_loss=0.010300 | tv=0.691397 | nps=0.000000 | grad_norm=0.0023840840440243483 | selected_counts=[10, 4, 6, 8, 4, 2, 10, 11]\n",
      "[epoch 5 batch 9] total_loss=0.008628 | det_loss=0.007937 | tv=0.691243 | nps=0.000000 | grad_norm=0.002972474554553628 | selected_counts=[4, 7, 2, 6, 8, 10, 8, 9]\n",
      "[epoch 5 batch 10] total_loss=0.014079 | det_loss=0.013388 | tv=0.691088 | nps=0.000000 | grad_norm=0.003415801329538226 | selected_counts=[8, 2, 2, 10, 1, 1, 6, 5]\n",
      "[epoch 5 batch 11] total_loss=0.015316 | det_loss=0.014625 | tv=0.690931 | nps=0.000000 | grad_norm=0.0031544221565127373 | selected_counts=[3, 7, 3, 11, 2, 7, 5, 1]\n",
      "[epoch 5 batch 12] total_loss=0.006561 | det_loss=0.005870 | tv=0.690775 | nps=0.000000 | grad_norm=0.0012121483450755477 | selected_counts=[1, 4, 2, 8, 4, 4, 8, 1]\n",
      "[epoch 5 batch 13] total_loss=0.015347 | det_loss=0.014657 | tv=0.690625 | nps=0.000000 | grad_norm=0.0025978381745517254 | selected_counts=[4, 8, 1, 2, 2, 1, 1, 4]\n",
      "[epoch 5 batch 14] total_loss=0.004871 | det_loss=0.004180 | tv=0.690473 | nps=0.000000 | grad_norm=0.0013169575249776244 | selected_counts=[10, 1, 5, 1, 10, 8, 3, 4]\n",
      "[epoch 5 batch 15] total_loss=0.008583 | det_loss=0.007893 | tv=0.690328 | nps=0.000000 | grad_norm=0.0019478404428809881 | selected_counts=[11, 1, 2, 1, 7, 2, 6, 8]\n",
      "[epoch 5 batch 16] total_loss=0.019080 | det_loss=0.018389 | tv=0.690187 | nps=0.000000 | grad_norm=0.0030638279858976603 | selected_counts=[2, 4, 11, 1, 3, 3, 1, 3]\n",
      "[epoch 5 batch 17] total_loss=0.020310 | det_loss=0.019620 | tv=0.690043 | nps=0.000000 | grad_norm=0.0031670546159148216 | selected_counts=[1, 1, 2, 1, 2, 5, 2, 5]\n",
      "[epoch 5 batch 18] total_loss=0.006636 | det_loss=0.005946 | tv=0.689896 | nps=0.000000 | grad_norm=0.0016529718413949013 | selected_counts=[5, 7, 1, 2, 5, 4, 2, 9]\n",
      "[epoch 5 batch 19] total_loss=0.007445 | det_loss=0.006756 | tv=0.689753 | nps=0.000000 | grad_norm=0.0013209410244598985 | selected_counts=[2, 1, 1, 3, 3, 1, 1, 4]\n",
      "[epoch 5 batch 20] total_loss=0.016092 | det_loss=0.015402 | tv=0.689613 | nps=0.000000 | grad_norm=0.0025986800901591778 | selected_counts=[1, 5, 1, 0, 3, 1, 7, 1]\n",
      "[epoch 5 batch 21] total_loss=0.007384 | det_loss=0.006694 | tv=0.689474 | nps=0.000000 | grad_norm=0.0015205633826553822 | selected_counts=[1, 2, 8, 1, 9, 4, 3, 9]\n",
      "[epoch 5 batch 22] total_loss=0.014327 | det_loss=0.013637 | tv=0.689340 | nps=0.000000 | grad_norm=0.0035893551539629698 | selected_counts=[0, 4, 2, 1, 1, 1, 2, 2]\n",
      "[epoch 5 batch 23] total_loss=0.009488 | det_loss=0.008798 | tv=0.689202 | nps=0.000000 | grad_norm=0.0017923444975167513 | selected_counts=[3, 3, 1, 1, 2, 3, 3, 7]\n",
      "[epoch 5 batch 24] total_loss=0.005655 | det_loss=0.004965 | tv=0.689067 | nps=0.000000 | grad_norm=0.0012055293191224337 | selected_counts=[4, 3, 10, 4, 5, 6, 2, 4]\n",
      "[epoch 5 batch 25] total_loss=0.006329 | det_loss=0.005640 | tv=0.688938 | nps=0.000000 | grad_norm=0.0012128503294661641 | selected_counts=[4, 2, 3, 3, 5, 4, 1, 4]\n",
      "[epoch 5 batch 26] total_loss=0.009755 | det_loss=0.009066 | tv=0.688815 | nps=0.000000 | grad_norm=0.002347246976569295 | selected_counts=[1, 11, 1, 9, 4, 8, 8, 4]\n",
      "[epoch 5 batch 27] total_loss=0.014303 | det_loss=0.013615 | tv=0.688693 | nps=0.000000 | grad_norm=0.003778117476031184 | selected_counts=[4, 3, 2, 7, 5, 2, 3, 2]\n",
      "[epoch 5 batch 28] total_loss=0.011618 | det_loss=0.010930 | tv=0.688565 | nps=0.000000 | grad_norm=0.005636694375425577 | selected_counts=[1, 8, 1, 3, 5, 3, 2, 5]\n",
      "[epoch 5 batch 29] total_loss=0.014673 | det_loss=0.013985 | tv=0.688422 | nps=0.000000 | grad_norm=0.0035479196812957525 | selected_counts=[2, 5, 6, 10, 0, 1, 2, 3]\n",
      "[epoch 5 batch 30] total_loss=0.023534 | det_loss=0.022846 | tv=0.688273 | nps=0.000000 | grad_norm=0.004004088696092367 | selected_counts=[1, 3, 2, 3, 3, 1, 3, 3]\n",
      "[epoch 5 batch 31] total_loss=0.005648 | det_loss=0.004960 | tv=0.688112 | nps=0.000000 | grad_norm=0.001775954500772059 | selected_counts=[15, 5, 1, 10, 7, 6, 7, 11]\n",
      "[epoch 5 batch 32] total_loss=0.004533 | det_loss=0.003845 | tv=0.687955 | nps=0.000000 | grad_norm=0.0018488902132958174 | selected_counts=[6, 3, 1, 4, 3, 4, 11, 11]\n",
      "[epoch 5 batch 33] total_loss=0.004582 | det_loss=0.003894 | tv=0.687805 | nps=0.000000 | grad_norm=0.0013777923304587603 | selected_counts=[5, 8, 12, 8, 3, 2, 11, 4]\n",
      "[epoch 5 batch 34] total_loss=0.012658 | det_loss=0.011970 | tv=0.687662 | nps=0.000000 | grad_norm=0.0028754258528351784 | selected_counts=[3, 2, 1, 1, 2, 12, 3, 5]\n",
      "[epoch 5 batch 35] total_loss=0.010791 | det_loss=0.010104 | tv=0.687519 | nps=0.000000 | grad_norm=0.002277909778058529 | selected_counts=[8, 7, 5, 2, 3, 1, 3, 1]\n",
      "[epoch 5 batch 36] total_loss=0.013300 | det_loss=0.012612 | tv=0.687381 | nps=0.000000 | grad_norm=0.002204668940976262 | selected_counts=[9, 2, 6, 12, 1, 8, 11, 1]\n",
      "[epoch 5 batch 37] total_loss=0.010289 | det_loss=0.009602 | tv=0.687247 | nps=0.000000 | grad_norm=0.002531980397179723 | selected_counts=[6, 12, 5, 2, 7, 16, 8, 8]\n",
      "[epoch 5 batch 38] total_loss=0.013397 | det_loss=0.012710 | tv=0.687115 | nps=0.000000 | grad_norm=0.005041757598519325 | selected_counts=[6, 2, 2, 4, 8, 2, 6, 4]\n",
      "[epoch 5 batch 39] total_loss=0.007378 | det_loss=0.006691 | tv=0.686975 | nps=0.000000 | grad_norm=0.001673216582275927 | selected_counts=[9, 1, 8, 5, 2, 4, 6, 5]\n",
      "[epoch 5 batch 40] total_loss=0.010944 | det_loss=0.010257 | tv=0.686840 | nps=0.000000 | grad_norm=0.0022071038838475943 | selected_counts=[1, 3, 5, 2, 2, 7, 1, 4]\n",
      "[epoch 5 batch 41] total_loss=0.013282 | det_loss=0.012596 | tv=0.686706 | nps=0.000000 | grad_norm=0.0027893693186342716 | selected_counts=[3, 1, 4, 4, 1, 3, 13, 3]\n",
      "[epoch 5 batch 42] total_loss=0.010010 | det_loss=0.009323 | tv=0.686573 | nps=0.000000 | grad_norm=0.0018902946030721068 | selected_counts=[10, 5, 11, 1, 3, 2, 11, 1]\n",
      "[epoch 5 batch 43] total_loss=0.011870 | det_loss=0.011184 | tv=0.686442 | nps=0.000000 | grad_norm=0.003213257296010852 | selected_counts=[2, 3, 1, 5, 5, 8, 2, 3]\n",
      "[epoch 5 batch 44] total_loss=0.009259 | det_loss=0.008573 | tv=0.686313 | nps=0.000000 | grad_norm=0.0020279043819755316 | selected_counts=[5, 3, 2, 6, 1, 3, 1, 1]\n",
      "[epoch 5 batch 45] total_loss=0.008946 | det_loss=0.008260 | tv=0.686186 | nps=0.000000 | grad_norm=0.0020198170095682144 | selected_counts=[9, 1, 1, 3, 9, 3, 4, 3]\n",
      "[epoch 5 batch 46] total_loss=0.012410 | det_loss=0.011723 | tv=0.686062 | nps=0.000000 | grad_norm=0.0038640336133539677 | selected_counts=[6, 1, 9, 3, 5, 2, 6, 1]\n",
      "[epoch 5 batch 47] total_loss=0.012960 | det_loss=0.012274 | tv=0.685930 | nps=0.000000 | grad_norm=0.0021019382402300835 | selected_counts=[9, 1, 7, 2, 11, 1, 4, 1]\n",
      "[epoch 5 batch 48] total_loss=0.016007 | det_loss=0.015321 | tv=0.685802 | nps=0.000000 | grad_norm=0.0034860200248658657 | selected_counts=[1, 8, 11, 4, 2, 1, 3, 1]\n",
      "[epoch 5 batch 49] total_loss=0.009062 | det_loss=0.008376 | tv=0.685666 | nps=0.000000 | grad_norm=0.002751811407506466 | selected_counts=[8, 11, 6, 16, 1, 6, 4, 1]\n",
      "[epoch 5 batch 50] total_loss=0.021802 | det_loss=0.021116 | tv=0.685528 | nps=0.000000 | grad_norm=0.0023441663943231106 | selected_counts=[1, 1, 2, 11, 0, 7, 0, 5]\n",
      "[epoch 5 batch 51] total_loss=0.010350 | det_loss=0.009665 | tv=0.685394 | nps=0.000000 | grad_norm=0.0024301682133227587 | selected_counts=[0, 2, 1, 1, 8, 7, 6, 1]\n",
      "[epoch 5 batch 52] total_loss=0.018325 | det_loss=0.017640 | tv=0.685261 | nps=0.000000 | grad_norm=0.0027372862678021193 | selected_counts=[3, 2, 3, 4, 8, 1, 1, 2]\n",
      "[epoch 5 batch 53] total_loss=0.018867 | det_loss=0.018182 | tv=0.685124 | nps=0.000000 | grad_norm=0.004775838926434517 | selected_counts=[7, 2, 2, 3, 1, 7, 10, 8]\n",
      "[epoch 5 batch 54] total_loss=0.007992 | det_loss=0.007307 | tv=0.684979 | nps=0.000000 | grad_norm=0.0015605748631060123 | selected_counts=[8, 2, 8, 11, 1, 4, 8, 1]\n",
      "[epoch 5 batch 55] total_loss=0.010500 | det_loss=0.009815 | tv=0.684836 | nps=0.000000 | grad_norm=0.0020837741903960705 | selected_counts=[2, 2, 7, 14, 3, 1, 7, 16]\n",
      "[epoch 5 batch 56] total_loss=0.020567 | det_loss=0.019882 | tv=0.684696 | nps=0.000000 | grad_norm=0.003136333543807268 | selected_counts=[1, 4, 1, 2, 2, 1, 8, 7]\n",
      "[epoch 5 batch 57] total_loss=0.012995 | det_loss=0.012311 | tv=0.684552 | nps=0.000000 | grad_norm=0.002995030954480171 | selected_counts=[1, 3, 2, 6, 1, 2, 4, 7]\n",
      "[epoch 5 batch 58] total_loss=0.015407 | det_loss=0.014723 | tv=0.684409 | nps=0.000000 | grad_norm=0.004185568541288376 | selected_counts=[9, 8, 1, 1, 4, 3, 5, 11]\n",
      "[epoch 5 batch 59] total_loss=0.013281 | det_loss=0.012596 | tv=0.684266 | nps=0.000000 | grad_norm=0.002055460587143898 | selected_counts=[3, 3, 3, 6, 8, 3, 1, 10]\n",
      "[epoch 5 batch 60] total_loss=0.010725 | det_loss=0.010041 | tv=0.684126 | nps=0.000000 | grad_norm=0.0026231897063553333 | selected_counts=[3, 2, 3, 4, 14, 2, 10, 2]\n",
      "[epoch 5 batch 61] total_loss=0.030465 | det_loss=0.029781 | tv=0.683987 | nps=0.000000 | grad_norm=0.003955966793000698 | selected_counts=[1, 2, 1, 5, 1, 7, 6, 1]\n",
      "[epoch 5 batch 62] total_loss=0.005634 | det_loss=0.004950 | tv=0.683841 | nps=0.000000 | grad_norm=0.0013930058339610696 | selected_counts=[12, 1, 5, 6, 8, 4, 6, 3]\n",
      "[epoch 5 batch 63] total_loss=0.012366 | det_loss=0.011682 | tv=0.683701 | nps=0.000000 | grad_norm=0.0028416551649570465 | selected_counts=[1, 1, 2, 10, 4, 6, 10, 5]\n",
      "[epoch 5 batch 64] total_loss=0.008998 | det_loss=0.008314 | tv=0.683566 | nps=0.000000 | grad_norm=0.003611927619203925 | selected_counts=[2, 7, 3, 1, 6, 8, 1, 4]\n",
      "[epoch 5 batch 65] total_loss=0.013162 | det_loss=0.012479 | tv=0.683428 | nps=0.000000 | grad_norm=0.001873071538284421 | selected_counts=[5, 4, 6, 1, 5, 1, 0, 7]\n",
      "[epoch 5 batch 66] total_loss=0.014654 | det_loss=0.013971 | tv=0.683292 | nps=0.000000 | grad_norm=0.003774260403588414 | selected_counts=[2, 2, 1, 3, 4, 1, 3, 3]\n",
      "[epoch 5 batch 67] total_loss=0.006043 | det_loss=0.005360 | tv=0.683152 | nps=0.000000 | grad_norm=0.0013774478575214744 | selected_counts=[1, 11, 10, 9, 20, 4, 8, 6]\n",
      "[epoch 5 batch 68] total_loss=0.003649 | det_loss=0.002966 | tv=0.683016 | nps=0.000000 | grad_norm=0.0007549186702817678 | selected_counts=[10, 6, 13, 6, 1, 2, 3, 12]\n",
      "[epoch 5 batch 69] total_loss=0.005697 | det_loss=0.005015 | tv=0.682887 | nps=0.000000 | grad_norm=0.0021844503935426474 | selected_counts=[1, 4, 5, 5, 3, 8, 9, 1]\n",
      "[epoch 5 batch 70] total_loss=0.008114 | det_loss=0.007431 | tv=0.682761 | nps=0.000000 | grad_norm=0.0016781004378572106 | selected_counts=[5, 3, 3, 4, 2, 1, 2, 1]\n",
      "[epoch 5 batch 71] total_loss=0.012729 | det_loss=0.012046 | tv=0.682639 | nps=0.000000 | grad_norm=0.003552801441401243 | selected_counts=[2, 5, 7, 6, 3, 1, 8, 1]\n",
      "[epoch 5 batch 72] total_loss=0.010008 | det_loss=0.009326 | tv=0.682510 | nps=0.000000 | grad_norm=0.002818533219397068 | selected_counts=[4, 7, 2, 10, 3, 1, 3, 1]\n",
      "[epoch 5 batch 73] total_loss=0.016767 | det_loss=0.016085 | tv=0.682378 | nps=0.000000 | grad_norm=0.004359917715191841 | selected_counts=[9, 1, 2, 5, 4, 10, 3, 13]\n",
      "[epoch 5 batch 74] total_loss=0.008703 | det_loss=0.008021 | tv=0.682239 | nps=0.000000 | grad_norm=0.0016880513867363334 | selected_counts=[1, 2, 5, 1, 1, 4, 1, 1]\n",
      "[epoch 5 batch 75] total_loss=0.009585 | det_loss=0.008903 | tv=0.682103 | nps=0.000000 | grad_norm=0.002484226133674383 | selected_counts=[3, 7, 4, 15, 2, 3, 3, 4]\n",
      "[epoch 5 batch 76] total_loss=0.010986 | det_loss=0.010304 | tv=0.681968 | nps=0.000000 | grad_norm=0.0019503963412716985 | selected_counts=[1, 2, 5, 1, 7, 2]\n",
      "Epoch 5 saved patch snapshot.\n",
      "[epoch 6 batch 0] total_loss=0.010262 | det_loss=0.009580 | tv=0.681836 | nps=0.000000 | grad_norm=0.0023207569029182196 | selected_counts=[11, 7, 1, 8, 4, 2, 8, 12]\n",
      "[epoch 6 batch 1] total_loss=0.014289 | det_loss=0.013608 | tv=0.681704 | nps=0.000000 | grad_norm=0.0025842594914138317 | selected_counts=[5, 9, 3, 1, 1, 3, 1, 2]\n",
      "[epoch 6 batch 2] total_loss=0.007969 | det_loss=0.007287 | tv=0.681575 | nps=0.000000 | grad_norm=0.0025583640672266483 | selected_counts=[1, 3, 1, 9, 6, 10, 1, 1]\n",
      "[epoch 6 batch 3] total_loss=0.006655 | det_loss=0.005973 | tv=0.681442 | nps=0.000000 | grad_norm=0.0016275752568617463 | selected_counts=[1, 12, 3, 3, 4, 2, 1, 2]\n",
      "[epoch 6 batch 4] total_loss=0.018225 | det_loss=0.017544 | tv=0.681314 | nps=0.000000 | grad_norm=0.003380016889423132 | selected_counts=[3, 6, 1, 3, 7, 1, 1, 2]\n",
      "[epoch 6 batch 5] total_loss=0.020555 | det_loss=0.019874 | tv=0.681179 | nps=0.000000 | grad_norm=0.0037786720786243677 | selected_counts=[12, 6, 5, 1, 3, 1, 5, 1]\n",
      "[epoch 6 batch 6] total_loss=0.015258 | det_loss=0.014577 | tv=0.681042 | nps=0.000000 | grad_norm=0.0020314680878072977 | selected_counts=[1, 10, 1, 6, 0, 8, 4, 12]\n",
      "[epoch 6 batch 7] total_loss=0.018571 | det_loss=0.017890 | tv=0.680909 | nps=0.000000 | grad_norm=0.003643253818154335 | selected_counts=[3, 4, 2, 8, 1, 1, 1, 7]\n",
      "[epoch 6 batch 8] total_loss=0.016915 | det_loss=0.016234 | tv=0.680772 | nps=0.000000 | grad_norm=0.0036428405437618494 | selected_counts=[8, 1, 4, 9, 4, 3, 1, 11]\n",
      "[epoch 6 batch 9] total_loss=0.014258 | det_loss=0.013577 | tv=0.680632 | nps=0.000000 | grad_norm=0.0024132574908435345 | selected_counts=[7, 13, 3, 2, 3, 1, 1, 5]\n",
      "[epoch 6 batch 10] total_loss=0.008441 | det_loss=0.007760 | tv=0.680492 | nps=0.000000 | grad_norm=0.0021737071219831705 | selected_counts=[2, 1, 1, 6, 6, 2, 7, 1]\n",
      "[epoch 6 batch 11] total_loss=0.008212 | det_loss=0.007532 | tv=0.680355 | nps=0.000000 | grad_norm=0.001830419059842825 | selected_counts=[12, 2, 2, 2, 2, 4, 3, 1]\n",
      "[epoch 6 batch 12] total_loss=0.008956 | det_loss=0.008275 | tv=0.680223 | nps=0.000000 | grad_norm=0.0017206259071826935 | selected_counts=[6, 4, 2, 4, 9, 3, 1, 1]\n",
      "[epoch 6 batch 13] total_loss=0.011690 | det_loss=0.011010 | tv=0.680094 | nps=0.000000 | grad_norm=0.0032870424911379814 | selected_counts=[7, 3, 9, 4, 4, 9, 1, 0]\n",
      "[epoch 6 batch 14] total_loss=0.007499 | det_loss=0.006819 | tv=0.679963 | nps=0.000000 | grad_norm=0.0027340110391378403 | selected_counts=[2, 18, 5, 8, 6, 5, 2, 1]\n",
      "[epoch 6 batch 15] total_loss=0.010200 | det_loss=0.009520 | tv=0.679836 | nps=0.000000 | grad_norm=0.0020951328333467245 | selected_counts=[8, 8, 1, 7, 7, 1, 3, 1]\n",
      "[epoch 6 batch 16] total_loss=0.010583 | det_loss=0.009903 | tv=0.679710 | nps=0.000000 | grad_norm=0.002043955959379673 | selected_counts=[10, 3, 12, 1, 5, 19, 2, 1]\n",
      "[epoch 6 batch 17] total_loss=0.011514 | det_loss=0.010834 | tv=0.679586 | nps=0.000000 | grad_norm=0.007434263825416565 | selected_counts=[2, 2, 9, 0, 3, 8, 3, 3]\n",
      "[epoch 6 batch 18] total_loss=0.012684 | det_loss=0.012005 | tv=0.679448 | nps=0.000000 | grad_norm=0.0016547897830605507 | selected_counts=[1, 1, 2, 4, 2, 2, 3, 5]\n",
      "[epoch 6 batch 19] total_loss=0.009870 | det_loss=0.009190 | tv=0.679310 | nps=0.000000 | grad_norm=0.0017360110068693757 | selected_counts=[3, 4, 3, 1, 6, 3, 1, 4]\n",
      "[epoch 6 batch 20] total_loss=0.009985 | det_loss=0.009306 | tv=0.679176 | nps=0.000000 | grad_norm=0.0030526616610586643 | selected_counts=[1, 2, 2, 2, 10, 1, 3, 5]\n",
      "[epoch 6 batch 21] total_loss=0.015767 | det_loss=0.015088 | tv=0.679043 | nps=0.000000 | grad_norm=0.002618141006678343 | selected_counts=[1, 4, 5, 1, 2, 1, 9, 5]\n",
      "[epoch 6 batch 22] total_loss=0.003474 | det_loss=0.002795 | tv=0.678912 | nps=0.000000 | grad_norm=0.0008759041083976626 | selected_counts=[2, 6, 3, 10, 10, 7, 4, 2]\n",
      "[epoch 6 batch 23] total_loss=0.015533 | det_loss=0.014854 | tv=0.678786 | nps=0.000000 | grad_norm=0.0020014881156384945 | selected_counts=[11, 2, 1, 10, 1, 1, 1, 3]\n",
      "[epoch 6 batch 24] total_loss=0.020782 | det_loss=0.020103 | tv=0.678662 | nps=0.000000 | grad_norm=0.0035304706543684006 | selected_counts=[2, 1, 11, 1, 8, 2, 10, 1]\n",
      "[epoch 6 batch 25] total_loss=0.014718 | det_loss=0.014040 | tv=0.678534 | nps=0.000000 | grad_norm=0.003349816193804145 | selected_counts=[2, 2, 4, 9, 4, 2, 7, 2]\n",
      "[epoch 6 batch 26] total_loss=0.008160 | det_loss=0.007482 | tv=0.678400 | nps=0.000000 | grad_norm=0.0018337818328291178 | selected_counts=[10, 6, 2, 4, 5, 2, 9, 1]\n",
      "[epoch 6 batch 27] total_loss=0.010259 | det_loss=0.009580 | tv=0.678270 | nps=0.000000 | grad_norm=0.001847007661126554 | selected_counts=[6, 1, 2, 3, 10, 7, 10, 1]\n",
      "[epoch 6 batch 28] total_loss=0.004871 | det_loss=0.004192 | tv=0.678145 | nps=0.000000 | grad_norm=0.001956470776349306 | selected_counts=[1, 3, 2, 14, 1, 4, 8, 7]\n",
      "[epoch 6 batch 29] total_loss=0.008640 | det_loss=0.007962 | tv=0.678024 | nps=0.000000 | grad_norm=0.001754605327732861 | selected_counts=[3, 1, 1, 2, 1, 12, 2, 3]\n",
      "[epoch 6 batch 30] total_loss=0.011876 | det_loss=0.011198 | tv=0.677908 | nps=0.000000 | grad_norm=0.003676259657368064 | selected_counts=[7, 10, 1, 1, 1, 2, 8, 5]\n",
      "[epoch 6 batch 31] total_loss=0.015312 | det_loss=0.014634 | tv=0.677789 | nps=0.000000 | grad_norm=0.0033543205354362726 | selected_counts=[4, 4, 5, 2, 1, 8, 1, 4]\n",
      "[epoch 6 batch 32] total_loss=0.011038 | det_loss=0.010361 | tv=0.677666 | nps=0.000000 | grad_norm=0.004445389378815889 | selected_counts=[6, 3, 8, 5, 1, 8, 2, 9]\n",
      "[epoch 6 batch 33] total_loss=0.019340 | det_loss=0.018663 | tv=0.677536 | nps=0.000000 | grad_norm=0.002818564185872674 | selected_counts=[1, 15, 1, 1, 2, 6, 2, 3]\n",
      "[epoch 6 batch 34] total_loss=0.014902 | det_loss=0.014224 | tv=0.677407 | nps=0.000000 | grad_norm=0.0030705872923135757 | selected_counts=[2, 9, 15, 1, 1, 3, 4, 3]\n",
      "[epoch 6 batch 35] total_loss=0.012482 | det_loss=0.011804 | tv=0.677275 | nps=0.000000 | grad_norm=0.002539447508752346 | selected_counts=[1, 3, 5, 9, 2, 3, 3, 9]\n",
      "[epoch 6 batch 36] total_loss=0.008775 | det_loss=0.008098 | tv=0.677142 | nps=0.000000 | grad_norm=0.0024508151691406965 | selected_counts=[1, 10, 6, 1, 1, 9, 2, 9]\n",
      "[epoch 6 batch 37] total_loss=0.008125 | det_loss=0.007448 | tv=0.677011 | nps=0.000000 | grad_norm=0.0019296029349789023 | selected_counts=[1, 11, 1, 9, 2, 8, 8, 1]\n",
      "[epoch 6 batch 38] total_loss=0.013342 | det_loss=0.012665 | tv=0.676883 | nps=0.000000 | grad_norm=0.0026178890839219093 | selected_counts=[1, 1, 1, 7, 1, 12, 1, 1]\n",
      "[epoch 6 batch 39] total_loss=0.009852 | det_loss=0.009176 | tv=0.676755 | nps=0.000000 | grad_norm=0.002300020307302475 | selected_counts=[3, 15, 3, 7, 1, 4, 4, 6]\n",
      "[epoch 6 batch 40] total_loss=0.014536 | det_loss=0.013859 | tv=0.676628 | nps=0.000000 | grad_norm=0.002378883073106408 | selected_counts=[1, 3, 5, 2, 1, 0, 4, 1]\n",
      "[epoch 6 batch 41] total_loss=0.013242 | det_loss=0.012566 | tv=0.676503 | nps=0.000000 | grad_norm=0.0037799477577209473 | selected_counts=[1, 1, 9, 4, 3, 11, 12, 1]\n",
      "[epoch 6 batch 42] total_loss=0.004688 | det_loss=0.004011 | tv=0.676372 | nps=0.000000 | grad_norm=0.0013780350564047694 | selected_counts=[14, 9, 1, 11, 1, 1, 12, 12]\n",
      "[epoch 6 batch 43] total_loss=0.009632 | det_loss=0.008955 | tv=0.676246 | nps=0.000000 | grad_norm=0.0038270007353276014 | selected_counts=[6, 6, 1, 5, 4, 4, 10, 1]\n",
      "[epoch 6 batch 44] total_loss=0.006084 | det_loss=0.005408 | tv=0.676116 | nps=0.000000 | grad_norm=0.0016282802680507302 | selected_counts=[2, 8, 5, 4, 2, 2, 7, 1]\n",
      "[epoch 6 batch 45] total_loss=0.006762 | det_loss=0.006086 | tv=0.675991 | nps=0.000000 | grad_norm=0.0016866306541487575 | selected_counts=[2, 4, 5, 5, 1, 5, 0, 2]\n",
      "[epoch 6 batch 46] total_loss=0.011574 | det_loss=0.010898 | tv=0.675869 | nps=0.000000 | grad_norm=0.0032945536077022552 | selected_counts=[3, 2, 2, 1, 1, 4, 8, 3]\n",
      "[epoch 6 batch 47] total_loss=0.009747 | det_loss=0.009071 | tv=0.675744 | nps=0.000000 | grad_norm=0.002510779071599245 | selected_counts=[3, 3, 2, 5, 2, 1, 2, 4]\n",
      "[epoch 6 batch 48] total_loss=0.014681 | det_loss=0.014006 | tv=0.675621 | nps=0.000000 | grad_norm=0.0029761099722236395 | selected_counts=[2, 14, 1, 2, 3, 9, 2, 7]\n",
      "[epoch 6 batch 49] total_loss=0.007324 | det_loss=0.006648 | tv=0.675497 | nps=0.000000 | grad_norm=0.0007218851242214441 | selected_counts=[2, 4, 8, 2, 0, 9, 9, 5]\n",
      "[epoch 6 batch 50] total_loss=0.013552 | det_loss=0.012877 | tv=0.675379 | nps=0.000000 | grad_norm=0.0021625817753374577 | selected_counts=[1, 1, 3, 1, 5, 4, 1, 1]\n",
      "[epoch 6 batch 51] total_loss=0.008137 | det_loss=0.007462 | tv=0.675260 | nps=0.000000 | grad_norm=0.0024417091626673937 | selected_counts=[2, 5, 5, 4, 11, 1, 1, 5]\n",
      "[epoch 6 batch 52] total_loss=0.006365 | det_loss=0.005689 | tv=0.675141 | nps=0.000000 | grad_norm=0.002018141560256481 | selected_counts=[6, 5, 4, 4, 2, 4, 11, 15]\n",
      "[epoch 6 batch 53] total_loss=0.011131 | det_loss=0.010456 | tv=0.675023 | nps=0.000000 | grad_norm=0.001919219153933227 | selected_counts=[1, 4, 5, 2, 1, 3, 1, 3]\n",
      "[epoch 6 batch 54] total_loss=0.010110 | det_loss=0.009435 | tv=0.674904 | nps=0.000000 | grad_norm=0.001439772779121995 | selected_counts=[3, 5, 17, 5, 1, 3, 1, 4]\n",
      "[epoch 6 batch 55] total_loss=0.014383 | det_loss=0.013708 | tv=0.674789 | nps=0.000000 | grad_norm=0.0027896405663341284 | selected_counts=[5, 2, 8, 1, 15, 4, 2, 4]\n",
      "[epoch 6 batch 56] total_loss=0.007164 | det_loss=0.006490 | tv=0.674672 | nps=0.000000 | grad_norm=0.0022085311356931925 | selected_counts=[6, 4, 7, 2, 7, 4, 5, 3]\n",
      "[epoch 6 batch 57] total_loss=0.007560 | det_loss=0.006886 | tv=0.674556 | nps=0.000000 | grad_norm=0.0023922899272292852 | selected_counts=[4, 2, 2, 2, 2, 4, 1, 1]\n",
      "[epoch 6 batch 58] total_loss=0.005864 | det_loss=0.005189 | tv=0.674440 | nps=0.000000 | grad_norm=0.0011919628595933318 | selected_counts=[1, 6, 9, 4, 6, 1, 3, 1]\n",
      "[epoch 6 batch 59] total_loss=0.007577 | det_loss=0.006903 | tv=0.674329 | nps=0.000000 | grad_norm=0.0015365533763542771 | selected_counts=[7, 4, 1, 7, 3, 7, 1, 8]\n",
      "[epoch 6 batch 60] total_loss=0.014594 | det_loss=0.013920 | tv=0.674217 | nps=0.000000 | grad_norm=0.0049065095372498035 | selected_counts=[3, 2, 5, 5, 8, 7, 8, 2]\n",
      "[epoch 6 batch 61] total_loss=0.008309 | det_loss=0.007635 | tv=0.674091 | nps=0.000000 | grad_norm=0.0021335480269044638 | selected_counts=[2, 3, 6, 6, 3, 7, 2, 1]\n",
      "[epoch 6 batch 62] total_loss=0.015520 | det_loss=0.014846 | tv=0.673963 | nps=0.000000 | grad_norm=0.0024107303470373154 | selected_counts=[1, 1, 1, 6, 1, 3, 3, 1]\n",
      "[epoch 6 batch 63] total_loss=0.010793 | det_loss=0.010119 | tv=0.673836 | nps=0.000000 | grad_norm=0.0026339932810515165 | selected_counts=[3, 6, 1, 2, 4, 5, 12, 8]\n",
      "[epoch 6 batch 64] total_loss=0.009588 | det_loss=0.008914 | tv=0.673709 | nps=0.000000 | grad_norm=0.0030308649875223637 | selected_counts=[1, 2, 3, 5, 3, 5, 6, 5]\n",
      "[epoch 6 batch 65] total_loss=0.020743 | det_loss=0.020070 | tv=0.673578 | nps=0.000000 | grad_norm=0.003858991200104356 | selected_counts=[1, 1, 1, 3, 3, 2, 2, 7]\n",
      "[epoch 6 batch 66] total_loss=0.012868 | det_loss=0.012194 | tv=0.673441 | nps=0.000000 | grad_norm=0.0027505967300385237 | selected_counts=[1, 4, 1, 1, 4, 5, 1, 16]\n",
      "[epoch 6 batch 67] total_loss=0.010018 | det_loss=0.009344 | tv=0.673304 | nps=0.000000 | grad_norm=0.0027193408459424973 | selected_counts=[7, 2, 9, 5, 4, 9, 3, 7]\n",
      "[epoch 6 batch 68] total_loss=0.009391 | det_loss=0.008718 | tv=0.673169 | nps=0.000000 | grad_norm=0.0016819237498566508 | selected_counts=[1, 7, 12, 1, 7, 7, 4, 2]\n",
      "[epoch 6 batch 69] total_loss=0.016681 | det_loss=0.016008 | tv=0.673039 | nps=0.000000 | grad_norm=0.0028205353301018476 | selected_counts=[2, 9, 8, 2, 0, 3, 4, 8]\n",
      "[epoch 6 batch 70] total_loss=0.013286 | det_loss=0.012613 | tv=0.672905 | nps=0.000000 | grad_norm=0.0024629440158605576 | selected_counts=[1, 1, 5, 3, 6, 6, 4, 8]\n",
      "[epoch 6 batch 71] total_loss=0.007032 | det_loss=0.006359 | tv=0.672773 | nps=0.000000 | grad_norm=0.0016462901839986444 | selected_counts=[1, 4, 4, 8, 2, 6, 1, 10]\n",
      "[epoch 6 batch 72] total_loss=0.021949 | det_loss=0.021276 | tv=0.672644 | nps=0.000000 | grad_norm=0.003121128538623452 | selected_counts=[11, 2, 1, 12, 3, 1, 4, 2]\n",
      "[epoch 6 batch 73] total_loss=0.010442 | det_loss=0.009769 | tv=0.672510 | nps=0.000000 | grad_norm=0.002137697534635663 | selected_counts=[5, 6, 1, 5, 0, 3, 4, 2]\n",
      "[epoch 6 batch 74] total_loss=0.009937 | det_loss=0.009264 | tv=0.672379 | nps=0.000000 | grad_norm=0.003483725478872657 | selected_counts=[2, 1, 1, 3, 6, 7, 1, 2]\n",
      "[epoch 6 batch 75] total_loss=0.011815 | det_loss=0.011142 | tv=0.672245 | nps=0.000000 | grad_norm=0.002328911330550909 | selected_counts=[1, 3, 3, 2, 4, 1, 11, 11]\n",
      "[epoch 6 batch 76] total_loss=0.003197 | det_loss=0.002525 | tv=0.672112 | nps=0.000000 | grad_norm=0.0018071187660098076 | selected_counts=[5, 7, 7, 1, 6, 5]\n",
      "Epoch 6 saved patch snapshot.\n",
      "[epoch 7 batch 0] total_loss=0.010952 | det_loss=0.010280 | tv=0.671984 | nps=0.000000 | grad_norm=0.0021911358926445246 | selected_counts=[13, 2, 11, 1, 4, 2, 1, 1]\n",
      "[epoch 7 batch 1] total_loss=0.010699 | det_loss=0.010027 | tv=0.671858 | nps=0.000000 | grad_norm=0.002001185202971101 | selected_counts=[3, 6, 6, 1, 1, 1, 4, 6]\n",
      "[epoch 7 batch 2] total_loss=0.012393 | det_loss=0.011721 | tv=0.671735 | nps=0.000000 | grad_norm=0.0025511588901281357 | selected_counts=[5, 6, 3, 2, 2, 5, 2, 10]\n",
      "[epoch 7 batch 3] total_loss=0.011235 | det_loss=0.010563 | tv=0.671615 | nps=0.000000 | grad_norm=0.0019969569984823465 | selected_counts=[6, 1, 13, 5, 7, 1, 1, 9]\n",
      "[epoch 7 batch 4] total_loss=0.004556 | det_loss=0.003885 | tv=0.671498 | nps=0.000000 | grad_norm=0.0013188119046390057 | selected_counts=[6, 3, 1, 3, 4, 1, 1, 8]\n",
      "[epoch 7 batch 5] total_loss=0.015315 | det_loss=0.014643 | tv=0.671384 | nps=0.000000 | grad_norm=0.0035842827055603266 | selected_counts=[3, 2, 7, 14, 3, 1, 1, 10]\n",
      "[epoch 7 batch 6] total_loss=0.011040 | det_loss=0.010369 | tv=0.671265 | nps=0.000000 | grad_norm=0.00265339738689363 | selected_counts=[1, 4, 2, 1, 3, 1, 3, 3]\n",
      "[epoch 7 batch 7] total_loss=0.012511 | det_loss=0.011840 | tv=0.671142 | nps=0.000000 | grad_norm=0.00286427466198802 | selected_counts=[2, 14, 4, 2, 5, 5, 3, 7]\n",
      "[epoch 7 batch 8] total_loss=0.008279 | det_loss=0.007608 | tv=0.671014 | nps=0.000000 | grad_norm=0.00218663876876235 | selected_counts=[11, 3, 5, 3, 2, 12, 2, 4]\n",
      "[epoch 7 batch 9] total_loss=0.014299 | det_loss=0.013628 | tv=0.670887 | nps=0.000000 | grad_norm=0.0027855802327394485 | selected_counts=[1, 3, 1, 6, 5, 1, 11, 0]\n",
      "[epoch 7 batch 10] total_loss=0.007857 | det_loss=0.007186 | tv=0.670760 | nps=0.000000 | grad_norm=0.0019404781050980091 | selected_counts=[11, 2, 2, 2, 2, 4, 4, 10]\n",
      "[epoch 7 batch 11] total_loss=0.005213 | det_loss=0.004543 | tv=0.670636 | nps=0.000000 | grad_norm=0.0015774430939927697 | selected_counts=[4, 5, 13, 4, 2, 9, 1, 10]\n",
      "[epoch 7 batch 12] total_loss=0.005166 | det_loss=0.004495 | tv=0.670514 | nps=0.000000 | grad_norm=0.0009648384293541312 | selected_counts=[11, 5, 1, 3, 1, 3, 6, 2]\n",
      "[epoch 7 batch 13] total_loss=0.015638 | det_loss=0.014968 | tv=0.670398 | nps=0.000000 | grad_norm=0.001610334264114499 | selected_counts=[1, 5, 4, 1, 2, 2, 5, 1]\n",
      "[epoch 7 batch 14] total_loss=0.008906 | det_loss=0.008236 | tv=0.670286 | nps=0.000000 | grad_norm=0.0015590391121804714 | selected_counts=[4, 9, 3, 1, 7, 4, 1, 3]\n",
      "[epoch 7 batch 15] total_loss=0.019463 | det_loss=0.018792 | tv=0.670177 | nps=0.000000 | grad_norm=0.002472338732331991 | selected_counts=[1, 1, 1, 4, 1, 2, 6, 17]\n",
      "[epoch 7 batch 16] total_loss=0.011280 | det_loss=0.010610 | tv=0.670067 | nps=0.000000 | grad_norm=0.0016771960072219372 | selected_counts=[9, 5, 2, 1, 8, 1, 7, 1]\n",
      "[epoch 7 batch 17] total_loss=0.006125 | det_loss=0.005455 | tv=0.669957 | nps=0.000000 | grad_norm=0.0010402490152046084 | selected_counts=[0, 5, 11, 3, 1, 2, 12, 10]\n",
      "[epoch 7 batch 18] total_loss=0.021448 | det_loss=0.020778 | tv=0.669852 | nps=0.000000 | grad_norm=0.0029953033663332462 | selected_counts=[3, 7, 3, 2, 2, 9, 1, 2]\n",
      "[epoch 7 batch 19] total_loss=0.009340 | det_loss=0.008671 | tv=0.669744 | nps=0.000000 | grad_norm=0.004009311553090811 | selected_counts=[1, 4, 7, 3, 2, 7, 2, 3]\n",
      "[epoch 7 batch 20] total_loss=0.005555 | det_loss=0.004886 | tv=0.669629 | nps=0.000000 | grad_norm=0.0015584020875394344 | selected_counts=[8, 4, 1, 1, 5, 1, 4, 8]\n",
      "[epoch 7 batch 21] total_loss=0.013161 | det_loss=0.012492 | tv=0.669516 | nps=0.000000 | grad_norm=0.003746308386325836 | selected_counts=[5, 1, 1, 4, 5, 6, 7, 11]\n",
      "[epoch 7 batch 22] total_loss=0.007213 | det_loss=0.006544 | tv=0.669392 | nps=0.000000 | grad_norm=0.002196161774918437 | selected_counts=[14, 3, 3, 1, 11, 7, 2, 4]\n",
      "[epoch 7 batch 23] total_loss=0.011968 | det_loss=0.011298 | tv=0.669269 | nps=0.000000 | grad_norm=0.0022200371604412794 | selected_counts=[7, 2, 1, 5, 3, 3, 1, 6]\n",
      "[epoch 7 batch 24] total_loss=0.019903 | det_loss=0.019234 | tv=0.669146 | nps=0.000000 | grad_norm=0.002617397578433156 | selected_counts=[9, 1, 2, 3, 1, 8, 13, 3]\n",
      "[epoch 7 batch 25] total_loss=0.004831 | det_loss=0.004162 | tv=0.669024 | nps=0.000000 | grad_norm=0.002005701418966055 | selected_counts=[6, 7, 10, 9, 8, 2, 3, 18]\n",
      "[epoch 7 batch 26] total_loss=0.013637 | det_loss=0.012968 | tv=0.668902 | nps=0.000000 | grad_norm=0.0029743006452918053 | selected_counts=[3, 9, 1, 7, 3, 2, 1, 1]\n",
      "[epoch 7 batch 27] total_loss=0.015037 | det_loss=0.014368 | tv=0.668779 | nps=0.000000 | grad_norm=0.005885913968086243 | selected_counts=[2, 7, 1, 7, 5, 9, 7, 3]\n",
      "[epoch 7 batch 28] total_loss=0.009730 | det_loss=0.009061 | tv=0.668641 | nps=0.000000 | grad_norm=0.0027776327915489674 | selected_counts=[3, 6, 1, 1, 7, 2, 5, 1]\n",
      "[epoch 7 batch 29] total_loss=0.005633 | det_loss=0.004964 | tv=0.668502 | nps=0.000000 | grad_norm=0.0012758991215378046 | selected_counts=[4, 1, 10, 6, 1, 3, 12, 9]\n",
      "[epoch 7 batch 30] total_loss=0.003257 | det_loss=0.002589 | tv=0.668369 | nps=0.000000 | grad_norm=0.0010442485800012946 | selected_counts=[9, 9, 11, 3, 7, 2, 11, 1]\n",
      "[epoch 7 batch 31] total_loss=0.012554 | det_loss=0.011886 | tv=0.668241 | nps=0.000000 | grad_norm=0.0025549831334501505 | selected_counts=[14, 1, 2, 5, 4, 0, 15, 1]\n",
      "[epoch 7 batch 32] total_loss=0.007410 | det_loss=0.006741 | tv=0.668117 | nps=0.000000 | grad_norm=0.0013986057601869106 | selected_counts=[10, 8, 3, 4, 8, 1, 1, 3]\n",
      "[epoch 7 batch 33] total_loss=0.014428 | det_loss=0.013760 | tv=0.667996 | nps=0.000000 | grad_norm=0.007593974005430937 | selected_counts=[6, 11, 8, 1, 3, 4, 2, 7]\n",
      "[epoch 7 batch 34] total_loss=0.011219 | det_loss=0.010552 | tv=0.667855 | nps=0.000000 | grad_norm=0.002691359492018819 | selected_counts=[9, 1, 1, 2, 5, 1, 1, 2]\n",
      "[epoch 7 batch 35] total_loss=0.015583 | det_loss=0.014916 | tv=0.667712 | nps=0.000000 | grad_norm=0.0043061766773462296 | selected_counts=[1, 4, 5, 2, 1, 2, 15, 2]\n",
      "[epoch 7 batch 36] total_loss=0.007083 | det_loss=0.006416 | tv=0.667562 | nps=0.000000 | grad_norm=0.0012194685405120254 | selected_counts=[1, 2, 3, 7, 1, 6, 1, 4]\n",
      "[epoch 7 batch 37] total_loss=0.004910 | det_loss=0.004242 | tv=0.667417 | nps=0.000000 | grad_norm=0.0016438767779618502 | selected_counts=[3, 7, 7, 10, 4, 2, 5, 4]\n",
      "[epoch 7 batch 38] total_loss=0.008231 | det_loss=0.007564 | tv=0.667276 | nps=0.000000 | grad_norm=0.00228896108455956 | selected_counts=[2, 1, 7, 1, 2, 12, 1, 4]\n",
      "[epoch 7 batch 39] total_loss=0.013584 | det_loss=0.012917 | tv=0.667139 | nps=0.000000 | grad_norm=0.0029653480742126703 | selected_counts=[2, 4, 3, 9, 5, 4, 9, 1]\n",
      "[epoch 7 batch 40] total_loss=0.011106 | det_loss=0.010439 | tv=0.666997 | nps=0.000000 | grad_norm=0.001621146104298532 | selected_counts=[6, 10, 1, 0, 7, 1, 5, 5]\n",
      "[epoch 7 batch 41] total_loss=0.009200 | det_loss=0.008533 | tv=0.666860 | nps=0.000000 | grad_norm=0.002186531201004982 | selected_counts=[1, 9, 4, 1, 7, 2, 6, 5]\n",
      "[epoch 7 batch 42] total_loss=0.009520 | det_loss=0.008853 | tv=0.666727 | nps=0.000000 | grad_norm=0.001759061822667718 | selected_counts=[1, 6, 1, 2, 7, 3, 10, 1]\n",
      "[epoch 7 batch 43] total_loss=0.006230 | det_loss=0.005563 | tv=0.666600 | nps=0.000000 | grad_norm=0.0018414616351947188 | selected_counts=[7, 3, 7, 4, 1, 2, 2, 4]\n",
      "[epoch 7 batch 44] total_loss=0.008139 | det_loss=0.007473 | tv=0.666475 | nps=0.000000 | grad_norm=0.0030521098524332047 | selected_counts=[6, 1, 2, 12, 1, 1, 5, 5]\n",
      "[epoch 7 batch 45] total_loss=0.005755 | det_loss=0.005089 | tv=0.666349 | nps=0.000000 | grad_norm=0.0017625640612095594 | selected_counts=[4, 4, 2, 4, 5, 8, 1, 4]\n",
      "[epoch 7 batch 46] total_loss=0.004043 | det_loss=0.003377 | tv=0.666227 | nps=0.000000 | grad_norm=0.0008152275113388896 | selected_counts=[2, 8, 3, 7, 3, 8, 3, 4]\n",
      "[epoch 7 batch 47] total_loss=0.012867 | det_loss=0.012201 | tv=0.666111 | nps=0.000000 | grad_norm=0.0024066055193543434 | selected_counts=[11, 1, 2, 1, 11, 1, 1, 5]\n",
      "[epoch 7 batch 48] total_loss=0.008514 | det_loss=0.007848 | tv=0.665997 | nps=0.000000 | grad_norm=0.001568664563819766 | selected_counts=[1, 7, 4, 0, 5, 1, 3, 3]\n",
      "[epoch 7 batch 49] total_loss=0.021997 | det_loss=0.021331 | tv=0.665886 | nps=0.000000 | grad_norm=0.005061648786067963 | selected_counts=[1, 2, 1, 1, 4, 1, 2, 2]\n",
      "[epoch 7 batch 50] total_loss=0.013147 | det_loss=0.012481 | tv=0.665759 | nps=0.000000 | grad_norm=0.002621844643726945 | selected_counts=[3, 1, 6, 3, 3, 1, 10, 7]\n",
      "[epoch 7 batch 51] total_loss=0.010958 | det_loss=0.010293 | tv=0.665631 | nps=0.000000 | grad_norm=0.0018361966358497739 | selected_counts=[5, 1, 2, 1, 6, 1, 8, 9]\n",
      "[epoch 7 batch 52] total_loss=0.005993 | det_loss=0.005327 | tv=0.665505 | nps=0.000000 | grad_norm=0.0017571718199178576 | selected_counts=[3, 4, 8, 4, 1, 5, 9, 10]\n",
      "[epoch 7 batch 53] total_loss=0.004353 | det_loss=0.003688 | tv=0.665382 | nps=0.000000 | grad_norm=0.000951040827203542 | selected_counts=[1, 3, 5, 1, 3, 8, 4, 1]\n",
      "[epoch 7 batch 54] total_loss=0.008719 | det_loss=0.008054 | tv=0.665265 | nps=0.000000 | grad_norm=0.0025289023760706186 | selected_counts=[4, 3, 1, 3, 7, 7, 3, 2]\n",
      "[epoch 7 batch 55] total_loss=0.009807 | det_loss=0.009142 | tv=0.665147 | nps=0.000000 | grad_norm=0.003380194306373596 | selected_counts=[1, 1, 16, 3, 7, 14, 1, 1]\n",
      "[epoch 7 batch 56] total_loss=0.009159 | det_loss=0.008494 | tv=0.665025 | nps=0.000000 | grad_norm=0.0019375351257622242 | selected_counts=[3, 10, 12, 2, 1, 7, 6, 1]\n",
      "[epoch 7 batch 57] total_loss=0.009531 | det_loss=0.008866 | tv=0.664904 | nps=0.000000 | grad_norm=0.002926708199083805 | selected_counts=[2, 3, 10, 1, 5, 2, 2, 10]\n",
      "[epoch 7 batch 58] total_loss=0.014243 | det_loss=0.013578 | tv=0.664782 | nps=0.000000 | grad_norm=0.0023933073971420527 | selected_counts=[1, 3, 6, 1, 5, 3, 5, 2]\n",
      "[epoch 7 batch 59] total_loss=0.006527 | det_loss=0.005862 | tv=0.664660 | nps=0.000000 | grad_norm=0.0012982505140826106 | selected_counts=[1, 6, 7, 1, 2, 2, 11, 1]\n",
      "[epoch 7 batch 60] total_loss=0.011607 | det_loss=0.010943 | tv=0.664541 | nps=0.000000 | grad_norm=0.0034009318333119154 | selected_counts=[10, 1, 1, 4, 4, 3, 5, 9]\n",
      "[epoch 7 batch 61] total_loss=0.004585 | det_loss=0.003921 | tv=0.664417 | nps=0.000000 | grad_norm=0.001110056764446199 | selected_counts=[3, 3, 1, 1, 4, 5, 1, 8]\n",
      "[epoch 7 batch 62] total_loss=0.006610 | det_loss=0.005946 | tv=0.664299 | nps=0.000000 | grad_norm=0.0017444443656131625 | selected_counts=[7, 2, 4, 1, 3, 8, 4, 3]\n",
      "[epoch 7 batch 63] total_loss=0.008592 | det_loss=0.007928 | tv=0.664181 | nps=0.000000 | grad_norm=0.0026354275178164244 | selected_counts=[1, 6, 5, 2, 1, 2, 3, 4]\n",
      "[epoch 7 batch 64] total_loss=0.004669 | det_loss=0.004005 | tv=0.664060 | nps=0.000000 | grad_norm=0.0015921486774459481 | selected_counts=[1, 2, 2, 7, 1, 2, 2, 8]\n",
      "[epoch 7 batch 65] total_loss=0.008314 | det_loss=0.007650 | tv=0.663943 | nps=0.000000 | grad_norm=0.0020497667137533426 | selected_counts=[1, 4, 6, 5, 4, 2, 7, 3]\n",
      "[epoch 7 batch 66] total_loss=0.012655 | det_loss=0.011992 | tv=0.663824 | nps=0.000000 | grad_norm=0.0026041667442768812 | selected_counts=[1, 3, 1, 1, 2, 3, 1, 1]\n",
      "[epoch 7 batch 67] total_loss=0.003867 | det_loss=0.003203 | tv=0.663705 | nps=0.000000 | grad_norm=0.001036871806718409 | selected_counts=[5, 7, 10, 7, 7, 8, 1, 5]\n",
      "[epoch 7 batch 68] total_loss=0.005698 | det_loss=0.005035 | tv=0.663590 | nps=0.000000 | grad_norm=0.0031636778730899096 | selected_counts=[1, 8, 7, 5, 1, 4, 4, 8]\n",
      "[epoch 7 batch 69] total_loss=0.006407 | det_loss=0.005744 | tv=0.663475 | nps=0.000000 | grad_norm=0.00425749970600009 | selected_counts=[4, 7, 1, 6, 6, 5, 5, 5]\n",
      "[epoch 7 batch 70] total_loss=0.014404 | det_loss=0.013741 | tv=0.663356 | nps=0.000000 | grad_norm=0.0019141444936394691 | selected_counts=[7, 1, 1, 1, 1, 2, 6, 1]\n",
      "[epoch 7 batch 71] total_loss=0.014867 | det_loss=0.014204 | tv=0.663236 | nps=0.000000 | grad_norm=0.003894707653671503 | selected_counts=[3, 6, 1, 4, 10, 5, 3, 5]\n",
      "[epoch 7 batch 72] total_loss=0.006013 | det_loss=0.005350 | tv=0.663114 | nps=0.000000 | grad_norm=0.0024473199155181646 | selected_counts=[13, 9, 1, 9, 3, 13, 1, 6]\n",
      "[epoch 7 batch 73] total_loss=0.017043 | det_loss=0.016380 | tv=0.662990 | nps=0.000000 | grad_norm=0.0036503938026726246 | selected_counts=[3, 5, 2, 9, 5, 2, 1, 4]\n",
      "[epoch 7 batch 74] total_loss=0.005784 | det_loss=0.005121 | tv=0.662862 | nps=0.000000 | grad_norm=0.002619117731228471 | selected_counts=[8, 5, 1, 2, 10, 1, 3, 5]\n",
      "[epoch 7 batch 75] total_loss=0.014132 | det_loss=0.013469 | tv=0.662736 | nps=0.000000 | grad_norm=0.003681850852444768 | selected_counts=[5, 2, 2, 1, 3, 7, 13, 2]\n",
      "[epoch 7 batch 76] total_loss=0.009239 | det_loss=0.008576 | tv=0.662606 | nps=0.000000 | grad_norm=0.0034257061779499054 | selected_counts=[1, 6, 7, 6, 2, 1]\n",
      "Epoch 7 saved patch snapshot.\n",
      "[epoch 8 batch 0] total_loss=0.012339 | det_loss=0.011676 | tv=0.662469 | nps=0.000000 | grad_norm=0.005132664926350117 | selected_counts=[8, 2, 4, 4, 2, 1, 2, 6]\n",
      "[epoch 8 batch 1] total_loss=0.007474 | det_loss=0.006812 | tv=0.662324 | nps=0.000000 | grad_norm=0.0021550729870796204 | selected_counts=[7, 10, 1, 7, 7, 10, 6, 5]\n",
      "[epoch 8 batch 2] total_loss=0.006444 | det_loss=0.005782 | tv=0.662181 | nps=0.000000 | grad_norm=0.002394727198407054 | selected_counts=[2, 3, 1, 1, 1, 6, 9, 3]\n",
      "[epoch 8 batch 3] total_loss=0.015707 | det_loss=0.015044 | tv=0.662042 | nps=0.000000 | grad_norm=0.0030924431048333645 | selected_counts=[4, 3, 6, 5, 1, 5, 4, 1]\n",
      "[epoch 8 batch 4] total_loss=0.005220 | det_loss=0.004558 | tv=0.661901 | nps=0.000000 | grad_norm=0.0011361339129507542 | selected_counts=[3, 2, 7, 2, 2, 4, 6, 6]\n",
      "[epoch 8 batch 5] total_loss=0.009571 | det_loss=0.008909 | tv=0.661766 | nps=0.000000 | grad_norm=0.002126560313627124 | selected_counts=[2, 2, 0, 3, 5, 1, 4, 3]\n",
      "[epoch 8 batch 6] total_loss=0.010464 | det_loss=0.009803 | tv=0.661633 | nps=0.000000 | grad_norm=0.0023025476839393377 | selected_counts=[3, 2, 2, 2, 1, 2, 8, 4]\n",
      "[epoch 8 batch 7] total_loss=0.004058 | det_loss=0.003396 | tv=0.661503 | nps=0.000000 | grad_norm=0.0011279791360720992 | selected_counts=[11, 9, 7, 3, 2, 2, 1, 6]\n",
      "[epoch 8 batch 8] total_loss=0.014166 | det_loss=0.013505 | tv=0.661380 | nps=0.000000 | grad_norm=0.0027911749202758074 | selected_counts=[3, 1, 1, 9, 4, 11, 1, 1]\n",
      "[epoch 8 batch 9] total_loss=0.012319 | det_loss=0.011658 | tv=0.661253 | nps=0.000000 | grad_norm=0.0023312498815357685 | selected_counts=[4, 9, 1, 2, 11, 0, 6, 3]\n",
      "[epoch 8 batch 10] total_loss=0.007272 | det_loss=0.006611 | tv=0.661129 | nps=0.000000 | grad_norm=0.0023398001212626696 | selected_counts=[2, 1, 11, 3, 6, 8, 1, 2]\n",
      "[epoch 8 batch 11] total_loss=0.013366 | det_loss=0.012705 | tv=0.661003 | nps=0.000000 | grad_norm=0.0031817033886909485 | selected_counts=[10, 5, 2, 12, 2, 11, 4, 0]\n",
      "[epoch 8 batch 12] total_loss=0.010720 | det_loss=0.010059 | tv=0.660875 | nps=0.000000 | grad_norm=0.002025374909862876 | selected_counts=[10, 2, 2, 4, 12, 5, 6, 2]\n",
      "[epoch 8 batch 13] total_loss=0.012457 | det_loss=0.011796 | tv=0.660749 | nps=0.000000 | grad_norm=0.003257772186771035 | selected_counts=[4, 2, 1, 1, 4, 3, 1, 4]\n",
      "[epoch 8 batch 14] total_loss=0.008623 | det_loss=0.007963 | tv=0.660620 | nps=0.000000 | grad_norm=0.0030705423559993505 | selected_counts=[6, 3, 2, 4, 1, 1, 7, 3]\n",
      "[epoch 8 batch 15] total_loss=0.008364 | det_loss=0.007704 | tv=0.660489 | nps=0.000000 | grad_norm=0.0019954654853791 | selected_counts=[4, 1, 1, 3, 1, 9, 1, 7]\n",
      "[epoch 8 batch 16] total_loss=0.003683 | det_loss=0.003023 | tv=0.660359 | nps=0.000000 | grad_norm=0.000995625858195126 | selected_counts=[10, 12, 10, 3, 6, 1, 6, 2]\n",
      "[epoch 8 batch 17] total_loss=0.005090 | det_loss=0.004429 | tv=0.660234 | nps=0.000000 | grad_norm=0.0010137605713680387 | selected_counts=[2, 1, 13, 2, 5, 1, 3, 5]\n",
      "[epoch 8 batch 18] total_loss=0.021805 | det_loss=0.021144 | tv=0.660113 | nps=0.000000 | grad_norm=0.004572695586830378 | selected_counts=[4, 2, 2, 7, 1, 6, 1, 0]\n",
      "[epoch 8 batch 19] total_loss=0.012034 | det_loss=0.011374 | tv=0.659979 | nps=0.000000 | grad_norm=0.002279824111610651 | selected_counts=[3, 3, 1, 2, 4, 9, 4, 2]\n",
      "[epoch 8 batch 20] total_loss=0.003499 | det_loss=0.002839 | tv=0.659846 | nps=0.000000 | grad_norm=0.0012853145599365234 | selected_counts=[7, 9, 2, 7, 2, 13, 1, 11]\n",
      "[epoch 8 batch 21] total_loss=0.011022 | det_loss=0.010362 | tv=0.659719 | nps=0.000000 | grad_norm=0.002570419805124402 | selected_counts=[6, 1, 1, 3, 6, 7, 3, 9]\n",
      "[epoch 8 batch 22] total_loss=0.009366 | det_loss=0.008707 | tv=0.659595 | nps=0.000000 | grad_norm=0.0023356908932328224 | selected_counts=[2, 1, 2, 5, 3, 6, 1, 10]\n",
      "[epoch 8 batch 23] total_loss=0.010322 | det_loss=0.009663 | tv=0.659471 | nps=0.000000 | grad_norm=0.001598106580786407 | selected_counts=[4, 4, 7, 6, 3, 5, 4, 1]\n",
      "[epoch 8 batch 24] total_loss=0.014089 | det_loss=0.013430 | tv=0.659351 | nps=0.000000 | grad_norm=0.0038091838359832764 | selected_counts=[2, 6, 2, 2, 2, 2, 6, 7]\n",
      "[epoch 8 batch 25] total_loss=0.006189 | det_loss=0.005530 | tv=0.659225 | nps=0.000000 | grad_norm=0.0028009808156639338 | selected_counts=[3, 5, 1, 7, 17, 5, 1, 1]\n",
      "[epoch 8 batch 26] total_loss=0.012571 | det_loss=0.011912 | tv=0.659099 | nps=0.000000 | grad_norm=0.0033508476335555315 | selected_counts=[6, 5, 11, 1, 6, 13, 3, 5]\n",
      "[epoch 8 batch 27] total_loss=0.003844 | det_loss=0.003185 | tv=0.658970 | nps=0.000000 | grad_norm=0.0021766284480690956 | selected_counts=[3, 2, 3, 14, 2, 11, 2, 1]\n",
      "[epoch 8 batch 28] total_loss=0.005305 | det_loss=0.004646 | tv=0.658841 | nps=0.000000 | grad_norm=0.002416385104879737 | selected_counts=[4, 7, 7, 12, 8, 3, 3, 5]\n",
      "[epoch 8 batch 29] total_loss=0.007094 | det_loss=0.006435 | tv=0.658715 | nps=0.000000 | grad_norm=0.0021504953037947416 | selected_counts=[4, 1, 1, 2, 2, 2, 1, 3]\n",
      "[epoch 8 batch 30] total_loss=0.006704 | det_loss=0.006046 | tv=0.658592 | nps=0.000000 | grad_norm=0.002071645110845566 | selected_counts=[6, 2, 1, 1, 5, 1, 1, 1]\n",
      "[epoch 8 batch 31] total_loss=0.006237 | det_loss=0.005578 | tv=0.658472 | nps=0.000000 | grad_norm=0.0010475951712578535 | selected_counts=[1, 1, 5, 9, 1, 2, 9, 9]\n",
      "[epoch 8 batch 32] total_loss=0.008195 | det_loss=0.007537 | tv=0.658358 | nps=0.000000 | grad_norm=0.0014062696136534214 | selected_counts=[5, 1, 5, 1, 5, 6, 6, 1]\n",
      "[epoch 8 batch 33] total_loss=0.008989 | det_loss=0.008331 | tv=0.658249 | nps=0.000000 | grad_norm=0.0015491739613935351 | selected_counts=[3, 2, 5, 1, 16, 1, 1, 8]\n",
      "[epoch 8 batch 34] total_loss=0.004656 | det_loss=0.003998 | tv=0.658144 | nps=0.000000 | grad_norm=0.0010916064493358135 | selected_counts=[2, 1, 2, 4, 6, 5, 2, 4]\n",
      "[epoch 8 batch 35] total_loss=0.005718 | det_loss=0.005060 | tv=0.658044 | nps=0.000000 | grad_norm=0.001617147121578455 | selected_counts=[3, 5, 13, 3, 1, 1, 5, 9]\n",
      "[epoch 8 batch 36] total_loss=0.014540 | det_loss=0.013882 | tv=0.657946 | nps=0.000000 | grad_norm=0.0037199617363512516 | selected_counts=[2, 2, 2, 1, 1, 3, 6, 1]\n",
      "[epoch 8 batch 37] total_loss=0.006842 | det_loss=0.006184 | tv=0.657840 | nps=0.000000 | grad_norm=0.001016933936625719 | selected_counts=[1, 1, 2, 2, 7, 1, 5, 10]\n",
      "[epoch 8 batch 38] total_loss=0.009969 | det_loss=0.009311 | tv=0.657736 | nps=0.000000 | grad_norm=0.0028378749266266823 | selected_counts=[9, 3, 0, 2, 6, 5, 6, 4]\n",
      "[epoch 8 batch 39] total_loss=0.008206 | det_loss=0.007548 | tv=0.657627 | nps=0.000000 | grad_norm=0.002512661274522543 | selected_counts=[3, 1, 13, 7, 2, 2, 2, 8]\n",
      "[epoch 8 batch 40] total_loss=0.008094 | det_loss=0.007436 | tv=0.657515 | nps=0.000000 | grad_norm=0.0016403925837948918 | selected_counts=[11, 1, 1, 1, 7, 1, 3, 7]\n",
      "[epoch 8 batch 41] total_loss=0.010287 | det_loss=0.009630 | tv=0.657404 | nps=0.000000 | grad_norm=0.002500528935343027 | selected_counts=[2, 3, 8, 7, 1, 10, 4, 4]\n",
      "[epoch 8 batch 42] total_loss=0.006108 | det_loss=0.005450 | tv=0.657290 | nps=0.000000 | grad_norm=0.0014106000307947397 | selected_counts=[1, 7, 1, 2, 3, 4, 6, 11]\n",
      "[epoch 8 batch 43] total_loss=0.006143 | det_loss=0.005486 | tv=0.657179 | nps=0.000000 | grad_norm=0.0016903096111491323 | selected_counts=[1, 5, 1, 11, 1, 4, 2, 13]\n",
      "[epoch 8 batch 44] total_loss=0.006699 | det_loss=0.006042 | tv=0.657071 | nps=0.000000 | grad_norm=0.0018635656451806426 | selected_counts=[3, 2, 4, 2, 14, 4, 3, 12]\n",
      "[epoch 8 batch 45] total_loss=0.011694 | det_loss=0.011037 | tv=0.656963 | nps=0.000000 | grad_norm=0.0029175421223044395 | selected_counts=[1, 6, 9, 5, 1, 1, 1, 1]\n",
      "[epoch 8 batch 46] total_loss=0.015676 | det_loss=0.015019 | tv=0.656847 | nps=0.000000 | grad_norm=0.0025020164903253317 | selected_counts=[3, 0, 4, 3, 4, 1, 2, 1]\n",
      "[epoch 8 batch 47] total_loss=0.008704 | det_loss=0.008047 | tv=0.656728 | nps=0.000000 | grad_norm=0.002053680829703808 | selected_counts=[11, 9, 8, 1, 1, 2, 1, 7]\n",
      "[epoch 8 batch 48] total_loss=0.011925 | det_loss=0.011269 | tv=0.656609 | nps=0.000000 | grad_norm=0.002986351028084755 | selected_counts=[9, 2, 2, 10, 4, 3, 3, 1]\n",
      "[epoch 8 batch 49] total_loss=0.007996 | det_loss=0.007339 | tv=0.656489 | nps=0.000000 | grad_norm=0.002057464327663183 | selected_counts=[13, 1, 12, 6, 1, 1, 1, 3]\n",
      "[epoch 8 batch 50] total_loss=0.006964 | det_loss=0.006308 | tv=0.656370 | nps=0.000000 | grad_norm=0.002095612930133939 | selected_counts=[1, 1, 8, 1, 5, 11, 5, 10]\n",
      "[epoch 8 batch 51] total_loss=0.003869 | det_loss=0.003213 | tv=0.656251 | nps=0.000000 | grad_norm=0.0013148710131645203 | selected_counts=[4, 6, 11, 5, 1, 13, 3, 4]\n",
      "[epoch 8 batch 52] total_loss=0.020177 | det_loss=0.019521 | tv=0.656137 | nps=0.000000 | grad_norm=0.003006802638992667 | selected_counts=[5, 1, 1, 5, 6, 2, 3, 6]\n",
      "[epoch 8 batch 53] total_loss=0.004062 | det_loss=0.003406 | tv=0.656020 | nps=0.000000 | grad_norm=0.0009967009536921978 | selected_counts=[11, 1, 1, 10, 4, 1, 1, 2]\n",
      "[epoch 8 batch 54] total_loss=0.004633 | det_loss=0.003977 | tv=0.655906 | nps=0.000000 | grad_norm=0.001261735102161765 | selected_counts=[1, 1, 14, 3, 3, 7, 4, 2]\n",
      "[epoch 8 batch 55] total_loss=0.004432 | det_loss=0.003776 | tv=0.655797 | nps=0.000000 | grad_norm=0.001198153360746801 | selected_counts=[1, 5, 3, 4, 10, 2, 7, 8]\n",
      "[epoch 8 batch 56] total_loss=0.005479 | det_loss=0.004823 | tv=0.655691 | nps=0.000000 | grad_norm=0.0015981807373464108 | selected_counts=[5, 4, 6, 3, 8, 1, 1, 1]\n",
      "[epoch 8 batch 57] total_loss=0.011559 | det_loss=0.010904 | tv=0.655589 | nps=0.000000 | grad_norm=0.002843237714841962 | selected_counts=[11, 5, 6, 2, 9, 1, 7, 7]\n",
      "[epoch 8 batch 58] total_loss=0.006506 | det_loss=0.005851 | tv=0.655487 | nps=0.000000 | grad_norm=0.001479433965869248 | selected_counts=[1, 1, 3, 6, 9, 5, 1, 2]\n",
      "[epoch 8 batch 59] total_loss=0.011154 | det_loss=0.010498 | tv=0.655384 | nps=0.000000 | grad_norm=0.002215105341747403 | selected_counts=[1, 5, 3, 4, 5, 2, 1, 5]\n",
      "[epoch 8 batch 60] total_loss=0.004978 | det_loss=0.004323 | tv=0.655276 | nps=0.000000 | grad_norm=0.0017595713725313544 | selected_counts=[1, 8, 1, 10, 4, 1, 1, 3]\n",
      "[epoch 8 batch 61] total_loss=0.006183 | det_loss=0.005527 | tv=0.655169 | nps=0.000000 | grad_norm=0.00158098922111094 | selected_counts=[1, 2, 6, 2, 2, 1, 9, 2]\n",
      "[epoch 8 batch 62] total_loss=0.009867 | det_loss=0.009212 | tv=0.655066 | nps=0.000000 | grad_norm=0.0017781349597498775 | selected_counts=[1, 1, 1, 10, 10, 5, 9, 3]\n",
      "[epoch 8 batch 63] total_loss=0.008076 | det_loss=0.007421 | tv=0.654962 | nps=0.000000 | grad_norm=0.0020500896498560905 | selected_counts=[2, 8, 6, 1, 1, 1, 2, 8]\n",
      "[epoch 8 batch 64] total_loss=0.010672 | det_loss=0.010017 | tv=0.654859 | nps=0.000000 | grad_norm=0.0034461747854948044 | selected_counts=[4, 3, 4, 8, 6, 3, 1, 2]\n",
      "[epoch 8 batch 65] total_loss=0.005425 | det_loss=0.004770 | tv=0.654751 | nps=0.000000 | grad_norm=0.001542361220344901 | selected_counts=[1, 3, 3, 2, 4, 2, 4, 3]\n",
      "[epoch 8 batch 66] total_loss=0.006667 | det_loss=0.006012 | tv=0.654645 | nps=0.000000 | grad_norm=0.0012026376789435744 | selected_counts=[1, 5, 1, 3, 1, 7, 9, 3]\n",
      "[epoch 8 batch 67] total_loss=0.007284 | det_loss=0.006629 | tv=0.654542 | nps=0.000000 | grad_norm=0.002804732648655772 | selected_counts=[4, 1, 2, 3, 11, 11, 0, 7]\n",
      "[epoch 8 batch 68] total_loss=0.008458 | det_loss=0.007804 | tv=0.654432 | nps=0.000000 | grad_norm=0.001558966701850295 | selected_counts=[4, 1, 5, 2, 2, 8, 11, 2]\n",
      "[epoch 8 batch 69] total_loss=0.011992 | det_loss=0.011338 | tv=0.654325 | nps=0.000000 | grad_norm=0.0023577362298965454 | selected_counts=[1, 8, 1, 1, 3, 1, 11, 6]\n",
      "[epoch 8 batch 70] total_loss=0.004706 | det_loss=0.004052 | tv=0.654219 | nps=0.000000 | grad_norm=0.0030782187823206186 | selected_counts=[6, 1, 2, 6, 1, 5, 5, 2]\n",
      "[epoch 8 batch 71] total_loss=0.010856 | det_loss=0.010202 | tv=0.654111 | nps=0.000000 | grad_norm=0.003571998095139861 | selected_counts=[2, 1, 9, 1, 4, 12, 1, 2]\n",
      "[epoch 8 batch 72] total_loss=0.004605 | det_loss=0.003951 | tv=0.653995 | nps=0.000000 | grad_norm=0.0017128534382209182 | selected_counts=[6, 8, 6, 3, 2, 10, 8, 4]\n",
      "[epoch 8 batch 73] total_loss=0.004200 | det_loss=0.003546 | tv=0.653879 | nps=0.000000 | grad_norm=0.0008485223515890539 | selected_counts=[3, 4, 7, 9, 12, 1, 2, 1]\n",
      "[epoch 8 batch 74] total_loss=0.014300 | det_loss=0.013646 | tv=0.653767 | nps=0.000000 | grad_norm=0.0023204893805086613 | selected_counts=[6, 1, 1, 1, 6, 3, 12, 3]\n",
      "[epoch 8 batch 75] total_loss=0.007804 | det_loss=0.007150 | tv=0.653655 | nps=0.000000 | grad_norm=0.002119929762557149 | selected_counts=[4, 9, 3, 3, 3, 3, 2, 2]\n",
      "[epoch 8 batch 76] total_loss=0.013415 | det_loss=0.012761 | tv=0.653543 | nps=0.000000 | grad_norm=0.002276603365316987 | selected_counts=[1, 8, 6, 6, 1, 1]\n",
      "Epoch 8 saved patch snapshot.\n",
      "[epoch 9 batch 0] total_loss=0.006133 | det_loss=0.005479 | tv=0.653434 | nps=0.000000 | grad_norm=0.001802240265533328 | selected_counts=[14, 13, 1, 1, 4, 1, 1, 1]\n",
      "[epoch 9 batch 1] total_loss=0.002879 | det_loss=0.002226 | tv=0.653324 | nps=0.000000 | grad_norm=0.0008112963987514377 | selected_counts=[8, 2, 2, 3, 3, 4, 1, 3]\n",
      "[epoch 9 batch 2] total_loss=0.013118 | det_loss=0.012465 | tv=0.653217 | nps=0.000000 | grad_norm=0.0018537999130785465 | selected_counts=[1, 1, 1, 0, 2, 1, 7, 4]\n",
      "[epoch 9 batch 3] total_loss=0.004515 | det_loss=0.003862 | tv=0.653112 | nps=0.000000 | grad_norm=0.0018001707503572106 | selected_counts=[3, 8, 6, 1, 13, 10, 6, 5]\n",
      "[epoch 9 batch 4] total_loss=0.013118 | det_loss=0.012465 | tv=0.653009 | nps=0.000000 | grad_norm=0.0027563590556383133 | selected_counts=[4, 10, 5, 6, 6, 3, 1, 4]\n",
      "[epoch 9 batch 5] total_loss=0.011377 | det_loss=0.010724 | tv=0.652903 | nps=0.000000 | grad_norm=0.00253767310641706 | selected_counts=[1, 1, 2, 6, 4, 1, 1, 5]\n",
      "[epoch 9 batch 6] total_loss=0.008263 | det_loss=0.007610 | tv=0.652796 | nps=0.000000 | grad_norm=0.0055914632976055145 | selected_counts=[1, 7, 1, 8, 1, 6, 3, 6]\n",
      "[epoch 9 batch 7] total_loss=0.008432 | det_loss=0.007780 | tv=0.652674 | nps=0.000000 | grad_norm=0.002696866635233164 | selected_counts=[2, 4, 1, 3, 2, 2, 2, 5]\n",
      "[epoch 9 batch 8] total_loss=0.006774 | det_loss=0.006122 | tv=0.652549 | nps=0.000000 | grad_norm=0.0013973896857351065 | selected_counts=[1, 13, 10, 10, 14, 6, 1, 4]\n",
      "[epoch 9 batch 9] total_loss=0.009975 | det_loss=0.009322 | tv=0.652427 | nps=0.000000 | grad_norm=0.0023674648255109787 | selected_counts=[5, 4, 4, 8, 1, 2, 1, 2]\n",
      "[epoch 9 batch 10] total_loss=0.009858 | det_loss=0.009206 | tv=0.652303 | nps=0.000000 | grad_norm=0.0020049321465194225 | selected_counts=[1, 11, 13, 5, 4, 7, 13, 5]\n",
      "[epoch 9 batch 11] total_loss=0.008578 | det_loss=0.007926 | tv=0.652185 | nps=0.000000 | grad_norm=0.0017571659991517663 | selected_counts=[6, 3, 5, 1, 12, 1, 1, 2]\n",
      "[epoch 9 batch 12] total_loss=0.004312 | det_loss=0.003659 | tv=0.652070 | nps=0.000000 | grad_norm=0.0010047558462247252 | selected_counts=[3, 2, 8, 10, 6, 11, 5, 1]\n",
      "[epoch 9 batch 13] total_loss=0.004848 | det_loss=0.004196 | tv=0.651959 | nps=0.000000 | grad_norm=0.0011651902459561825 | selected_counts=[3, 4, 1, 3, 3, 7, 3, 4]\n",
      "[epoch 9 batch 14] total_loss=0.012162 | det_loss=0.011510 | tv=0.651853 | nps=0.000000 | grad_norm=0.002661135047674179 | selected_counts=[6, 3, 6, 3, 2, 3, 2, 12]\n",
      "[epoch 9 batch 15] total_loss=0.014985 | det_loss=0.014333 | tv=0.651746 | nps=0.000000 | grad_norm=0.003310665488243103 | selected_counts=[4, 2, 1, 2, 7, 1, 2, 3]\n",
      "[epoch 9 batch 16] total_loss=0.010013 | det_loss=0.009362 | tv=0.651635 | nps=0.000000 | grad_norm=0.0029572374187409878 | selected_counts=[7, 1, 2, 1, 5, 4, 1, 2]\n",
      "[epoch 9 batch 17] total_loss=0.012122 | det_loss=0.011470 | tv=0.651519 | nps=0.000000 | grad_norm=0.003029332961887121 | selected_counts=[4, 6, 1, 1, 1, 0, 2, 2]\n",
      "[epoch 9 batch 18] total_loss=0.005606 | det_loss=0.004954 | tv=0.651400 | nps=0.000000 | grad_norm=0.0006431288784369826 | selected_counts=[3, 4, 4, 1, 4, 0, 7, 1]\n",
      "[epoch 9 batch 19] total_loss=0.007335 | det_loss=0.006683 | tv=0.651287 | nps=0.000000 | grad_norm=0.0033663923386484385 | selected_counts=[1, 4, 15, 1, 1, 1, 1, 5]\n",
      "[epoch 9 batch 20] total_loss=0.005325 | det_loss=0.004674 | tv=0.651170 | nps=0.000000 | grad_norm=0.002666258020326495 | selected_counts=[5, 9, 1, 3, 14, 2, 7, 7]\n",
      "[epoch 9 batch 21] total_loss=0.007778 | det_loss=0.007127 | tv=0.651049 | nps=0.000000 | grad_norm=0.0017175429966300726 | selected_counts=[8, 3, 2, 7, 1, 1, 1, 7]\n",
      "[epoch 9 batch 22] total_loss=0.004126 | det_loss=0.003475 | tv=0.650930 | nps=0.000000 | grad_norm=0.0010285002645105124 | selected_counts=[1, 2, 6, 5, 6, 12, 2, 1]\n",
      "[epoch 9 batch 23] total_loss=0.004002 | det_loss=0.003351 | tv=0.650815 | nps=0.000000 | grad_norm=0.0008729049586690962 | selected_counts=[1, 10, 2, 21, 6, 1, 5, 1]\n",
      "[epoch 9 batch 24] total_loss=0.009039 | det_loss=0.008388 | tv=0.650706 | nps=0.000000 | grad_norm=0.002543036127462983 | selected_counts=[1, 3, 1, 1, 4, 1, 4, 1]\n",
      "[epoch 9 batch 25] total_loss=0.002610 | det_loss=0.001959 | tv=0.650595 | nps=0.000000 | grad_norm=0.0009401257848367095 | selected_counts=[6, 3, 12, 4, 8, 8, 5, 1]\n",
      "[epoch 9 batch 26] total_loss=0.005629 | det_loss=0.004979 | tv=0.650491 | nps=0.000000 | grad_norm=0.0009377212845720351 | selected_counts=[1, 0, 1, 7, 6, 8, 4, 2]\n",
      "[epoch 9 batch 27] total_loss=0.008476 | det_loss=0.007826 | tv=0.650390 | nps=0.000000 | grad_norm=0.002812204882502556 | selected_counts=[9, 4, 3, 2, 11, 5, 6, 3]\n",
      "[epoch 9 batch 28] total_loss=0.011294 | det_loss=0.010644 | tv=0.650285 | nps=0.000000 | grad_norm=0.0040452247485518456 | selected_counts=[1, 9, 2, 1, 6, 16, 4, 2]\n",
      "[epoch 9 batch 29] total_loss=0.003650 | det_loss=0.003000 | tv=0.650170 | nps=0.000000 | grad_norm=0.002319503342732787 | selected_counts=[2, 4, 1, 1, 4, 6, 7, 3]\n",
      "[epoch 9 batch 30] total_loss=0.031881 | det_loss=0.031231 | tv=0.650052 | nps=0.000000 | grad_norm=0.0037675348576158285 | selected_counts=[1, 1, 1, 1, 2, 5, 1, 3]\n",
      "[epoch 9 batch 31] total_loss=0.008535 | det_loss=0.007885 | tv=0.649925 | nps=0.000000 | grad_norm=0.0025334330275654793 | selected_counts=[8, 8, 3, 1, 5, 3, 2, 3]\n",
      "[epoch 9 batch 32] total_loss=0.004834 | det_loss=0.004184 | tv=0.649796 | nps=0.000000 | grad_norm=0.0010796007700264454 | selected_counts=[4, 7, 4, 4, 11, 17, 3, 7]\n",
      "[epoch 9 batch 33] total_loss=0.012476 | det_loss=0.011827 | tv=0.649674 | nps=0.000000 | grad_norm=0.002474564127624035 | selected_counts=[2, 3, 1, 2, 4, 1, 4, 1]\n",
      "[epoch 9 batch 34] total_loss=0.005064 | det_loss=0.004415 | tv=0.649551 | nps=0.000000 | grad_norm=0.0010151432361453772 | selected_counts=[4, 5, 4, 2, 3, 6, 9, 1]\n",
      "[epoch 9 batch 35] total_loss=0.007870 | det_loss=0.007221 | tv=0.649435 | nps=0.000000 | grad_norm=0.0017940724501386285 | selected_counts=[4, 8, 5, 16, 4, 1, 1, 5]\n",
      "[epoch 9 batch 36] total_loss=0.016805 | det_loss=0.016156 | tv=0.649321 | nps=0.000000 | grad_norm=0.0030984627082943916 | selected_counts=[1, 1, 2, 1, 3, 10, 7, 1]\n",
      "[epoch 9 batch 37] total_loss=0.005417 | det_loss=0.004768 | tv=0.649204 | nps=0.000000 | grad_norm=0.002184764016419649 | selected_counts=[1, 1, 7, 1, 1, 4, 2, 8]\n",
      "[epoch 9 batch 38] total_loss=0.006942 | det_loss=0.006293 | tv=0.649085 | nps=0.000000 | grad_norm=0.002810690551996231 | selected_counts=[3, 2, 1, 5, 4, 15, 3, 1]\n",
      "[epoch 9 batch 39] total_loss=0.010812 | det_loss=0.010163 | tv=0.648963 | nps=0.000000 | grad_norm=0.0027411403134465218 | selected_counts=[1, 1, 6, 1, 6, 1, 3, 12]\n",
      "[epoch 9 batch 40] total_loss=0.008094 | det_loss=0.007446 | tv=0.648841 | nps=0.000000 | grad_norm=0.0019460786134004593 | selected_counts=[5, 1, 4, 5, 2, 3, 1, 8]\n",
      "[epoch 9 batch 41] total_loss=0.009405 | det_loss=0.008756 | tv=0.648724 | nps=0.000000 | grad_norm=0.0032294767443090677 | selected_counts=[6, 5, 2, 3, 5, 1, 1, 1]\n",
      "[epoch 9 batch 42] total_loss=0.003795 | det_loss=0.003146 | tv=0.648599 | nps=0.000000 | grad_norm=0.0013220716500654817 | selected_counts=[7, 2, 3, 4, 1, 8, 5, 1]\n",
      "[epoch 9 batch 43] total_loss=0.008111 | det_loss=0.007462 | tv=0.648479 | nps=0.000000 | grad_norm=0.0028008108492940664 | selected_counts=[2, 4, 1, 6, 10, 4, 5, 2]\n",
      "[epoch 9 batch 44] total_loss=0.006958 | det_loss=0.006309 | tv=0.648355 | nps=0.000000 | grad_norm=0.0014464287087321281 | selected_counts=[2, 7, 0, 2, 2, 5, 3, 1]\n",
      "[epoch 9 batch 45] total_loss=0.005096 | det_loss=0.004447 | tv=0.648236 | nps=0.000000 | grad_norm=0.0026350547559559345 | selected_counts=[2, 5, 8, 2, 6, 8, 2, 5]\n",
      "[epoch 9 batch 46] total_loss=0.010289 | det_loss=0.009641 | tv=0.648116 | nps=0.000000 | grad_norm=0.0020535478834062815 | selected_counts=[1, 2, 1, 5, 0, 2, 3, 12]\n",
      "[epoch 9 batch 47] total_loss=0.012502 | det_loss=0.011854 | tv=0.647997 | nps=0.000000 | grad_norm=0.002705270890146494 | selected_counts=[3, 3, 7, 1, 8, 8, 4, 1]\n",
      "[epoch 9 batch 48] total_loss=0.011838 | det_loss=0.011190 | tv=0.647877 | nps=0.000000 | grad_norm=0.0028488400857895613 | selected_counts=[7, 1, 1, 10, 3, 8, 2, 15]\n",
      "[epoch 9 batch 49] total_loss=0.011344 | det_loss=0.010697 | tv=0.647757 | nps=0.000000 | grad_norm=0.0033873345237225294 | selected_counts=[3, 1, 7, 6, 11, 1, 2, 2]\n",
      "[epoch 9 batch 50] total_loss=0.016746 | det_loss=0.016098 | tv=0.647626 | nps=0.000000 | grad_norm=0.004576214589178562 | selected_counts=[4, 6, 1, 2, 3, 1, 10, 4]\n",
      "[epoch 9 batch 51] total_loss=0.008138 | det_loss=0.007491 | tv=0.647489 | nps=0.000000 | grad_norm=0.001877509173937142 | selected_counts=[1, 4, 1, 10, 8, 6, 3, 6]\n",
      "[epoch 9 batch 52] total_loss=0.014962 | det_loss=0.014315 | tv=0.647354 | nps=0.000000 | grad_norm=0.005494080949574709 | selected_counts=[1, 13, 1, 7, 4, 1, 4, 2]\n",
      "[epoch 9 batch 53] total_loss=0.010989 | det_loss=0.010342 | tv=0.647205 | nps=0.000000 | grad_norm=0.0036300797946751118 | selected_counts=[11, 4, 2, 3, 1, 3, 6, 1]\n",
      "[epoch 9 batch 54] total_loss=0.002995 | det_loss=0.002348 | tv=0.647054 | nps=0.000000 | grad_norm=0.0010045890230685472 | selected_counts=[5, 3, 10, 9, 6, 11, 4, 6]\n",
      "[epoch 9 batch 55] total_loss=0.007610 | det_loss=0.006963 | tv=0.646909 | nps=0.000000 | grad_norm=0.0007747120689600706 | selected_counts=[0, 16, 6, 10, 0, 3, 1, 9]\n",
      "[epoch 9 batch 56] total_loss=0.007317 | det_loss=0.006671 | tv=0.646772 | nps=0.000000 | grad_norm=0.0017397436313331127 | selected_counts=[5, 1, 4, 4, 8, 1, 2, 4]\n",
      "[epoch 9 batch 57] total_loss=0.004238 | det_loss=0.003591 | tv=0.646642 | nps=0.000000 | grad_norm=0.001227012020535767 | selected_counts=[1, 7, 3, 3, 1, 6, 1, 3]\n",
      "[epoch 9 batch 58] total_loss=0.010267 | det_loss=0.009621 | tv=0.646516 | nps=0.000000 | grad_norm=0.001434533973224461 | selected_counts=[4, 10, 6, 3, 1, 2, 2, 2]\n",
      "[epoch 9 batch 59] total_loss=0.005153 | det_loss=0.004506 | tv=0.646396 | nps=0.000000 | grad_norm=0.0017755655571818352 | selected_counts=[1, 2, 1, 4, 1, 3, 2, 5]\n",
      "[epoch 9 batch 60] total_loss=0.012320 | det_loss=0.011674 | tv=0.646278 | nps=0.000000 | grad_norm=0.0029741236940026283 | selected_counts=[3, 9, 6, 6, 1, 1, 1, 1]\n",
      "[epoch 9 batch 61] total_loss=0.006254 | det_loss=0.005608 | tv=0.646157 | nps=0.000000 | grad_norm=0.0015196263557299972 | selected_counts=[4, 1, 1, 1, 1, 1, 1, 9]\n",
      "[epoch 9 batch 62] total_loss=0.007573 | det_loss=0.006927 | tv=0.646041 | nps=0.000000 | grad_norm=0.0033392873592674732 | selected_counts=[12, 1, 1, 5, 5, 3, 4, 1]\n",
      "[epoch 9 batch 63] total_loss=0.010693 | det_loss=0.010047 | tv=0.645922 | nps=0.000000 | grad_norm=0.0029005741234868765 | selected_counts=[1, 1, 6, 7, 4, 1, 2, 2]\n",
      "[epoch 9 batch 64] total_loss=0.004374 | det_loss=0.003728 | tv=0.645800 | nps=0.000000 | grad_norm=0.0011241546599194407 | selected_counts=[6, 3, 1, 1, 3, 5, 12, 8]\n",
      "[epoch 9 batch 65] total_loss=0.003607 | det_loss=0.002961 | tv=0.645681 | nps=0.000000 | grad_norm=0.0008363585802726448 | selected_counts=[1, 7, 5, 5, 8, 3, 7, 1]\n",
      "[epoch 9 batch 66] total_loss=0.004189 | det_loss=0.003544 | tv=0.645569 | nps=0.000000 | grad_norm=0.0035722742322832346 | selected_counts=[5, 2, 3, 2, 10, 8, 1, 3]\n",
      "[epoch 9 batch 67] total_loss=0.004932 | det_loss=0.004287 | tv=0.645454 | nps=0.000000 | grad_norm=0.0012077905703336 | selected_counts=[1, 7, 4, 6, 5, 3, 7, 1]\n",
      "[epoch 9 batch 68] total_loss=0.008708 | det_loss=0.008062 | tv=0.645343 | nps=0.000000 | grad_norm=0.00227926392108202 | selected_counts=[7, 10, 3, 1, 2, 6, 10, 9]\n",
      "[epoch 9 batch 69] total_loss=0.005432 | det_loss=0.004787 | tv=0.645227 | nps=0.000000 | grad_norm=0.0014529804466292262 | selected_counts=[3, 4, 5, 1, 1, 2, 12, 3]\n",
      "[epoch 9 batch 70] total_loss=0.002450 | det_loss=0.001804 | tv=0.645115 | nps=0.000000 | grad_norm=0.0007489760755561292 | selected_counts=[6, 9, 4, 3, 3, 1, 10, 7]\n",
      "[epoch 9 batch 71] total_loss=0.009410 | det_loss=0.008765 | tv=0.645008 | nps=0.000000 | grad_norm=0.0007831986295059323 | selected_counts=[0, 3, 12, 5, 2, 1, 4, 3]\n",
      "[epoch 9 batch 72] total_loss=0.003562 | det_loss=0.002917 | tv=0.644907 | nps=0.000000 | grad_norm=0.0011542196152731776 | selected_counts=[9, 3, 2, 2, 11, 3, 11, 13]\n",
      "[epoch 9 batch 73] total_loss=0.002791 | det_loss=0.002146 | tv=0.644810 | nps=0.000000 | grad_norm=0.0007731543737463653 | selected_counts=[6, 11, 5, 0, 3, 11, 12, 2]\n",
      "[epoch 9 batch 74] total_loss=0.008991 | det_loss=0.008346 | tv=0.644717 | nps=0.000000 | grad_norm=0.0021941838786005974 | selected_counts=[1, 8, 2, 5, 2, 7, 1, 6]\n",
      "[epoch 9 batch 75] total_loss=0.006749 | det_loss=0.006105 | tv=0.644623 | nps=0.000000 | grad_norm=0.002433761255815625 | selected_counts=[4, 2, 9, 11, 4, 0, 2, 2]\n",
      "[epoch 9 batch 76] total_loss=0.006028 | det_loss=0.005384 | tv=0.644525 | nps=0.000000 | grad_norm=0.002957711461931467 | selected_counts=[1, 3, 3, 7, 2, 6]\n",
      "Epoch 9 saved patch snapshot.\n",
      "[epoch 10 batch 0] total_loss=0.006414 | det_loss=0.005769 | tv=0.644421 | nps=0.000000 | grad_norm=0.002523118397220969 | selected_counts=[2, 1, 1, 3, 9, 7, 6, 1]\n",
      "[epoch 10 batch 1] total_loss=0.008997 | det_loss=0.008353 | tv=0.644313 | nps=0.000000 | grad_norm=0.0026991332415491343 | selected_counts=[5, 6, 1, 5, 8, 1, 2, 4]\n",
      "[epoch 10 batch 2] total_loss=0.007081 | det_loss=0.006437 | tv=0.644204 | nps=0.000000 | grad_norm=0.001498193247243762 | selected_counts=[5, 8, 1, 3, 10, 2, 1, 1]\n",
      "[epoch 10 batch 3] total_loss=0.016000 | det_loss=0.015356 | tv=0.644097 | nps=0.000000 | grad_norm=0.005137824919074774 | selected_counts=[2, 3, 0, 3, 1, 2, 3, 8]\n",
      "[epoch 10 batch 4] total_loss=0.005566 | det_loss=0.004922 | tv=0.643972 | nps=0.000000 | grad_norm=0.001549761975184083 | selected_counts=[1, 9, 1, 1, 4, 7, 10, 4]\n",
      "[epoch 10 batch 5] total_loss=0.005767 | det_loss=0.005123 | tv=0.643846 | nps=0.000000 | grad_norm=0.001135892583988607 | selected_counts=[0, 1, 8, 9, 1, 2, 9, 1]\n",
      "[epoch 10 batch 6] total_loss=0.005618 | det_loss=0.004974 | tv=0.643725 | nps=0.000000 | grad_norm=0.002002060879021883 | selected_counts=[2, 5, 13, 1, 7, 4, 7, 1]\n",
      "[epoch 10 batch 7] total_loss=0.006365 | det_loss=0.005722 | tv=0.643607 | nps=0.000000 | grad_norm=0.001620273687876761 | selected_counts=[1, 10, 1, 8, 1, 1, 3, 3]\n",
      "[epoch 10 batch 8] total_loss=0.009726 | det_loss=0.009083 | tv=0.643492 | nps=0.000000 | grad_norm=0.0028823392931371927 | selected_counts=[1, 3, 1, 3, 9, 6, 2, 5]\n",
      "[epoch 10 batch 9] total_loss=0.005567 | det_loss=0.004924 | tv=0.643372 | nps=0.000000 | grad_norm=0.0015758868539705873 | selected_counts=[13, 2, 1, 5, 7, 1, 1, 9]\n",
      "[epoch 10 batch 10] total_loss=0.006279 | det_loss=0.005636 | tv=0.643255 | nps=0.000000 | grad_norm=0.0012375677470117807 | selected_counts=[1, 1, 0, 4, 8, 1, 4, 1]\n",
      "[epoch 10 batch 11] total_loss=0.006011 | det_loss=0.005367 | tv=0.643143 | nps=0.000000 | grad_norm=0.0017627447377890348 | selected_counts=[3, 17, 1, 1, 6, 5, 13, 6]\n",
      "[epoch 10 batch 12] total_loss=0.009012 | det_loss=0.008369 | tv=0.643035 | nps=0.000000 | grad_norm=0.003266731509938836 | selected_counts=[2, 4, 2, 4, 5, 6, 2, 7]\n",
      "[epoch 10 batch 13] total_loss=0.008836 | det_loss=0.008193 | tv=0.642924 | nps=0.000000 | grad_norm=0.00249131815508008 | selected_counts=[2, 3, 1, 8, 3, 2, 5, 1]\n",
      "[epoch 10 batch 14] total_loss=0.014329 | det_loss=0.013687 | tv=0.642813 | nps=0.000000 | grad_norm=0.0021390256006270647 | selected_counts=[6, 12, 1, 5, 7, 8, 5, 2]\n",
      "[epoch 10 batch 15] total_loss=0.004188 | det_loss=0.003545 | tv=0.642700 | nps=0.000000 | grad_norm=0.0010266209719702601 | selected_counts=[7, 1, 7, 1, 7, 2, 3, 1]\n",
      "[epoch 10 batch 16] total_loss=0.005674 | det_loss=0.005031 | tv=0.642591 | nps=0.000000 | grad_norm=0.001860812888480723 | selected_counts=[1, 6, 7, 5, 2, 7, 7, 4]\n",
      "[epoch 10 batch 17] total_loss=0.006617 | det_loss=0.005975 | tv=0.642483 | nps=0.000000 | grad_norm=0.0024893535301089287 | selected_counts=[1, 1, 6, 6, 1, 3, 4, 3]\n",
      "[epoch 10 batch 18] total_loss=0.005846 | det_loss=0.005204 | tv=0.642373 | nps=0.000000 | grad_norm=0.0012329354649409652 | selected_counts=[1, 7, 6, 3, 1, 7, 6, 2]\n",
      "[epoch 10 batch 19] total_loss=0.003838 | det_loss=0.003196 | tv=0.642265 | nps=0.000000 | grad_norm=0.0014244111953303218 | selected_counts=[8, 1, 6, 10, 5, 8, 7, 1]\n",
      "[epoch 10 batch 20] total_loss=0.005209 | det_loss=0.004567 | tv=0.642161 | nps=0.000000 | grad_norm=0.001282610697671771 | selected_counts=[3, 1, 10, 2, 11, 2, 7, 1]\n",
      "[epoch 10 batch 21] total_loss=0.007529 | det_loss=0.006887 | tv=0.642058 | nps=0.000000 | grad_norm=0.004103487823158503 | selected_counts=[9, 7, 4, 6, 11, 1, 2, 1]\n",
      "[epoch 10 batch 22] total_loss=0.007830 | det_loss=0.007188 | tv=0.641946 | nps=0.000000 | grad_norm=0.00238202977925539 | selected_counts=[3, 3, 3, 2, 8, 3, 11, 1]\n",
      "[epoch 10 batch 23] total_loss=0.016886 | det_loss=0.016244 | tv=0.641829 | nps=0.000000 | grad_norm=0.002405878622084856 | selected_counts=[9, 9, 0, 3, 2, 10, 1, 1]\n",
      "[epoch 10 batch 24] total_loss=0.005195 | det_loss=0.004553 | tv=0.641710 | nps=0.000000 | grad_norm=0.001956516643986106 | selected_counts=[11, 7, 4, 1, 1, 7, 1, 1]\n",
      "[epoch 10 batch 25] total_loss=0.007527 | det_loss=0.006885 | tv=0.641591 | nps=0.000000 | grad_norm=0.0029553824570029974 | selected_counts=[1, 5, 4, 2, 2, 4, 6, 8]\n",
      "[epoch 10 batch 26] total_loss=0.009364 | det_loss=0.008722 | tv=0.641468 | nps=0.000000 | grad_norm=0.002573984442278743 | selected_counts=[2, 1, 2, 10, 10, 7, 4, 4]\n",
      "[epoch 10 batch 27] total_loss=0.009205 | det_loss=0.008564 | tv=0.641348 | nps=0.000000 | grad_norm=0.004800085909664631 | selected_counts=[2, 2, 5, 3, 8, 3, 4, 1]\n",
      "[epoch 10 batch 28] total_loss=0.004051 | det_loss=0.003409 | tv=0.641220 | nps=0.000000 | grad_norm=0.001251518027856946 | selected_counts=[2, 9, 5, 5, 8, 2, 3, 8]\n",
      "[epoch 10 batch 29] total_loss=0.003924 | det_loss=0.003283 | tv=0.641095 | nps=0.000000 | grad_norm=0.001647806609980762 | selected_counts=[2, 3, 6, 6, 10, 3, 4, 4]\n",
      "[epoch 10 batch 30] total_loss=0.013428 | det_loss=0.012787 | tv=0.640975 | nps=0.000000 | grad_norm=0.003963731229305267 | selected_counts=[1, 3, 2, 1, 1, 1, 4, 2]\n",
      "[epoch 10 batch 31] total_loss=0.012271 | det_loss=0.011630 | tv=0.640857 | nps=0.000000 | grad_norm=0.0013916287571191788 | selected_counts=[3, 1, 0, 11, 4, 1, 4, 4]\n",
      "[epoch 10 batch 32] total_loss=0.005365 | det_loss=0.004725 | tv=0.640742 | nps=0.000000 | grad_norm=0.001491087139584124 | selected_counts=[3, 1, 5, 3, 1, 5, 9, 1]\n",
      "[epoch 10 batch 33] total_loss=0.009602 | det_loss=0.008962 | tv=0.640635 | nps=0.000000 | grad_norm=0.0017153779044747353 | selected_counts=[0, 4, 9, 11, 4, 3, 5, 7]\n",
      "[epoch 10 batch 34] total_loss=0.006066 | det_loss=0.005426 | tv=0.640529 | nps=0.000000 | grad_norm=0.0024639505427330732 | selected_counts=[1, 7, 3, 4, 7, 1, 1, 7]\n",
      "[epoch 10 batch 35] total_loss=0.009994 | det_loss=0.009354 | tv=0.640426 | nps=0.000000 | grad_norm=0.0022895755246281624 | selected_counts=[1, 13, 3, 2, 5, 1, 4, 1]\n",
      "[epoch 10 batch 36] total_loss=0.005552 | det_loss=0.004912 | tv=0.640324 | nps=0.000000 | grad_norm=0.0016221203841269016 | selected_counts=[3, 7, 1, 5, 3, 2, 7, 1]\n",
      "[epoch 10 batch 37] total_loss=0.004880 | det_loss=0.004240 | tv=0.640223 | nps=0.000000 | grad_norm=0.0018875086680054665 | selected_counts=[1, 1, 3, 2, 3, 1, 8, 4]\n",
      "[epoch 10 batch 38] total_loss=0.004002 | det_loss=0.003362 | tv=0.640125 | nps=0.000000 | grad_norm=0.0013262012507766485 | selected_counts=[2, 2, 15, 7, 3, 1, 3, 1]\n",
      "[epoch 10 batch 39] total_loss=0.010830 | det_loss=0.010190 | tv=0.640029 | nps=0.000000 | grad_norm=0.0031228347215801477 | selected_counts=[4, 5, 3, 3, 6, 1, 1, 1]\n",
      "[epoch 10 batch 40] total_loss=0.008771 | det_loss=0.008131 | tv=0.639932 | nps=0.000000 | grad_norm=0.002638650592416525 | selected_counts=[1, 16, 1, 2, 2, 1, 13, 10]\n",
      "[epoch 10 batch 41] total_loss=0.008457 | det_loss=0.007817 | tv=0.639834 | nps=0.000000 | grad_norm=0.003088785568252206 | selected_counts=[1, 2, 11, 1, 6, 4, 5, 1]\n",
      "[epoch 10 batch 42] total_loss=0.004956 | det_loss=0.004316 | tv=0.639731 | nps=0.000000 | grad_norm=0.0015131818363443017 | selected_counts=[7, 4, 2, 1, 2, 5, 1, 2]\n",
      "[epoch 10 batch 43] total_loss=0.006496 | det_loss=0.005856 | tv=0.639629 | nps=0.000000 | grad_norm=0.002680831588804722 | selected_counts=[5, 5, 3, 6, 8, 2, 7, 5]\n",
      "[epoch 10 batch 44] total_loss=0.003896 | det_loss=0.003257 | tv=0.639525 | nps=0.000000 | grad_norm=0.0017728019738569856 | selected_counts=[6, 7, 6, 4, 1, 9, 2, 4]\n",
      "[epoch 10 batch 45] total_loss=0.008138 | det_loss=0.007498 | tv=0.639424 | nps=0.000000 | grad_norm=0.002279073465615511 | selected_counts=[3, 3, 1, 3, 1, 2, 7, 4]\n",
      "[epoch 10 batch 46] total_loss=0.008323 | det_loss=0.007683 | tv=0.639322 | nps=0.000000 | grad_norm=0.002733121393248439 | selected_counts=[1, 4, 4, 3, 4, 3, 8, 2]\n",
      "[epoch 10 batch 47] total_loss=0.014353 | det_loss=0.013714 | tv=0.639221 | nps=0.000000 | grad_norm=0.004159923642873764 | selected_counts=[1, 8, 2, 4, 11, 3, 7, 6]\n",
      "[epoch 10 batch 48] total_loss=0.004462 | det_loss=0.003823 | tv=0.639104 | nps=0.000000 | grad_norm=0.0016553160967305303 | selected_counts=[12, 2, 10, 4, 1, 7, 2, 2]\n",
      "[epoch 10 batch 49] total_loss=0.003615 | det_loss=0.002976 | tv=0.638989 | nps=0.000000 | grad_norm=0.0013806208735331893 | selected_counts=[3, 2, 8, 5, 4, 11, 6, 3]\n",
      "[epoch 10 batch 50] total_loss=0.005385 | det_loss=0.004746 | tv=0.638877 | nps=0.000000 | grad_norm=0.0010705402819439769 | selected_counts=[7, 2, 2, 7, 1, 4, 6, 8]\n",
      "[epoch 10 batch 51] total_loss=0.009848 | det_loss=0.009209 | tv=0.638769 | nps=0.000000 | grad_norm=0.0022525889798998833 | selected_counts=[2, 3, 1, 5, 4, 5, 1, 10]\n",
      "[epoch 10 batch 52] total_loss=0.011575 | det_loss=0.010937 | tv=0.638659 | nps=0.000000 | grad_norm=0.00237512425519526 | selected_counts=[4, 12, 5, 2, 1, 1, 8, 5]\n",
      "[epoch 10 batch 53] total_loss=0.002350 | det_loss=0.001712 | tv=0.638543 | nps=0.000000 | grad_norm=0.0006199665949679911 | selected_counts=[1, 4, 6, 2, 11, 8, 1, 2]\n",
      "[epoch 10 batch 54] total_loss=0.005185 | det_loss=0.004546 | tv=0.638432 | nps=0.000000 | grad_norm=0.0012477334821596742 | selected_counts=[10, 2, 1, 5, 8, 1, 2, 1]\n",
      "[epoch 10 batch 55] total_loss=0.006638 | det_loss=0.006000 | tv=0.638325 | nps=0.000000 | grad_norm=0.0023062515538185835 | selected_counts=[6, 8, 1, 1, 5, 1, 6, 2]\n",
      "[epoch 10 batch 56] total_loss=0.009931 | det_loss=0.009292 | tv=0.638219 | nps=0.000000 | grad_norm=0.003067243145778775 | selected_counts=[2, 1, 1, 4, 3, 1, 7, 3]\n",
      "[epoch 10 batch 57] total_loss=0.007452 | det_loss=0.006814 | tv=0.638110 | nps=0.000000 | grad_norm=0.003618476213887334 | selected_counts=[1, 4, 2, 5, 6, 3, 1, 1]\n",
      "[epoch 10 batch 58] total_loss=0.005493 | det_loss=0.004855 | tv=0.637992 | nps=0.000000 | grad_norm=0.002660763217136264 | selected_counts=[14, 2, 6, 3, 7, 3, 5, 4]\n",
      "[epoch 10 batch 59] total_loss=0.005394 | det_loss=0.004757 | tv=0.637871 | nps=0.000000 | grad_norm=0.001818383694626391 | selected_counts=[10, 3, 1, 5, 1, 1, 8, 1]\n",
      "[epoch 10 batch 60] total_loss=0.017465 | det_loss=0.016827 | tv=0.637748 | nps=0.000000 | grad_norm=0.00657104654237628 | selected_counts=[2, 2, 3, 11, 13, 5, 1, 15]\n",
      "[epoch 10 batch 61] total_loss=0.003826 | det_loss=0.003188 | tv=0.637605 | nps=0.000000 | grad_norm=0.0008302099886350334 | selected_counts=[1, 8, 2, 2, 4, 3, 1, 6]\n",
      "[epoch 10 batch 62] total_loss=0.007057 | det_loss=0.006419 | tv=0.637465 | nps=0.000000 | grad_norm=0.0018948798533529043 | selected_counts=[3, 3, 14, 2, 2, 7, 3, 15]\n",
      "[epoch 10 batch 63] total_loss=0.012015 | det_loss=0.011378 | tv=0.637329 | nps=0.000000 | grad_norm=0.004272362217307091 | selected_counts=[1, 2, 1, 2, 2, 14, 8, 4]\n",
      "[epoch 10 batch 64] total_loss=0.010812 | det_loss=0.010175 | tv=0.637188 | nps=0.000000 | grad_norm=0.0017348541878163815 | selected_counts=[4, 12, 8, 0, 5, 1, 3, 1]\n",
      "[epoch 10 batch 65] total_loss=0.005809 | det_loss=0.005172 | tv=0.637050 | nps=0.000000 | grad_norm=0.0022844269406050444 | selected_counts=[1, 2, 5, 7, 2, 15, 2, 8]\n",
      "[epoch 10 batch 66] total_loss=0.009358 | det_loss=0.008721 | tv=0.636915 | nps=0.000000 | grad_norm=0.0020756544545292854 | selected_counts=[2, 3, 1, 1, 1, 4, 11, 1]\n",
      "[epoch 10 batch 67] total_loss=0.012546 | det_loss=0.011910 | tv=0.636784 | nps=0.000000 | grad_norm=0.00408957852050662 | selected_counts=[2, 2, 2, 4, 9, 5, 2, 3]\n",
      "[epoch 10 batch 68] total_loss=0.003076 | det_loss=0.002439 | tv=0.636645 | nps=0.000000 | grad_norm=0.0009502999018877745 | selected_counts=[1, 1, 8, 12, 1, 6, 2, 3]\n",
      "[epoch 10 batch 69] total_loss=0.004416 | det_loss=0.003780 | tv=0.636512 | nps=0.000000 | grad_norm=0.0017056834185495973 | selected_counts=[4, 3, 3, 4, 3, 7, 3, 10]\n",
      "[epoch 10 batch 70] total_loss=0.004276 | det_loss=0.003640 | tv=0.636384 | nps=0.000000 | grad_norm=0.0012900112196803093 | selected_counts=[1, 6, 2, 3, 1, 4, 4, 4]\n",
      "[epoch 10 batch 71] total_loss=0.004424 | det_loss=0.003788 | tv=0.636260 | nps=0.000000 | grad_norm=0.001202236395329237 | selected_counts=[1, 4, 5, 1, 3, 1, 8, 3]\n",
      "[epoch 10 batch 72] total_loss=0.004989 | det_loss=0.004353 | tv=0.636142 | nps=0.000000 | grad_norm=0.001244156970642507 | selected_counts=[8, 1, 3, 3, 4, 5, 12, 1]\n",
      "[epoch 10 batch 73] total_loss=0.002913 | det_loss=0.002277 | tv=0.636029 | nps=0.000000 | grad_norm=0.0009304822306148708 | selected_counts=[6, 5, 1, 6, 5, 6, 1, 1]\n",
      "[epoch 10 batch 74] total_loss=0.003726 | det_loss=0.003090 | tv=0.635920 | nps=0.000000 | grad_norm=0.0009753482881933451 | selected_counts=[7, 2, 2, 4, 4, 8, 4, 3]\n",
      "[epoch 10 batch 75] total_loss=0.006424 | det_loss=0.005788 | tv=0.635816 | nps=0.000000 | grad_norm=0.002120498102158308 | selected_counts=[3, 2, 1, 4, 1, 3, 8, 1]\n",
      "[epoch 10 batch 76] total_loss=0.002193 | det_loss=0.001558 | tv=0.635714 | nps=0.000000 | grad_norm=0.0005242995684966445 | selected_counts=[1, 13, 2, 8, 7, 6]\n",
      "Epoch 10 saved patch snapshot.\n",
      "✅ TMM已移除所有hook\n",
      "Done. Final patch saved to /opt/data/private/BlackBox/save/demo\n"
     ]
    }
   ],
   "source": [
    "# %%writefile train.py\n",
    "# /opt/data/private/BlackBox/train.py\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, draw_bounding_boxes\n",
    "from torchvision.ops import box_convert, nms\n",
    "from torch.nn.functional import interpolate\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from inria_dataloader import get_inria_dataloader\n",
    "from tmm import TransformerMaskingMatrix, load_detr_r50, NestedTensor\n",
    "from gse import GradientSelfEnsemble\n",
    "from loss import BlackBoxLoss\n",
    "\n",
    "# -----------------------\n",
    "# Config (可调整)\n",
    "# -----------------------\n",
    "ROOT = \"/opt/data/private/BlackBox\"\n",
    "DATA_ROOT = os.path.join(ROOT, \"data\", \"INRIAPerson\")\n",
    "SAVE_DIR = os.path.join(ROOT, \"save\", \"demo\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# training params\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10           # 可根据需要放大（paper 使用更大迭代）\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# patch params\n",
    "PATCH_SIDE = 300          # 固定 global patch side (严格对齐论文)\n",
    "PATCH_INIT_STD = 0.5\n",
    "MIN_PATCH_PX = 16         # fallback minimum when resizing\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model / detection params\n",
    "MODEL_INPUT_H, MODEL_INPUT_W = 640, 640  # dataloader resize (H, W)\n",
    "TARGET_CLASS_IDX = 1\n",
    "SCORE_THRESH = 0.5\n",
    "FALLBACK_TO_TOP = True\n",
    "FALLBACK_SCORE_THRESH = 0.2\n",
    "IOU_NMS_THRESH = 0.5\n",
    "MIN_BOX_SIDE = 5\n",
    "\n",
    "# loss weights (默认基于论文设置，可调整)\n",
    "DETECTION_WEIGHT = 1.0\n",
    "TV_WEIGHT = 1e-3\n",
    "NPS_WEIGHT = 0.0\n",
    "\n",
    "# EoT / augmentation switches (简单实现)\n",
    "USE_EOT = True\n",
    "EOT_SCALE = (0.9, 1.1)\n",
    "EOT_ROT_DEG = (-10, 10)\n",
    "EOT_BRIGHT = (0.9, 1.1)\n",
    "EOT_CONTRAST = (0.9, 1.1)\n",
    "\n",
    "# reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def detach_cpu(img: torch.Tensor):\n",
    "    \"\"\"return CPU float tensor in 0..1\"\"\"\n",
    "    return img.detach().cpu().clamp(0,1)\n",
    "\n",
    "def draw_boxes_on_tensor(img_tensor: torch.Tensor, boxes_xyxy_cpu: torch.Tensor):\n",
    "    \"\"\"draw boxes (cpu tensor img 3,H,W)\"\"\"\n",
    "    if boxes_xyxy_cpu is None or boxes_xyxy_cpu.numel() == 0:\n",
    "        return img_tensor\n",
    "    img_uint8 = (img_tensor * 255).byte()\n",
    "    boxes = boxes_xyxy_cpu.clone()\n",
    "    H, W = img_tensor.shape[1], img_tensor.shape[2]\n",
    "    boxes[:, [0,2]] = boxes[:, [0,2]].clamp(0, W-1)\n",
    "    boxes[:, [1,3]] = boxes[:, [1,3]].clamp(0, H-1)\n",
    "    valid = (boxes[:,2] > boxes[:,0]) & (boxes[:,3] > boxes[:,1])\n",
    "    boxes = boxes[valid]\n",
    "    if boxes.shape[0] == 0:\n",
    "        return img_tensor\n",
    "    boxes_int = boxes.to(torch.int64)\n",
    "    img_boxes = draw_bounding_boxes(img_uint8, boxes=boxes_int, colors=\"red\", width=2)\n",
    "    return img_boxes.float() / 255.0\n",
    "\n",
    "def detr_boxes_to_xyxy_pixel(pred_boxes):\n",
    "    \"\"\"\n",
    "    pred_boxes: [Q,4] cx,cy,w,h (normalized 0..1 or absolute)\n",
    "    returns [Q,4] xyxy in pixel coords (CPU tensor)\n",
    "    \"\"\"\n",
    "    pb = pred_boxes.clone()\n",
    "    if pb.max() <= 1.01:\n",
    "        pb[:,0] = pb[:,0] * MODEL_INPUT_W\n",
    "        pb[:,1] = pb[:,1] * MODEL_INPUT_H\n",
    "        pb[:,2] = pb[:,2] * MODEL_INPUT_W\n",
    "        pb[:,3] = pb[:,3] * MODEL_INPUT_H\n",
    "    xyxy = box_convert(pb, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "    return xyxy.cpu()\n",
    "\n",
    "def eot_transform_patch(patch_tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    patch_tensor: [1,3,H,W] on DEVICE\n",
    "    apply small random scale / rotate / brightness / contrast\n",
    "    returns transformed patch [1,3,h2,w2]\n",
    "    \"\"\"\n",
    "    if not USE_EOT:\n",
    "        return patch_tensor\n",
    "    # to CPU PIL for some transforms, but to keep gradient chain we operate in tensor domain\n",
    "    _, _, H, W = patch_tensor.shape\n",
    "    # random scale\n",
    "    scale = float(np.random.uniform(EOT_SCALE[0], EOT_SCALE[1]))\n",
    "    new_side = max(1, int(round(PATCH_SIDE * scale)))\n",
    "    p = interpolate(patch_tensor, size=(new_side, new_side), mode='bilinear', align_corners=False)\n",
    "    # random rotate (use TF.affine which accepts tensor)\n",
    "    angle = float(np.random.uniform(EOT_ROT_DEG[0], EOT_ROT_DEG[1]))\n",
    "    # torchvision's functional.affine expects shape [...,H,W], supports tensors\n",
    "    # We'll apply rotate around center (no translate, no shear)\n",
    "    p = TF.affine(\n",
    "        p,\n",
    "        angle=angle,\n",
    "        translate=[0, 0],\n",
    "        scale=1.0,\n",
    "        shear=[0.0, 0.0],\n",
    "        interpolation=InterpolationMode.BILINEAR,  # 替换 resample\n",
    "        fill=0\n",
    "        )\n",
    "    # brightness & contrast by simple scale/add\n",
    "    b = float(np.random.uniform(EOT_BRIGHT[0], EOT_BRIGHT[1]))\n",
    "    c = float(np.random.uniform(EOT_CONTRAST[0], EOT_CONTRAST[1]))\n",
    "    p = torch.clamp((p * c) * b, 0.0, 1.0)\n",
    "    return p\n",
    "\n",
    "def paste_patch_via_mask(base_img: torch.Tensor, patch_tensor: torch.Tensor, center_xy: tuple):\n",
    "    \"\"\"\n",
    "    base_img: [3,H,W] float on device\n",
    "    patch_tensor: [1,3,ph,pw] or [3,ph,pw] on same device\n",
    "    center_xy: (cx, cy) pixel coords (float)\n",
    "    returns new image [3,H,W] (device) with patch pasted (non-inplace, gradient-preserving)\n",
    "    Implemented via mask fusion: out = base*(1-mask) + patch*mask\n",
    "    \"\"\"\n",
    "    if patch_tensor.dim() == 4 and patch_tensor.shape[0] == 1:\n",
    "        p = patch_tensor[0]\n",
    "    elif patch_tensor.dim() == 3:\n",
    "        p = patch_tensor\n",
    "    else:\n",
    "        raise ValueError(\"invalid patch shape\")\n",
    "\n",
    "    ph, pw = p.shape[1], p.shape[2]\n",
    "    cx, cy = int(round(center_xy[0])), int(round(center_xy[1]))\n",
    "    x0 = cx - pw // 2\n",
    "    y0 = cy - ph // 2\n",
    "\n",
    "    H, W = base_img.shape[1], base_img.shape[2]\n",
    "\n",
    "    # compute crop ranges\n",
    "    src_x0, src_y0 = 0, 0\n",
    "    dst_x0, dst_y0 = x0, y0\n",
    "    dst_x1, dst_y1 = x0 + pw, y0 + ph\n",
    "\n",
    "    if dst_x0 < 0:\n",
    "        src_x0 = -dst_x0; dst_x0 = 0\n",
    "    if dst_y0 < 0:\n",
    "        src_y0 = -dst_y0; dst_y0 = 0\n",
    "    if dst_x1 > W:\n",
    "        dst_x1 = W\n",
    "    if dst_y1 > H:\n",
    "        dst_y1 = H\n",
    "\n",
    "    out_w = dst_x1 - dst_x0\n",
    "    out_h = dst_y1 - dst_y0\n",
    "    if out_w <= 0 or out_h <= 0:\n",
    "        return base_img.clone()\n",
    "\n",
    "    src_x1 = src_x0 + out_w\n",
    "    src_y1 = src_y0 + out_h\n",
    "    p_cropped = p[:, src_y0:src_y1, src_x0:src_x1]\n",
    "\n",
    "    # create mask shaped [3,H,W], zeros then set box area to 1\n",
    "    mask = torch.zeros_like(base_img)\n",
    "    mask[:, dst_y0:dst_y1, dst_x0:dst_x1] = 1.0\n",
    "\n",
    "    # build padded_patch with same H,W by padding p_cropped to correct location\n",
    "    padded_patch = torch.zeros_like(base_img)\n",
    "    padded_patch[:, dst_y0:dst_y1, dst_x0:dst_x1] = p_cropped\n",
    "\n",
    "    # fusion (non-inplace)\n",
    "    fused = base_img * (1.0 - mask) + padded_patch * mask\n",
    "    return fused\n",
    "\n",
    "# -----------------------\n",
    "# Data and model init\n",
    "# -----------------------\n",
    "dataloader = get_inria_dataloader(DATA_ROOT, split=\"Train\", batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, disable_random_aug=False)\n",
    "print(\"Train dataset size:\", len(dataloader.dataset))\n",
    "\n",
    "model = load_detr_r50().to(DEVICE)\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# TMM: register once and keep enabled for entire training (严格按论文)\n",
    "tmm = TransformerMaskingMatrix(num_enc_layers=6, num_dec_layers=6, p_base=0.2, sampling_strategy='categorical', device=DEVICE)\n",
    "tmm.register_hooks(model)\n",
    "tmm.reset_grad_history()\n",
    "\n",
    "# GSE and loss\n",
    "gse = GradientSelfEnsemble(model=model, device=DEVICE)\n",
    "loss_fn = BlackBoxLoss(gse=gse, target_class=TARGET_CLASS_IDX,\n",
    "                       detection_weight=DETECTION_WEIGHT, tv_weight=TV_WEIGHT, l2_weight=0.0,\n",
    "                       layer_aggregation='per_layer_loss', use_sigmoid_for_binary=False,\n",
    "                       device=DEVICE)\n",
    "\n",
    "# initialize patch (requires_grad=True)\n",
    "patch = torch.randn(1, 3, PATCH_SIDE, PATCH_SIDE, device=DEVICE) * PATCH_INIT_STD + 0.7\n",
    "patch = patch.clamp(0.0, 1.0)\n",
    "patch.requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([patch], lr=0.005)\n",
    "\n",
    "print(\"Start training (TMM enabled during all forwards). Saving to:\", SAVE_DIR)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop (epoch)\n",
    "# -----------------------\n",
    "global_step = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.eval()  # model remains eval (weights frozen)\n",
    "    tmm.reset_grad_history()  # clear grad history at epoch start for stability\n",
    "\n",
    "    for batch_idx, (imgs, _) in enumerate(dataloader):\n",
    "        imgs = imgs.to(DEVICE).clamp(0,1)\n",
    "        B = imgs.shape[0]\n",
    "\n",
    "        # --- STEP: obtain detections with TMM active (we keep hooks active per-paper)\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                det_out = model(imgs)\n",
    "            except Exception:\n",
    "                det_out = model(NestedTensor(imgs))\n",
    "\n",
    "        batch_boxes_all = []\n",
    "        for bi in range(B):\n",
    "            logits = det_out['pred_logits'][bi]  # [Q,C]\n",
    "            boxes = det_out['pred_boxes'][bi]    # [Q,4]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            cls_scores = probs[..., TARGET_CLASS_IDX]  # [Q]\n",
    "\n",
    "            keep_idx = (cls_scores > SCORE_THRESH).nonzero(as_tuple=False).squeeze(1) if (cls_scores > SCORE_THRESH).any() else torch.tensor([], dtype=torch.long, device=cls_scores.device)\n",
    "            if keep_idx.numel() == 0 and FALLBACK_TO_TOP:\n",
    "                top_score, top_idx = torch.max(cls_scores, dim=0)\n",
    "                if top_score.item() >= FALLBACK_SCORE_THRESH:\n",
    "                    keep_idx = top_idx.unsqueeze(0)\n",
    "                else:\n",
    "                    keep_idx = torch.tensor([], dtype=torch.long, device=cls_scores.device)\n",
    "\n",
    "            if keep_idx.numel() == 0:\n",
    "                batch_boxes_all.append(torch.empty((0,4), dtype=torch.float32))\n",
    "                continue\n",
    "\n",
    "            sel_boxes = boxes[keep_idx]\n",
    "            sel_scores = cls_scores[keep_idx].detach()\n",
    "\n",
    "            sel_xyxy = detr_boxes_to_xyxy_pixel(sel_boxes.detach().cpu())\n",
    "            widths = (sel_xyxy[:,2] - sel_xyxy[:,0])\n",
    "            heights = (sel_xyxy[:,3] - sel_xyxy[:,1])\n",
    "            large_mask = (widths >= MIN_BOX_SIDE) & (heights >= MIN_BOX_SIDE)\n",
    "            if large_mask.sum() == 0:\n",
    "                batch_boxes_all.append(torch.empty((0,4), dtype=torch.float32))\n",
    "                continue\n",
    "            sel_xyxy = sel_xyxy[large_mask]\n",
    "            sel_scores_cpu = sel_scores.detach().cpu()[large_mask]\n",
    "\n",
    "            try:\n",
    "                keep_nms = nms(sel_xyxy, sel_scores_cpu, IOU_NMS_THRESH)\n",
    "            except Exception:\n",
    "                keep_nms = nms(sel_xyxy.cpu(), sel_scores_cpu.cpu(), IOU_NMS_THRESH)\n",
    "            sel_xyxy_nms = sel_xyxy[keep_nms]\n",
    "            batch_boxes_all.append(sel_xyxy_nms)\n",
    "\n",
    "        # --- STEP: build patched images (gradient-preserving fusion)\n",
    "        # keep TMM hooks enabled during forward of patched imgs (already registered)\n",
    "        patched = imgs.clone()\n",
    "        for bi in range(B):\n",
    "            sel_boxes_cpu = batch_boxes_all[bi]  # CPU [K,4]\n",
    "            if sel_boxes_cpu.numel() == 0:\n",
    "                continue\n",
    "            sel_boxes_dev = sel_boxes_cpu.to(DEVICE)\n",
    "            for box in sel_boxes_dev:\n",
    "                xmin, ymin, xmax, ymax = box.tolist()\n",
    "                box_w = max(int(xmax - xmin), 1)\n",
    "                box_h = max(int(ymax - ymin), 1)\n",
    "                short = min(box_w, box_h)\n",
    "                # use fixed patch size but optionally scale a bit relative to short side\n",
    "                scale = float(np.clip(short / PATCH_SIDE, 0.5, 2.0))  # relative scale\n",
    "                # ensure at least MIN_PATCH_PX\n",
    "                side = max(MIN_PATCH_PX, int(round(PATCH_SIDE * scale)))\n",
    "                # apply EoT transforms to patch (returns [1,3,side,side])\n",
    "                patch_to_paste = eot_transform_patch(patch)\n",
    "                # resize transformed patch to desired side\n",
    "                patch_resized = interpolate(patch_to_paste, size=(side, side), mode='bilinear', align_corners=False)\n",
    "                cx = (xmin + xmax) / 2.0\n",
    "                cy = (ymin + ymax) / 2.0\n",
    "                # fusion (non-inplace, gradient-preserving)\n",
    "                patched[bi] = paste_patch_via_mask(patched[bi], patch_resized, center_xy=(cx, cy))\n",
    "\n",
    "        # --- STEP: compute loss and update patch\n",
    "        # ensure tmm.grad_history is available for GSE/TMM internal use\n",
    "        loss_dict = loss_fn(imgs, patched, patch_tensor=patch)\n",
    "        total_loss = loss_dict['total_loss']\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # debug: inspect patch grad\n",
    "        if patch.grad is None:\n",
    "            print(f\"[epoch {epoch+1} batch {batch_idx}] WARNING: patch.grad is None\")\n",
    "            grad_norm = None\n",
    "        else:\n",
    "            grad_norm = patch.grad.detach().cpu().norm().item()\n",
    "\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            patch.clamp_(0.0, 1.0)\n",
    "\n",
    "        # debug print\n",
    "        det_loss_v = loss_dict.get('det_loss', torch.tensor(0.0)).item() if isinstance(loss_dict.get('det_loss', 0.0), torch.Tensor) else float(loss_dict.get('det_loss', 0.0))\n",
    "        tv_loss_v = loss_dict.get('tv_loss', torch.tensor(0.0)).item() if isinstance(loss_dict.get('tv_loss', 0.0), torch.Tensor) else float(loss_dict.get('tv_loss', 0.0))\n",
    "        nps_loss_v = loss_dict.get('nps_loss', torch.tensor(0.0)).item() if isinstance(loss_dict.get('nps_loss', 0.0), torch.Tensor) else float(loss_dict.get('nps_loss', 0.0))\n",
    "        print(f\"[epoch {epoch+1} batch {batch_idx}] total_loss={total_loss.item():.6f} | det_loss={det_loss_v:.6f} | tv={tv_loss_v:.6f} | nps={nps_loss_v:.6f} | grad_norm={grad_norm} | selected_counts={[b.shape[0] for b in batch_boxes_all]}\")\n",
    "\n",
    "        # save occasional visual snapshots\n",
    "        if global_step % 200 == 0:\n",
    "            # orig, patched, patch\n",
    "            save_image(detach_cpu(imgs[0]), os.path.join(SAVE_DIR, f\"step_{global_step}_orig.png\"))\n",
    "            save_image(detach_cpu(patched[0]), os.path.join(SAVE_DIR, f\"step_{global_step}_patched.png\"))\n",
    "            save_image(patch[0].detach().cpu(), os.path.join(SAVE_DIR, f\"step_{global_step}_patch.png\"))\n",
    "        global_step += 1\n",
    "\n",
    "    # end epoch: save epoch patch\n",
    "    save_image(patch[0].detach().cpu(), os.path.join(SAVE_DIR, f\"epoch_{epoch+1}_patch.png\"))\n",
    "    torch.save(patch[0].detach().cpu(), os.path.join(SAVE_DIR, f\"epoch_{epoch+1}_patch.pt\"))\n",
    "    print(f\"Epoch {epoch+1} saved patch snapshot.\")\n",
    "\n",
    "# cleanup & final save\n",
    "tmm.remove_hooks()\n",
    "save_image(patch[0].detach().cpu(), os.path.join(SAVE_DIR, \"final_patch.png\"))\n",
    "torch.save(patch[0].detach().cpu(), os.path.join(SAVE_DIR, \"final_patch.pt\"))\n",
    "print(\"Done. Final patch saved to\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fd0c2-3dbd-4beb-be4c-6f4f73481557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efe73e-b0b7-4bd5-88a2-1f569900faf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
